{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "Data_Engineering_TIL(20201014)\n",
    "\n",
    "#### # 실습목표\n",
    "\n",
    "Airflow를 이용하여 AWS EMR을 boto3로 띄워서 어떤 spark job을 두개 실행하고 job이 끝나면 EMR cluster를 terminate\n",
    "\n",
    "#### # 실습내용\n",
    "\n",
    "** 실습환경 : Amazon linux AMI 2\n",
    "\n",
    "step 1) Airflow에서 Task 병렬실행을 위한 localexecutor 셋팅\n",
    "\n",
    "https://minman2115.github.io/DE_TIL141/ 참고할 것\n",
    "\n",
    "step 2) DAG 작성 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-1-10-4 ~]$ cd ~/airflow\n",
    "\n",
    "[ec2-user@ip-10-1-10-4 airflow]$ ls\n",
    "airflow.cfg  airflow.db  airflow-webserver.pid  logs  unittests.cfg\n",
    "\n",
    "[ec2-user@ip-10-1-10-4 airflow]$ mkdir dags\n",
    "\n",
    "[ec2-user@ip-10-1-10-4 airflow]$ cd dags\n",
    "\n",
    "[ec2-user@ip-10-1-10-4 dags]$ sudo vim test.py\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "from airflow.contrib.operators.emr_create_job_flow_operator import EmrCreateJobFlowOperator\n",
    "from airflow.contrib.operators.emr_add_steps_operator import EmrAddStepsOperator\n",
    "from airflow.contrib.sensors.emr_step_sensor import EmrStepSensor\n",
    "from airflow.contrib.operators.emr_terminate_job_flow_operator import EmrTerminateJobFlowOperator\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'minman',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2020,10,14, 1, 55),\n",
    "    'email': ['[email_address]'],\n",
    "    'email_on_failure': True\n",
    "}\n",
    "\n",
    "dag = DAG('emr_job_flow_test',\n",
    "         default_args=default_args,\n",
    "         schedule_interval= dt.timedelta(hours=1),\n",
    "         catchup=False\n",
    ")\n",
    "\n",
    "JOB_FLOW_OVERRIDES = {\n",
    "    'Name' : 'pms-EMRtest-test',\n",
    "    'LogUri' : 's3://[bucket_name]/',\n",
    "    'ReleaseLabel' : 'emr-5.28.1',\n",
    "    'Instances' : {\n",
    "            'Ec2KeyName': '[key_name]',\n",
    "            'Ec2SubnetId': 'subnet-xxxxxxxxxxxxxxxxxx',\n",
    "            'EmrManagedMasterSecurityGroup': 'sg-xxxxxxxxxxxxxxxxxxxxx',\n",
    "            'EmrManagedSlaveSecurityGroup': 'sg-xxxxxxxxxxxxxxxxxxxx',\n",
    "            'KeepJobFlowAliveWhenNoSteps': True,\n",
    "            'TerminationProtected': False,\n",
    "            'InstanceGroups': [{\n",
    "                'InstanceRole': 'MASTER',\n",
    "                \"InstanceCount\": 1,\n",
    "                    \"InstanceType\": 'm5.2xlarge',\n",
    "                    \"Market\": \"SPOT\",\n",
    "                    \"Name\": \"Master\"\n",
    "                }, {\n",
    "                    'InstanceRole': 'CORE',\n",
    "                    \"InstanceCount\": 3,\n",
    "                    \"InstanceType\": 'm5.2xlarge',\n",
    "                    \"Market\": \"SPOT\",\n",
    "                    \"Name\": \"Core\",\n",
    "                }, {\n",
    "                    'InstanceRole': 'TASK',\n",
    "                    \"InstanceCount\": 3,\n",
    "                    \"InstanceType\": 'm5.2xlarge',\n",
    "                    \"Market\": \"SPOT\",\n",
    "                    \"Name\": \"Core\",\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    'Applications':[{'Name': 'Spark'},{'Name': 'Hadoop'},{'Name': 'Hive'}],\n",
    "    'JobFlowRole':'EMR_EC2_DefaultRole',\n",
    "    'ServiceRole':'EMR_DefaultRole',\n",
    "    'StepConcurrencyLevel': 10,\n",
    "    'Tags' : [{'Key': 'name', 'Value': 'pms-EMR-test'},\n",
    "              {'Key': 'expiry-date', 'Value': '2020-10-13'},\n",
    "              {'Key': 'owner', 'Value': 'pms'}],\n",
    "    'BootstrapActions':[\n",
    "            {\n",
    "                'Name': 'Maximize Spark Default Config',\n",
    "                'ScriptBootstrapAction': {'Path': 's3://[bucket_name]/maximize-spark-default-config.sh'}\n",
    "                # https://github.com/aws-samples/emr-bootstrap-actions/blob/master/spark/maximize-spark-default-config\n",
    "            }\n",
    "    ],\n",
    "    \"VisibleToAllUsers\": True\n",
    "}\n",
    "\n",
    "cluster_creator = EmrCreateJobFlowOperator(\n",
    "   task_id='create_job_flow',\n",
    "   job_flow_overrides=JOB_FLOW_OVERRIDES,\n",
    "   aws_conn_id='aws_default',\n",
    "   emr_conn_id='emr_default',\n",
    "   dag=dag\n",
    ")\n",
    "\n",
    "PRE_STEP = [\n",
    "    {\n",
    "        'Name': 'spark_job_01',\n",
    "        #'ActionOnFailure': 'TERMINATE_CLUSTER',\n",
    "        'HadoopJarStep': {\n",
    "            'Jar': 'command-runner.jar',\n",
    "            'Args': ['spark-submit',\n",
    "                      '--deploy-mode', 'cluster',\n",
    "                      '--master', 'yarn',\n",
    "                      's3://[bucket_name]/spark_job_01.py']\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "step_adder_pre_step = EmrAddStepsOperator(\n",
    "    task_id='pre_step',\n",
    "    job_flow_id=\"{{ task_instance.xcom_pull('create_job_flow', key='return_value') }}\",\n",
    "    aws_conn_id='aws_default',\n",
    "    steps=PRE_STEP,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "step_checker_pre = EmrStepSensor(\n",
    "        task_id='watch_step_of_pre',\n",
    "        job_flow_id=\"{{ task_instance.xcom_pull('create_job_flow', key='return_value') }}\",\n",
    "        step_id=\"{{ task_instance.xcom_pull('pre_step', key='return_value')[0] }}\",\n",
    "        aws_conn_id='aws_default',\n",
    "        dag=dag\n",
    " )\n",
    "\n",
    "ACTUAL_STEP = [\n",
    "    {\n",
    "        'Name': 'spark_job_03',\n",
    "         # 'ActionOnFailure': 'TERMINATE_CLUSTER',\n",
    "        'HadoopJarStep': {\n",
    "            'Jar': 'command-runner.jar',\n",
    "            'Args': ['spark-submit',\n",
    "                      '--deploy-mode', 'cluster',\n",
    "                      '--master', 'yarn',\n",
    "                      's3://[bucket_name]/spark_job_03.py']\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "step_adder_actual_step = EmrAddStepsOperator(\n",
    "    task_id='actual_step',\n",
    "    job_flow_id=\"{{ task_instance.xcom_pull('create_job_flow', key='return_value') }}\",\n",
    "    aws_conn_id='aws_default',\n",
    "    steps=ACTUAL_STEP,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "step_checker_actual = EmrStepSensor(\n",
    "        task_id='watch_step_of_actual',\n",
    "        job_flow_id=\"{{ task_instance.xcom_pull('create_job_flow', key='return_value') }}\",\n",
    "        step_id=\"{{ task_instance.xcom_pull('actual_step', key='return_value')[0] }}\",\n",
    "        aws_conn_id='aws_default',\n",
    "        dag=dag\n",
    ")\n",
    "\n",
    "cluster_remover = EmrTerminateJobFlowOperator(\n",
    "    task_id='remove_cluster',\n",
    "    job_flow_id=\"{{ task_instance.xcom_pull('create_job_flow', key='return_value') }}\",\n",
    "    aws_conn_id='aws_default',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "cluster_creator >> step_adder_pre_step >> step_checker_pre >> cluster_remover\n",
    "cluster_creator >> step_adder_actual_step >> step_checker_actual >> cluster_remover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAG 실행결과\n",
    "\n",
    "![result](https://user-images.githubusercontent.com/41605276/95938212-5d287780-0e14-11eb-813d-c3f98829a6c9.png)\n",
    "\n",
    "[위에 코드에서 참고사항]\n",
    "\n",
    "- 스케쥴링 방법 \n",
    "\n",
    "한국시간으로 현재시각이 오전 11시 50분인데 11시 55분 부터 1시간마다 스케쥴링을 걸어주고 싶다고 하면\n",
    "\n",
    "Airflow는 UTC 기준이기 때문에 AIrflow에서 시간은 오전 2시 50분인 것이다.\n",
    "\n",
    "결론적으로 위에 코드에서 `'start_date': datetime(2020,10,14, 1, 55)`로 해주면\n",
    "\n",
    "11시 55분부터 1시간씩 해당 DAG가 돌것이다.\n",
    "\n",
    "\n",
    "- spark_job_01.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ABC data test\").getOrCreate()\n",
    "\n",
    "_list = ['A','B','C']\n",
    "\n",
    "for elem in _list:\n",
    "    \n",
    "    df = spark.read.option(\"header\",\"true\").parquet(\"s3a://[bucket_name]/testdata/merge_data/*.parquet\")\n",
    "    \n",
    "    df = df.withColumnRenamed(\"event_time\",\"event_time_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"event_type\",\"event_type_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"product_id\",\"product_id_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"category_id\",\"category_id_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"category_code\",\"category_code_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"brand\",\"brand_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"price\",\"price_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"user_id\",\"user_id_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"year\",\"year_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"user_session\",\"user_session_{}\".format(elem))\\\n",
    "           .withColumn(\"id\", F.monotonically_increasing_id())\n",
    "    \n",
    "    df.write.option(\"header\", \"true\").csv(\"s3a://[bucket_name]/{}\".format(elem))\n",
    "    \n",
    "    \n",
    "df.show()\n",
    "\n",
    "print(\"record count : \", df.count())\n",
    "\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spark_job_03.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ABC data test\").getOrCreate()\n",
    "\n",
    "_list = ['D','E','F']\n",
    "\n",
    "for elem in _list:\n",
    "    \n",
    "    df = spark.read.option(\"header\",\"true\").parquet(\"s3a://[bucket_name]/testdata/merge_data/*.parquet\")\n",
    "    \n",
    "    df = df.withColumnRenamed(\"event_time\",\"event_time_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"event_type\",\"event_type_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"product_id\",\"product_id_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"category_id\",\"category_id_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"category_code\",\"category_code_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"brand\",\"brand_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"price\",\"price_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"user_id\",\"user_id_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"year\",\"year_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"user_session\",\"user_session_{}\".format(elem))\\\n",
    "           .withColumn(\"id\", F.monotonically_increasing_id())\n",
    "\n",
    "    df.write.option(\"header\", \"true\").csv(\"s3a://[bucket_name]/{}\".format(elem))\n",
    "    \n",
    "df.show()\n",
    "\n",
    "print(\"record count : \", df.count())\n",
    "\n",
    "spark.catalog.clearCache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
