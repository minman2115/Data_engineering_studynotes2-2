{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "Data_Engineering_TIL(20200823)\n",
    "\n",
    "\n",
    "### 실습 목적\n",
    "\n",
    "SQL query 위주로 구성된 DAG jobflow를 EMR pyspark code로 구현하는 예시\n",
    "\n",
    "- AS-IS : SQL query 위주로 구성된 외부시스템의 DAG jobflow\n",
    "\n",
    "\n",
    "- TO-BE : EMR pyspark code\n",
    "\n",
    "\n",
    "### AS-IS의 DAG jobflow 형태\n",
    "\n",
    "아래와 같이 데이터를 로딩해서 SQL Query를 위주로 데이터를 처리하여 다시 저장하는 일련의 jobflow 형태\n",
    "\n",
    "![a](https://user-images.githubusercontent.com/41605276/90974795-56465c80-e569-11ea-88a8-eb2f31c66645.png)\n",
    "\n",
    "### TO-BE의 EMR pyspark code 예시\n",
    "\n",
    "- EMR 테스트 환경\n",
    "\n",
    "1) EMR 버전 : emr-5.30.1 (spark 2.4.5)\n",
    "\n",
    "2) 마스터 1대, 코어 10대\n",
    "\n",
    "3) 사양 : 일괄 r5.4xlarge(16vCPU, 128GB RAM)\n",
    "\n",
    "#### STEP 1) AS-IS의 형태를 TO-BE형태의 pyspark code형태로 튜닝없이 그대로 마이그레이션\n",
    "\n",
    "- example_01.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import boto3\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# python code start time check\n",
    "start = time.time()\n",
    "\n",
    "DF_NAME = 'example_01'\n",
    "s3_bucket = 'minman'\n",
    "s3_sql_file = 'emr/example.sql'\n",
    "hdfs_temp_dir = \"/tmp/spark/temp\"\n",
    "\n",
    "def sqlfile_read(bucketname, filename):\n",
    "    '''\n",
    "    [running process]\n",
    "    step 1) data file from s3\n",
    "    step 2) read sql commands\n",
    "    step 3) save sql commands in python list variable(sqlcommand_list)\n",
    "    '''\n",
    "    query_list = []\n",
    "    \n",
    "    # Open and read the file as a single buffer\n",
    "    s3 = boto3.resource('s3')\n",
    "    content_object = s3.Object(bucketname, filename)\n",
    "    sql_file = content_object.get()['Body'].read().decode('utf-8')\n",
    "\n",
    "    # all SQL commands (split on ';')\n",
    "    sqlCommands = sql_file.split(';')\n",
    "\n",
    "    # make list from sql commands\n",
    "    for command in sqlCommands:\n",
    "        query_list.append(command)\n",
    "        \n",
    "    return query_list\n",
    "\n",
    "sqlcommand_list = sqlfile_read(s3_bucket,s3_sql_file)\n",
    "spark = SparkSession.builder.appName(DF_NAME).getOrCreate()\n",
    "\n",
    "# FID : a_table\n",
    "# FName : a_table\n",
    "a_table = spark.read.option('header','true').parquet('s3://minman/prefix1/prefix_a/*.parquet')\n",
    "a_table.createOrReplaceTempView('a_table')\n",
    "\n",
    "# FID : b_table\n",
    "# FName : b_table\n",
    "b_table = spark.read.option('header','true').parquet('s3://minman/prefix1/prefix_b/*.parquet')\n",
    "b_table.createOrReplaceTempView('b_table')\n",
    "\n",
    "# FID : c_table\n",
    "# FName : c_table\n",
    "c_table = spark.read.option('header','true').parquet('s3://minman/prefix1/prefix_c/*.parquet')\n",
    "c_table.createOrReplaceTempView('c_table')\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "# FID : query_1\n",
    "# FName : query_1\n",
    "query_1 = spark.sql('{}'.format(sqlcommand_list[0]))\n",
    "query_1.createOrReplaceTempView('query_1')\n",
    "\n",
    "# FID : query_2\n",
    "# FName : query_2\n",
    "query_2 = spark.sql('{}'.format(sqlcommand_list[1]))\n",
    "query_2.createOrReplaceTempView('query_2')\n",
    "\n",
    "# FID : query_3\n",
    "# FName : query_3\n",
    "query_3 = spark.sql('{}'.format(sqlcommand_list[2]))\n",
    "query_3.createOrReplaceTempView('query_3')\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "# FID : query_6\n",
    "# FName : query_6\n",
    "query_3 = spark.sql('{}'.format(sqlcommand_list[5]))\n",
    "query_3.createOrReplaceTempView('query_4')\n",
    "\n",
    "# FID : d_table\n",
    "# FName : d_table\n",
    "d_table = spark.read.option('header','true').parquet('s3://minman/prefix1/prefix_d/*.parquet')\n",
    "d_table.createOrReplaceTempView('d_table')\n",
    "\n",
    "# FID : query_7\n",
    "# FName : query_7\n",
    "query_7 = spark.sql('{}'.format(sqlcommand_list[6]))\n",
    "query_7.createOrReplaceTempView('query_7')\n",
    "\n",
    "# FID : query_8\n",
    "# FName : query_8\n",
    "query_8 = spark.sql('{}'.format(sqlcommand_list[7]))\n",
    "query_8.createOrReplaceTempView('query_8')\n",
    "\n",
    "...\n",
    "\n",
    "# FID : query_10\n",
    "# FName : query_10\n",
    "query_10 = spark.sql('{}'.format(sqlcommand_list[9]))\n",
    "query_10.createOrReplaceTempView('query_10')\n",
    "\n",
    "# FID : query_11\n",
    "# FName : query_11\n",
    "# used: 1\n",
    "query_11 = spark.sql('{}'.format(sqlcommand_list[10]))\n",
    "query_11.createOrReplaceTempView('query_11')\n",
    "\n",
    "...\n",
    "\n",
    "# FID : query_13\n",
    "# FName : query_13\n",
    "query_13 = spark.sql('{}'.format(sqlcommand_list[12]))\n",
    "query_13.createOrReplaceTempView('query_13')\n",
    "\n",
    "# FID : query_14\n",
    "# FName : query_14\n",
    "query_14 = spark.sql('{}'.format(sqlcommand_list[13]))\n",
    "query_14.createOrReplaceTempView('query_14')\n",
    "\n",
    "# FID : query_15\n",
    "# FName : query_15\n",
    "query_15 = spark.sql('{}'.format(sqlcommand_list[14]))\n",
    "query_15.createOrReplaceTempView('query_15')\n",
    "\n",
    "# FID : query_16\n",
    "# FName : query_16\n",
    "query_16 = spark.sql('{}'.format(sqlcommand_list[15]))\n",
    "query_16.createOrReplaceTempView('query_16')\n",
    "\n",
    "# FID : query_17\n",
    "# FName : query_17\n",
    "# used: 1\n",
    "query_17 = spark.sql('{}'.format(sqlcommand_list[16]))\n",
    "query_17.createOrReplaceTempView('query_17')\n",
    "\n",
    "# FID : query_18\n",
    "# FName : query_18\n",
    "query_17 = spark.sql('{}'.format(sqlcommand_list[17]))\n",
    "query_17.createOrReplaceTempView('query_18')\n",
    "\n",
    "# FID : query_19\n",
    "# FName : query_19\n",
    "# used: 1\n",
    "query_19 = spark.sql('{}'.format(sqlcommand_list[18]))\n",
    "query_19.createOrReplaceTempView('query_19')\n",
    "\n",
    "...\n",
    "\n",
    "# FID : query_20\n",
    "# FName : query_20\n",
    "query_20 = spark.sql('{}'.format(sqlcommand_list[19]))\n",
    "query_20.createOrReplaceTempView('query_20')\n",
    "\n",
    "# FID : query_21\n",
    "# FName : query_21\n",
    "query_21 = spark.sql('{}'.format(sqlcommand_list[20]))\n",
    "query_21.createOrReplaceTempView('query_21')\n",
    "\n",
    "# FID : f_table\n",
    "# FName : f_table\n",
    "f_table = spark.read.option('header','true').parquet('s3://minman/prefix1/prefix_f/*.parquet')\n",
    "f_table.createOrReplaceTempView('f_table')\n",
    "\n",
    "# FID : g_table\n",
    "# FName : g_table\n",
    "g_table = spark.read.option('header','true').parquet('s3://minman/prefix1/prefix_g/*.parquet')\n",
    "g_table.createOrReplaceTempView('g_table')\n",
    "\n",
    "\n",
    "# FID : query_22\n",
    "# FName : query_22\n",
    "query_22 = spark.sql('{}'.format(sqlcommand_list[21]))\n",
    "query_22.createOrReplaceTempView('query_22')\n",
    "\n",
    "# FID : query_23\n",
    "# FName : query_23\n",
    "query_23 = spark.sql('{}'.format(sqlcommand_list[22]))\n",
    "query_23.createOrReplaceTempView('query_23')\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "# FID : query_25\n",
    "# FName : query_25\n",
    "query_25 = spark.sql('{}'.format(sqlcommand_list[24]))\n",
    "query_25.createOrReplaceTempView('query_25')\n",
    "\n",
    "# FID : h_table\n",
    "# FName : h_table\n",
    "h_table = spark.read.option('header','true').parquet('s3://minman/prefix1/prefix_h/*.parquet')\n",
    "h_table.createOrReplaceTempView('h_table')\n",
    "\n",
    "...\n",
    "\n",
    "# FID : python_code_01\n",
    "# FName : python_code_01\n",
    "python_code_01 = query_24.filter(df['City']=='Manchester')\n",
    "python_code_01.createOrReplaceTempView('python_code_01')\n",
    "\n",
    "...\n",
    "\n",
    "# FID : query_27\n",
    "# FName : query_27\n",
    "query_27 = spark.sql('{}'.format(sqlcommand_list[26]))\n",
    "query_27.createOrReplaceTempView('query_27')\n",
    "\n",
    "# FID : python_code_02\n",
    "# FName : python_code_02\n",
    "# Python Script_NA 0처리\n",
    "input_dataframe = query_27\n",
    "temp_dataframe = input_dataframe.na.fill(0)\n",
    "outdata_dataframe = temp_dataframe.na.fill('')\n",
    "outdata_dataframe.createOrReplaceTempView('python_code_02')\n",
    "\n",
    "...\n",
    "\n",
    "# FID : query_29\n",
    "# FName : query_29\n",
    "query_29 = spark.sql('{}'.format(sqlcommand_list[28]))\n",
    "query_29.createOrReplaceTempView('query_29')\n",
    "\n",
    "# FID : Unload\n",
    "# FName : Unload\n",
    "query_29.write.mode('overwrite').option('header','true').parquet('s3://minman/emr/result')\n",
    "\n",
    "# python code endtime check\n",
    "endTimeQuery = time.clock()\n",
    "\n",
    "# python code running time print\n",
    "print(\"code running time (sec):\", time.time() - start,', code finish time (GMT) : ',datetime.now().strftime('%Y-%m-%d %H:%M'))\n",
    "\n",
    "# remove spark dataframe cache\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 작성한 example_01.py를 EMR의 마스터 노드에서 `spark-submit example_01.py` 명령어로 실행하면 AS-IS의 형태에서 처리되는 일련의 flow를 동일하게 재현할 수 있다.\n",
    "\n",
    "** 위에 python 코드에서 참조하는 sql파일 내용 형태\n",
    "\n",
    "- example.sql\n",
    "\n",
    ";(세미콜론)을 기준으로 쿼리문을 구분하였음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- FID : query_1\n",
    "-- FName : query_1\n",
    "SQL QUERY ;\n",
    "-- FID : query_2\n",
    "-- FName : query_2\n",
    "SQL QUERY ;\n",
    "-- FID : query_3\n",
    "-- FName : query_3\n",
    "SQL QUERY ;\n",
    "\n",
    "...\n",
    "\n",
    "-- FID : query_29\n",
    "-- FName : query_29\n",
    "SQL QUERY ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2) pyspark code형태로 튜닝없이 그대로 마이그레이션한 코드의 실행 퍼포먼스 향상을 위한 튜닝\n",
    "\n",
    "1) example_01.py code 문제점\n",
    "\n",
    "\n",
    "- 특정 sql query 소시지가 이전 step에서 수행된 sql query 소시지를 참조하는 경우가 대부분임. 이런경우 참조하는 이전 step에서의 sql query 소시지까지 한번에 합쳐서 실행하는 구조\n",
    "\n",
    "\n",
    "- 따라서 불필요하게 복잡한 sql 연산이 반복적으로 수행되므로 성능이 떨어지고, 디버깅을 트래킹하기에도 어려움  \n",
    "\n",
    "\n",
    "2) 튜닝 포인트\n",
    "\n",
    "\n",
    "- 여러번 참조되는 소시지의 dataframe의 경우 hdfs에 해당 dataframe을 save한 다음 그거를 다시 로딩하는 로직을 추가함\n",
    "\n",
    "\n",
    "- 이는 무슨의미냐면 특정 sql query 소시지의 dataframe을 다수의 다른 소시지에서 참조할 경우 해당 dataframe의 lineage(RDD가 연산되면서 변경되는 것에 대한 그래프 형태의 트리계보)를 단절시켜버림\n",
    "\n",
    "\n",
    "- 이를 어떻게 구현하냐면 아래예시와 같이 여러번 참조되는 소시지의 dataframe을 hdfs에 임시로 save한 다음 다시 로딩시켜버림\n",
    "\n",
    "\n",
    "\n",
    "예시)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_and_rad(sparkSession, df, df_path_name):\n",
    "    '''\n",
    "    [running process]\n",
    "   \n",
    "    step 1) dataframe write to HDFS\n",
    "    step 2) read dataframe from HDFS\n",
    "    '''\n",
    "    df.write.mode(\"overwrite\").parquet(\"{}/{}\".format(hdfs_temp_dir, df_path_name))\n",
    "    return sparkSession.read.parquet(\"{}/{}\".format(hdfs_temp_dir, df_path_name))\n",
    "\n",
    "# FID : query_example\n",
    "# FName : query_example\n",
    "# used(해당 데이터프레임의 참조횟수): 3\n",
    "query_example = spark.sql('{}'.format(sqlcommand_list[17]))\n",
    "query_example = save_df_and_rad(spark, query_example , \" query_example \")\n",
    "query_example .createOrReplaceTempView(' query_example ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 자주 참고되는 dataframe의 data lineage가 길게 늘어지게 되면 연산량이 많아져 코드 전체의 관점에서 퍼포먼스가 느려질수 밖에 없음. 따라서 이런 dataframe들은 HDFS에 저장하고 다시 load 하는 로직을 추가함으로써 불필요한 연산을 감소시킴 \n",
    "\n",
    "\n",
    "- 여러번 참조되는 일부 dataframe은 cache처리\n",
    "\n",
    "\n",
    "3) 위와 같은 내용이 반영된 example_02.py code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import boto3\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# python code start time check\n",
    "start = time.time()\n",
    "\n",
    "DF_NAME = 'example_02'\n",
    "s3_bucket = 'minman'\n",
    "s3_sql_file = 'emr/example.sql'\n",
    "hdfs_temp_dir = \"/tmp/spark/temp\"\n",
    "\n",
    "def sqlfile_read(bucketname, filename):\n",
    "    '''\n",
    "    [running process]\n",
    "    step 1) data file from s3\n",
    "    step 2) read sql commands\n",
    "    step 3) save sql commands in python list variable(sqlcommand_list)\n",
    "    '''\n",
    "    query_list = []\n",
    "    \n",
    "    # Open and read the file as a single buffer\n",
    "    s3 = boto3.resource('s3')\n",
    "    content_object = s3.Object(bucketname, filename)\n",
    "    sql_file = content_object.get()['Body'].read().decode('utf-8')\n",
    "\n",
    "    # all SQL commands (split on ';')\n",
    "    sqlCommands = sql_file.split(';')\n",
    "\n",
    "    # make list from sql commands\n",
    "    for command in sqlCommands:\n",
    "        query_list.append(command)\n",
    "        \n",
    "    return query_list\n",
    "\n",
    "def save_df_and_rad(sparkSession, df, df_path_name):\n",
    "    '''\n",
    "    [running process]\n",
    "    \n",
    "    step 1) dataframe write to HDFS\n",
    "    step 2) read dataframe from HDFS\n",
    "    '''\n",
    "    df.write.mode(\"overwrite\").parquet(\"{}/{}\".format(hdfs_temp_dir, df_path_name))\n",
    "    return sparkSession.read.parquet(\"{}/{}\".format(hdfs_temp_dir, df_path_name))\n",
    "\n",
    "sqlcommand_list = sqlfile_read(s3_bucket,s3_sql_file)\n",
    "\n",
    "spark = SparkSession.builder.appName(DF_NAME).getOrCreate()\n",
    "\n",
    "# FID : a_table\n",
    "# FName : a_table\n",
    "a_table = spark.read.option('header','true').parquet('s3://minman/prefix1/prefix_a/*.parquet')\n",
    "a_table.createOrReplaceTempView('a_table')\n",
    "\n",
    "# FID : b_table\n",
    "# FName : b_table\n",
    "b_table = spark.read.option('header','true').parquet('s3://minman/prefix1/prefix_b/*.parquet')\n",
    "b_table.createOrReplaceTempView('b_table')\n",
    "\n",
    "# FID : c_table\n",
    "# FName : c_table\n",
    "c_table = spark.read.option('header','true').parquet('s3://minman/prefix1/prefix_c/*.parquet')\n",
    "c_table.createOrReplaceTempView('c_table')\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "# FID : query_1\n",
    "# FName : query_1\n",
    "# used: 2번\n",
    "query_1 = spark.sql('{}'.format(sqlcommand_list[0]))\n",
    "query_1.createOrReplaceTempView('query_1')\n",
    "\n",
    "# FID : query_2\n",
    "# FName : query_2\n",
    "# used: 1번\n",
    "query_2 = spark.sql('{}'.format(sqlcommand_list[1]))\n",
    "query_2.createOrReplaceTempView('query_2')\n",
    "\n",
    "# FID : query_3\n",
    "# FName : query_3\n",
    "# used: 1번\n",
    "query_3 = spark.sql('{}'.format(sqlcommand_list[2]))\n",
    "query_3.createOrReplaceTempView('query_3')\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "# FID : query_6\n",
    "# FName : query_6\n",
    "# used: 1번\n",
    "query_3 = spark.sql('{}'.format(sqlcommand_list[5]))\n",
    "query_3.createOrReplaceTempView('query_4')\n",
    "\n",
    "# FID : d_table\n",
    "# FName : d_table\n",
    "# used: 5 \n",
    "d_table = spark.read.option('header','true').parquet('s3://minman/prefix1/prefix_d/*.parquet')\n",
    "# 다른 SQL 쿼리가 d_table을 5번 사용하기 때문에 아래와 같이 cache() 처리하였음\n",
    "d_table.cache()\n",
    "d_table.createOrReplaceTempView('d_table')\n",
    "\n",
    "# FID : query_7\n",
    "# FName : query_7\n",
    "# used: 1\n",
    "# 작은 여러 테이블을 join하는 쿼리\n",
    "query_7 = spark.sql('{}'.format(sqlcommand_list[6]))\n",
    "query_7.createOrReplaceTempView('query_7')\n",
    "\n",
    "# FID : query_8\n",
    "# FName : query_8\n",
    "# used: 3\n",
    "# in 절에 여러 조건을 찾으나 테이블 자체가 작음\n",
    "query_8 = spark.sql('{}'.format(sqlcommand_list[7]))\n",
    "query_8 = save_df_and_rad(spark, query_8, \"query_8\")\n",
    "query_8.createOrReplaceTempView('query_8')\n",
    "\n",
    "...\n",
    "\n",
    "# FID : query_10\n",
    "# FName : query_10\n",
    "# used: 2\n",
    "query_10 = spark.sql('{}'.format(sqlcommand_list[9]))\n",
    "query_10 = save_df_and_rad(spark, query_13, \"query_10\")\n",
    "query_10.createOrReplaceTempView('query_10')\n",
    "\n",
    "# FID : query_11\n",
    "# FName : query_11\n",
    "# used: 1\n",
    "query_11 = spark.sql('{}'.format(sqlcommand_list[10]))\n",
    "query_11.createOrReplaceTempView('query_11')\n",
    "\n",
    "...\n",
    "\n",
    "# FID : query_13\n",
    "# FName : query_13\n",
    "# used: 3\n",
    "query_13 = spark.sql('{}'.format(sqlcommand_list[12]))\n",
    "query_13 = save_df_and_rad(spark, query_13, \"query_13\")\n",
    "query_13.createOrReplaceTempView('query_13')\n",
    "\n",
    "# FID : query_14\n",
    "# FName : query_14\n",
    "# used: 2\n",
    "query_14 = spark.sql('{}'.format(sqlcommand_list[13]))\n",
    "query_14 = save_df_and_rad(spark, query_14, \"query_14\")\n",
    "query_14.createOrReplaceTempView('query_14')\n",
    "\n",
    "# FID : query_15\n",
    "# FName : query_15\n",
    "# used: 7\n",
    "query_15 = spark.sql('{}'.format(sqlcommand_list[14]))\n",
    "query_15 = save_df_and_rad(spark, query_15, \"query_15\")\n",
    "query_15.createOrReplaceTempView('query_15')\n",
    "\n",
    "# FID : query_16\n",
    "# FName : query_16\n",
    "# used: 3\n",
    "query_16 = spark.sql('{}'.format(sqlcommand_list[15]))\n",
    "query_16 = save_df_and_rad(spark, query_16, \"query_16\")\n",
    "query_16.createOrReplaceTempView('query_16')\n",
    "\n",
    "# FID : query_17\n",
    "# FName : query_17\n",
    "# used: 1\n",
    "query_17 = spark.sql('{}'.format(sqlcommand_list[16]))\n",
    "query_17.createOrReplaceTempView('query_17')\n",
    "\n",
    "# FID : query_18\n",
    "# FName : query_18\n",
    "# used: 1\n",
    "query_17 = spark.sql('{}'.format(sqlcommand_list[17]))\n",
    "query_17.createOrReplaceTempView('query_18')\n",
    "\n",
    "# FID : query_19\n",
    "# FName : query_19\n",
    "# used: 1\n",
    "query_19 = spark.sql('{}'.format(sqlcommand_list[18]))\n",
    "query_19.createOrReplaceTempView('query_19')\n",
    "\n",
    "...\n",
    "\n",
    "# FID : query_20\n",
    "# FName : query_20\n",
    "# used: 1\n",
    "query_20 = spark.sql('{}'.format(sqlcommand_list[19]))\n",
    "query_20.createOrReplaceTempView('query_20')\n",
    "\n",
    "# FID : query_21\n",
    "# FName : query_21\n",
    "# used: 1\n",
    "query_21 = spark.sql('{}'.format(sqlcommand_list[20]))\n",
    "query_21.createOrReplaceTempView('query_21')\n",
    "\n",
    "# FID : f_table\n",
    "# FName : f_table\n",
    "# used: 3\n",
    "f_table = spark.read.option('header','true').parquet('s3://minman/prefix1/prefix_f/*.parquet')\n",
    "f_table.cache()\n",
    "f_table.createOrReplaceTempView('f_table')\n",
    "\n",
    "# FID : g_table\n",
    "# FName : g_table\n",
    "# used: 3\n",
    "g_table = spark.read.option('header','true').parquet('s3://minman/prefix1/prefix_g/*.parquet')\n",
    "g_table.cache()\n",
    "g_table.createOrReplaceTempView('g_table')\n",
    "\n",
    "\n",
    "# FID : query_22\n",
    "# FName : query_22\n",
    "# used: 1\n",
    "query_22 = spark.sql('{}'.format(sqlcommand_list[21]))\n",
    "query_22.createOrReplaceTempView('query_22')\n",
    "\n",
    "# FID : query_23\n",
    "# FName : query_23\n",
    "# used: 3\n",
    "query_23 = spark.sql('{}'.format(sqlcommand_list[22]))\n",
    "query_23 = save_df_and_rad(spark, query_23, \"query_23\")\n",
    "query_23.createOrReplaceTempView('query_23')\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "# FID : query_25\n",
    "# FName : query_25\n",
    "# used: 3\n",
    "query_25 = spark.sql('{}'.format(sqlcommand_list[24]))\n",
    "query_25 = save_df_and_rad(spark, query_25, \"query_25\")\n",
    "query_25.createOrReplaceTempView('query_25')\n",
    "\n",
    "# FID : h_table\n",
    "# FName : h_table\n",
    "# used: 2\n",
    "h_table = spark.read.option('header','true').parquet('s3://minman/prefix1/prefix_h/*.parquet')\n",
    "h_table.createOrReplaceTempView('h_table')\n",
    "\n",
    "...\n",
    "\n",
    "# FID : python_code_01\n",
    "# FName : python_code_01\n",
    "python_code_01 = query_24.filter(df['City']=='Manchester')\n",
    "python_code_01.createOrReplaceTempView('python_code_01')\n",
    "\n",
    "...\n",
    "\n",
    "# FID : query_27\n",
    "# FName : query_27\n",
    "# used: 1\n",
    "query_27 = spark.sql('{}'.format(sqlcommand_list[26]))\n",
    "query_27.createOrReplaceTempView('query_27')\n",
    "\n",
    "# FID : python_code_02\n",
    "# FName : python_code_02\n",
    "# Python Script_NA 0처리\n",
    "input_dataframe = query_27\n",
    "temp_dataframe = input_dataframe.na.fill(0)\n",
    "outdata_dataframe = temp_dataframe.na.fill('')\n",
    "outdata_dataframe.createOrReplaceTempView('python_code_02')\n",
    "\n",
    "...\n",
    "\n",
    "# FID : query_29\n",
    "# FName : query_29\n",
    "query_29 = spark.sql('{}'.format(sqlcommand_list[28]))\n",
    "query_29.createOrReplaceTempView('query_29')\n",
    "\n",
    "# FID : Unload\n",
    "# FName : Unload\n",
    "query_29.write.mode('overwrite').option('header','true').parquet('s3://minman/emr/result')\n",
    "\n",
    "# python code endtime check\n",
    "endTimeQuery = time.clock()\n",
    "\n",
    "# python code running time print\n",
    "print(\"code running time (sec):\", time.time() - start,', code finish time (GMT) : ',datetime.now().strftime('%Y-%m-%d %H:%M'))\n",
    "\n",
    "# remove spark dataframe cache\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 작성한 example_02.py를 EMR의 마스터 노드에서 아래와 같은 명령어로 실행하면 AS-IS의 형태에서 처리되는 일련의 flow를 동일하게 재현하면서 성능은 더 향상된 결과를 얻을 수 있다.\n",
    "\n",
    "`spark-submit \\\n",
    "--master yarn \\\n",
    "--deploy-mode client \\\n",
    "--driver-memory 4g \\\n",
    "--num-executors 17 \\\n",
    "--executor-cores 5 \\\n",
    "--executor-memory 20g \\\n",
    "--conf spark.dynamicAllocation.enabled=false \\\n",
    "--conf spark.serializer=org.apache.spark.serializer.KryoSerializer \\\n",
    "--conf spark.default.parallelism=170 \\\n",
    "--conf spark.sql.crossJoin.enabled=true \\\n",
    "--conf \"spark.driver.extraJavaOptions=-XX:+UseG1GC\" \\\n",
    "--conf \"spark.executor.extraJavaOptions=-XX:+UseG1GC\" \\\n",
    " example_02.py`\n",
    " \n",
    " \n",
    "** spark application 차원에서 옵션\n",
    "\n",
    "driver cores: 1\n",
    "\n",
    "\n",
    "driver memory: 4GB\n",
    "\n",
    "\n",
    "executor cores: 85 (executor: 17, executor-core: 5 )\n",
    "\n",
    "\n",
    "executor memory: 340GB (executor 당 4GB)\n",
    "\n",
    "\n",
    "** spark 메모리 설정 참고자료\n",
    "\n",
    "URL : https://aws.amazon.com/ko/blogs/korea/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
