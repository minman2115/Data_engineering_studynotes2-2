{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "Data_Engineering_TIL(20201018)\n",
    "\n",
    "Youtube 채널 'min zzang' 님의 '아파치 스파크 Standalone 설치 및 구성 1&2' 을 공부한 내용입니다.\n",
    "\n",
    "** URL : https://youtu.be/mXUOxPETbm8\n",
    "\n",
    "[실습목표]\n",
    "\n",
    "\n",
    "가상머신 1대에 spark standalone mode를 설치한다.\n",
    "\n",
    "\n",
    "[실습내용]\n",
    "\n",
    "step 1) EC2 생성\n",
    "\n",
    "\n",
    "- 운영체제 : Amazon linux 2\n",
    "\n",
    "\n",
    "- 사양 : m5.4xlarge, 30GB 볼륨\n",
    "\n",
    "\n",
    "step 2) SSH로 접속해서 아래와 같은 명령어 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-1-10-126 ~]$ sudo yum update -y\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 ~]$ yum list java*jdk-devel\n",
    "Loaded plugins: extras_suggestions, langpacks, priorities, update-motd\n",
    "Available Packages\n",
    "java-1.7.0-openjdk-devel.x86_64                                 1:1.7.0.261-2.6.22.2.amzn2.0.1                                  amzn2-core\n",
    "java-1.8.0-openjdk-devel.x86_64                                 1:1.8.0.265.b01-1.amzn2.0.1                                     amzn2-core\n",
    "\n",
    "# 자바 설치\n",
    "[ec2-user@ip-10-1-10-126 ~]$ sudo yum install java-1.8.0-openjdk-devel.x86_64 -y\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 ~]$ java -version\n",
    "openjdk version \"1.8.0_265\"\n",
    "OpenJDK Runtime Environment (build 1.8.0_265-b01)\n",
    "OpenJDK 64-Bit Server VM (build 25.265-b01, mixed mode)\n",
    "\n",
    "# javac 위치확인\n",
    "[ec2-user@ip-10-1-10-126 ~]$ readlink -f /usr/bin/javac\n",
    "/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.265.b01-1.amzn2.0.1.x86_64/bin/javac\n",
    "\n",
    "# 메모리 가용자원 확인\n",
    "[ec2-user@ip-10-1-10-126 ~]$ free -g\n",
    "              total        used        free      shared  buff/cache   available\n",
    "Mem:             62           0          60           0           1          61\n",
    "Swap:             0           0           0\n",
    "\n",
    "# cpu 코어수 확인\n",
    "[ec2-user@ip-10-1-10-126 ~]$ grep -c processor /proc/cpuinfo\n",
    "16\n",
    "\n",
    "# spark 다운로드\n",
    "[ec2-user@ip-10-1-10-126 ~]$ wget https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz\n",
    "--2020-10-15 09:03:09--  https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz\n",
    "Resolving archive.apache.org (archive.apache.org)... 138.201.131.134, 2a01:4f8:172:2ec5::2\n",
    "Connecting to archive.apache.org (archive.apache.org)|138.201.131.134|:443... connected.\n",
    "HTTP request sent, awaiting response... 200 OK\n",
    "Length: 195636829 (187M) [application/x-gzip]\n",
    "Saving to: ‘spark-2.1.0-bin-hadoop2.7.tgz’\n",
    "\n",
    "100%[================================================================================================>] 195,636,829 5.15MB/s   in 38s\n",
    "\n",
    "2020-10-15 09:03:48 (4.93 MB/s) - ‘spark-2.1.0-bin-hadoop2.7.tgz’ saved [195636829/195636829]\n",
    "        \n",
    "[ec2-user@ip-10-1-10-126 ~]$ ls\n",
    "spark-2.1.0-bin-hadoop2.7.tgz\n",
    "\n",
    "# 다운로드한 spark 압축해제\n",
    "[ec2-user@ip-10-1-10-126 ~]$ tar -zxf spark-2.1.0-bin-hadoop2.7.tgz\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 ~]$ ls\n",
    "spark-2.1.0-bin-hadoop2.7  spark-2.1.0-bin-hadoop2.7.tgz\n",
    "\n",
    "# 자바와 spark PATH 설정을 해준다.\n",
    "[ec2-user@ip-10-1-10-126 ~]$ export PATH=$PATH:/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.265.b01-1.amzn2.0.1.x86_64/bin\n",
    "    \n",
    "[ec2-user@ip-10-1-10-126 ~]$ pwd\n",
    "/home/ec2-user\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 ~]$ export PATH=$PATH:/home/ec2-user/spark-2.1.0-bin-hadoop2.7/bin\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 ~]$ cd spark-2.1.0-bin-hadoop2.7/\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 spark-2.1.0-bin-hadoop2.7]$ ls\n",
    "bin  conf  data  examples  jars  LICENSE  licenses  NOTICE  python  R  README.md  RELEASE  sbin  yarn\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 spark-2.1.0-bin-hadoop2.7]$ cd conf\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 conf]$ ls\n",
    "docker.properties.template  log4j.properties.template    slaves.template               spark-env.sh.template\n",
    "fairscheduler.xml.template  metrics.properties.template  spark-defaults.conf.template\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 conf]$ cp spark-env.sh.template spark-env.sh\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 conf]$ ls\n",
    "docker.properties.template  log4j.properties.template    slaves.template               spark-env.sh\n",
    "fairscheduler.xml.template  metrics.properties.template  spark-defaults.conf.template  spark-env.sh.template\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 conf]$ sudo vim spark-env.sh\n",
    "# 아래내용 추가\n",
    "export SPARK_WORKER_INSTANCES=3\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 conf]$ cd ..\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 spark-2.1.0-bin-hadoop2.7]$ ls\n",
    "bin  conf  data  examples  jars  LICENSE  licenses  NOTICE  python  R  README.md  RELEASE  sbin  yarn\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 spark-2.1.0-bin-hadoop2.7]$ cd sbin\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 sbin]$ ls\n",
    "slaves.sh         start-history-server.sh         start-slave.sh          stop-master.sh                 stop-slaves.sh\n",
    "spark-config.sh   start-master.sh                 start-slaves.sh         stop-mesos-dispatcher.sh       stop-thriftserver.sh\n",
    "spark-daemon.sh   start-mesos-dispatcher.sh       start-thriftserver.sh   stop-mesos-shuffle-service.sh\n",
    "spark-daemons.sh  start-mesos-shuffle-service.sh  stop-all.sh             stop-shuffle-service.sh\n",
    "start-all.sh      start-shuffle-service.sh        stop-history-server.sh  stop-slave.sh\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 sbin]$ sh start-master.sh\n",
    "starting org.apache.spark.deploy.master.Master, logging to /home/ec2-user/spark-2.1.0-bin-hadoop2.7/logs/spark-ec2-user-org.apache.spark.deploy.master.Master-1-ip-10-1-10-126.ap-northeast-2.compute.internal.out\n",
    "\n",
    "# spark-shell가 잘 실행되는지 확인해본다.\n",
    "[ec2-user@ip-10-1-10-126 sbin]$ cd ..\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 spark-2.1.0-bin-hadoop2.7]$ ls\n",
    "bin  conf  data  examples  jars  LICENSE  licenses  logs  NOTICE  python  R  README.md  RELEASE  sbin  work  yarn\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 spark-2.1.0-bin-hadoop2.7]$ cd bin\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 bin]$ ls\n",
    "beeline      find-spark-home     metastore_db  pyspark.cmd      spark-class       sparkR       spark-shell       spark-sql          spark-submit.cmd\n",
    "beeline.cmd  load-spark-env.cmd  pyspark       run-example      spark-class2.cmd  sparkR2.cmd  spark-shell2.cmd  spark-submit\n",
    "derby.log    load-spark-env.sh   pyspark2.cmd  run-example.cmd  spark-class.cmd   sparkR.cmd   spark-shell.cmd   spark-submit2.cmd\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 bin]$ ./spark-shell\n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "20/10/18 10:57:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "20/10/18 10:57:37 WARN SparkConf:\n",
    "SPARK_WORKER_INSTANCES was detected (set to '3').\n",
    "This is deprecated in Spark 1.0+.\n",
    "\n",
    "Please instead use:\n",
    " - ./spark-submit with --num-executors to specify the number of executors\n",
    " - Or set SPARK_EXECUTOR_INSTANCES\n",
    " - spark.executor.instances to configure the number of instances in the spark config.\n",
    "\n",
    "20/10/18 10:57:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
    "20/10/18 10:57:41 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0\n",
    "20/10/18 10:57:41 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
    "20/10/18 10:57:42 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
    "Spark context Web UI available at http://10.1.10.126:4041\n",
    "Spark context available as 'sc' (master = local[*], app id = local-1603018658481).\n",
    "Spark session available as 'spark'.\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.1.0\n",
    "      /_/\n",
    "\n",
    "Using Scala version 2.11.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_265)\n",
    "Type in expressions to have them evaluated.\n",
    "Type :help for more information.\n",
    "\n",
    "scala> sc\n",
    "res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@64cec4d0\n",
    "    \n",
    "# 확인했으면 컨트롤+c 를 눌러 spark-shell을 빠져나간다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 마스터 노드 실행 후 웹브라우저를 열고 `[ec2_public_ip]:8080` 로 접속하면 아래 그림과 같은 화면이 전시될 것이다.\n",
    "\n",
    "그리고 해당화면에서 확인할 수 있는 주요정보는 다음과 같다.\n",
    "\n",
    "Spark Master at spark://ip-10-1-10-126.ap-northeast-2.compute.internal:7077\n",
    "\n",
    "\n",
    "URL: spark://ip-10-1-10-126.ap-northeast-2.compute.internal:7077\n",
    "\n",
    "\n",
    "REST URL: spark://ip-10-1-10-126.ap-northeast-2.compute.internal:6066 (cluster mode)\n",
    "\n",
    "\n",
    "Alive Workers: 0\n",
    "\n",
    "\n",
    "Cores in use: 0 Total, 0 Used\n",
    "\n",
    "\n",
    "Memory in use: 0.0 B Total, 0.0 B Used\n",
    "\n",
    "\n",
    "Applications: 0 Running, 0 Completed\n",
    "\n",
    "\n",
    "Drivers: 0 Running, 0 Completed\n",
    "\n",
    "\n",
    "Status: ALIVE\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/41605276/96364465-c4785b80-1175-11eb-9a04-243d5d37d82f.png)\n",
    "\n",
    "\n",
    "그리고 아래와 같이 슬레이브 노드도 스타트해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 슬레이브 실행시 뒤에 옵션이 붙는것에 유의한다.\n",
    "# sh start-slave.sh [방금 위에서 확인한 마스터노드 주소] -m [노드당 메모리] -c [노드당 코어수]\n",
    "# 이 가상머신의 메모리가 총 62G, 코어는 16 코어이므로 적당히 노드당 18기가, 코어는 4개씩으로 설정했다.\n",
    "# 슬레이브 노드수는 아까 spark-env.sh에서 설정한 3대가 생성된다.\n",
    "[ec2-user@ip-10-1-10-126 sbin]$ sh start-slave.sh spark://ip-10-1-10-126.ap-northeast-2.compute.internal:7077 -m 18g -c 4\n",
    "starting org.apache.spark.deploy.worker.Worker, logging to /home/ec2-user/spark-2.1.0-bin-hadoop2.7/logs/spark-ec2-user-org.apache.spark.deploy.worker.Worker-1-ip-10-1-10-126.ap-northeast-2.compute.internal.out\n",
    "starting org.apache.spark.deploy.worker.Worker, logging to /home/ec2-user/spark-2.1.0-bin-hadoop2.7/logs/spark-ec2-user-org.apache.spark.deploy.worker.Worker-2-ip-10-1-10-126.ap-northeast-2.compute.internal.out\n",
    "starting org.apache.spark.deploy.worker.Worker, logging to /home/ec2-user/spark-2.1.0-bin-hadoop2.7/logs/spark-ec2-user-org.apache.spark.deploy.worker.Worker-3-ip-10-1-10-126.ap-northeast-2.compute.internal.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런다음에 다시 마스터노드 WEB UI로 가보면 슬레이브 노드가 올라온것을 확인할 수 있다.\n",
    "\n",
    "Spark Master at spark://ip-10-1-10-126.ap-northeast-2.compute.internal:7077\n",
    "\n",
    "URL: spark://ip-10-1-10-126.ap-northeast-2.compute.internal:7077\n",
    "\n",
    "REST URL: spark://ip-10-1-10-126.ap-northeast-2.compute.internal:6066 (cluster mode)\n",
    "\n",
    "Alive Workers: 3\n",
    "\n",
    "Cores in use: 12 Total, 0 Used\n",
    "\n",
    "Memory in use: 54.0 GB Total, 0.0 B Used\n",
    "\n",
    "Applications: 0 Running, 0 Completed\n",
    "\n",
    "Drivers: 0 Running, 0 Completed\n",
    "\n",
    "Status: ALIVE\n",
    "\n",
    "Workers\n",
    "\n",
    "Worker Id\tAddress\tState\tCores\tMemory\n",
    "\n",
    "worker-20201018101815-10.1.10.126-37999\t10.1.10.126:37999\tALIVE\t4 (0 Used)\t18.0 GB (0.0 B Used)\n",
    "\n",
    "worker-20201018101818-10.1.10.126-36635\t10.1.10.126:36635\tALIVE\t4 (0 Used)\t18.0 GB (0.0 B Used)\n",
    "\n",
    "worker-20201018101820-10.1.10.126-45525\t10.1.10.126:45525\tALIVE\t4 (0 Used)\t18.0 GB (0.0 B Used)\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/41605276/96364758-1ff71900-1177-11eb-8b90-462cfcd6a111.png)\n",
    "\n",
    "하나의 버츄얼 머신에 세개의 쓰레드를 만든것이다.\n",
    "\n",
    "\n",
    "이제는 제플린을 설치하려고 한다. 참고로 spark와 제플린의 버전을 잘 맞춰서 설치해야하기 때문에 스파크 최신버전이 아니라 2.1 버전을 설치한 것이다 (제플린은 0.7.3 버전). 스팍과 제플린의 버전을 맞춰주지 않으면 Error 발생이 많아서 상당히 운영하기 어려울 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-1-10-126 sbin]$ cd ~\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 ~]$ wget https://archive.apache.org/dist/zeppelin/zeppelin-0.7.3/zeppelin-0.7.3-bin-all.tgz\n",
    "--2020-10-18 10:34:45--  https://archive.apache.org/dist/zeppelin/zeppelin-0.7.3/zeppelin-0.7.3-bin-all.tgz\n",
    "Resolving archive.apache.org (archive.apache.org)... 138.201.131.134, 2a01:4f8:172:2ec5::2\n",
    "Connecting to archive.apache.org (archive.apache.org)|138.201.131.134|:443... connected.\n",
    "HTTP request sent, awaiting response... 200 OK\n",
    "Length: 834875175 (796M) [application/x-gzip]\n",
    "Saving to: ‘zeppelin-0.7.3-bin-all.tgz’\n",
    "\n",
    "100%[==============================================================================================>] 834,875,175 5.59MB/s   in 2m 25s\n",
    "\n",
    "2020-10-18 10:37:11 (5.50 MB/s) - ‘zeppelin-0.7.3-bin-all.tgz’ saved [834875175/834875175]\n",
    "        \n",
    "[ec2-user@ip-10-1-10-126 ~]$ tar -zxf zeppelin-0.7.3-bin-all.tgz\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 ~]$ ls\n",
    "spark-2.1.0-bin-hadoop2.7  spark-2.1.0-bin-hadoop2.7.tgz  zeppelin-0.7.3-bin-all  zeppelin-0.7.3-bin-all.tgz\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 ~]$ cd zeppelin-0.7.3-bin-all/conf\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 conf]$ ls\n",
    "configuration.xsl  log4j.properties    zeppelin-env.cmd.template  zeppelin-site.xml.template\n",
    "interpreter-list   shiro.ini.template  zeppelin-env.sh.template\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 conf]$ cp zeppelin-site.xml.template zeppelin-site.xml\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 conf]$ ifconfig\n",
    "eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 9001\n",
    "        inet 10.1.10.126  netmask 255.255.255.0  broadcast 10.1.10.255\n",
    "        inet6 fe80::96:85ff:fedd:5f42  prefixlen 64  scopeid 0x20<link>\n",
    "        ether 02:96:85:dd:5f:42  txqueuelen 1000  (Ethernet)\n",
    "        RX packets 836189  bytes 1224343214 (1.1 GiB)\n",
    "        RX errors 0  dropped 0  overruns 0  frame 0\n",
    "        TX packets 369801  bytes 24755058 (23.6 MiB)\n",
    "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
    "\n",
    "lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536\n",
    "        inet 127.0.0.1  netmask 255.0.0.0\n",
    "        inet6 ::1  prefixlen 128  scopeid 0x10<host>\n",
    "        loop  txqueuelen 1000  (Local Loopback)\n",
    "        RX packets 975  bytes 532888 (520.3 KiB)\n",
    "        RX errors 0  dropped 0  overruns 0  frame 0\n",
    "        TX packets 975  bytes 532888 (520.3 KiB)\n",
    "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
    "\n",
    "# 위에서 ifconfig 했을때 10.1.10.126 (ec2 private ip) 이거를 IP 주소로 설정해주고, 제플린 포트도 원래 디폴트는 8080인데 스팍 마스터서버가 해당포트를\n",
    "# 쓰고있으니까 제플린은 7777을 쓰도록 설정해준다.\n",
    "[ec2-user@ip-10-1-10-126 conf]$ sudo vim zeppelin-site.xml\n",
    "<property>\n",
    "  <name>zeppelin.server.addr</name>\n",
    "  <value>10.1.10.126</value> \n",
    "  <description>Server address</description>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "  <name>zeppelin.server.port</name>\n",
    "  <value>7777</value>\n",
    "  <description>Server port.</description>\n",
    "</property>\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 conf]$ cp zeppelin-env.sh.template zeppelin-env.sh\n",
    "# 아래의 내용을 추가한다.\n",
    "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.265.b01-1.amzn2.0.1.x86_64\n",
    "export MASTER=spark://ip-10-1-10-126.ap-northeast-2.compute.internal:7077\n",
    "export SPARK_HOME=/home/ec2-user/spark-2.1.0-bin-hadoop2.7\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 conf]$ cd ..\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 zeppelin-0.7.3-bin-all]$ ls\n",
    "bin  conf  interpreter  lib  LICENSE  licenses  notebook  NOTICE  README.md  zeppelin-web-0.7.3.war\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 zeppelin-0.7.3-bin-all]$ cd bin\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 bin]$ ls\n",
    "common.cmd  functions.cmd  install-interpreter.sh  interpreter.sh  zeppelin-daemon.sh\n",
    "common.sh   functions.sh   interpreter.cmd         zeppelin.cmd    zeppelin.sh\n",
    "\n",
    "[ec2-user@ip-10-1-10-126 bin]$ ./zeppelin-daemon.sh start\n",
    "Log dir doesn't exist, create /home/ec2-user/zeppelin-0.7.3-bin-all/logs\n",
    "Pid dir doesn't exist, create /home/ec2-user/zeppelin-0.7.3-bin-all/run\n",
    "Zeppelin start                                             [  OK  ]\n",
    "\n",
    "# JVM에서 실행되는 프로세스가 어떤게 있는지 체크하여 우리가 띄운것들이 잘 구동되고 있는지 확인한다.\n",
    "[ec2-user@ip-10-1-10-126 bin]$ jps\n",
    "10001 ZeppelinServer\n",
    "9618 Worker\n",
    "9732 Worker\n",
    "10075 Jps\n",
    "12621 Master\n",
    "9503 Worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런 다음에 웹브라우져를 열어서 `[ec2-public-ip]:7777` 로 접속하면 제플린에 정상접속 되는것을 확인할 수 있다.\n",
    "\n",
    "접속해서 spark 노트북을 하나 만들고 `sc` 를 쓰고 실행했을때\n",
    "\n",
    "`res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@552dc360` 와 같이 sparkcontext가 잘 만들어지는지도 확인해본다.\n",
    "\n",
    "그러면 제플린에서 데이터를 처리하는 실습을 해보자\n",
    "\n",
    "먼저 버츄얼머신에 아래와 같이 샘플 데이터를 s3로부터 다운로드 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-1-10-126 ~]$ aws configure\n",
    "AWS Access Key ID [None]: xxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "AWS Secret Access Key [None]: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "Default region name [None]: ap-northeast-2\n",
    "Default output format [None]: json\n",
    "    \n",
    "[ec2-user@ip-10-1-10-126 ~]$ aws s3 cp s3://my_s3_bucket/oct/2019-Oct.csv sample_data.csv\n",
    "download: s3://pms-bucket-test/oct/2019-Oct.csv to ./sample_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런 다음에 제플린 노트북으로 돌아와서 \n",
    "\n",
    "`val data = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/home/ec2-user/sample_data.csv\")`를 실행하면\n",
    "\n",
    "`data: org.apache.spark.sql.DataFrame = [event_time: string, event_type: string ... 7 more fields]` 와 같이 결과가 뜨면서 데이터 프레임 형태로 데이터를 가져오게 된다.\n",
    "\n",
    "`val data_rdd = sc.textFile(\"/home/ec2-user/sample_data.csv\")` 와 같이 실행하면 \n",
    "\n",
    "`data_rdd: org.apache.spark.rdd.RDD[String] = /home/ec2-user/sample_data.csv MapPartitionsRDD[8] at textFile at <console>:27` 와 같이 rdd 형태로도 가져오게 된다.\n",
    "\n",
    "`data.show()` 를 하게되면 아래와 같이 데이터를 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
    "|          event_time|event_type|product_id|        category_id|       category_code|   brand|  price|  user_id|        user_session|\n",
    "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
    "|2019-10-01 00:00:...|      view|  44600062|2103807459595387724|                null|shiseido|  35.79|541312140|72d76fde-8bb3-4e0...|\n",
    "|2019-10-01 00:00:...|      view|   3900821|2053013552326770905|appliances.enviro...|    aqua|  33.20|554748717|9333dfbd-b87a-470...|\n",
    "|2019-10-01 00:00:...|      view|  17200506|2053013559792632471|furniture.living_...|    null| 543.10|519107250|566511c2-e2e3-422...|\n",
    "|2019-10-01 00:00:...|      view|   1307067|2053013558920217191|  computers.notebook|  lenovo| 251.74|550050854|7c90fc70-0e80-459...|\n",
    "|2019-10-01 00:00:...|      view|   1004237|2053013555631882655|electronics.smart...|   apple|1081.98|535871217|c6bd7419-2748-4c5...|\n",
    "|2019-10-01 00:00:...|      view|   1480613|2053013561092866779|   computers.desktop|  pulser| 908.62|512742880|0d0d91c2-c9c2-4e8...|\n",
    "|2019-10-01 00:00:...|      view|  17300353|2053013553853497655|                null|   creed| 380.96|555447699|4fe811e9-91de-46d...|\n",
    "|2019-10-01 00:00:...|      view|  31500053|2053013558031024687|                null|luminarc|  41.16|550978835|6280d577-25c8-414...|\n",
    "|2019-10-01 00:00:...|      view|  28719074|2053013565480109009|  apparel.shoes.keds|   baden| 102.71|520571932|ac1cd4e5-a3ce-422...|\n",
    "|2019-10-01 00:00:...|      view|   1004545|2053013555631882655|electronics.smart...|  huawei| 566.01|537918940|406c46ed-90a4-478...|\n",
    "|2019-10-01 00:00:...|      view|   2900536|2053013554776244595|appliances.kitche...|elenberg|  51.46|555158050|b5bdd0b3-4ca2-4c5...|\n",
    "|2019-10-01 00:00:...|      view|   1005011|2053013555631882655|electronics.smart...| samsung| 900.64|530282093|50a293fb-5940-41b...|\n",
    "|2019-10-01 00:00:...|      view|   3900746|2053013552326770905|appliances.enviro...|   haier| 102.38|555444559|98b88fa0-d8fa-4b9...|\n",
    "|2019-10-01 00:00:...|      view|  44600062|2103807459595387724|                null|shiseido|  35.79|541312140|72d76fde-8bb3-4e0...|\n",
    "|2019-10-01 00:00:...|      view|  13500240|2053013557099889147|furniture.bedroom...|     brw|  93.18|555446365|7f0062d8-ead0-4e0...|\n",
    "|2019-10-01 00:00:...|      view|  23100006|2053013561638126333|                null|    null| 357.79|513642368|17566c27-0a8f-450...|\n",
    "|2019-10-01 00:00:...|      view|   1801995|2053013554415534427|electronics.video.tv|   haier| 193.03|537192226|e3151795-c355-4ef...|\n",
    "|2019-10-01 00:00:...|      view|  10900029|2053013555069845885|appliances.kitche...|   bosch|  58.95|519528062|901b9e3c-3f8f-414...|\n",
    "|2019-10-01 00:00:...|      view|   1306631|2053013558920217191|  computers.notebook|      hp| 580.89|550050854|7c90fc70-0e80-459...|\n",
    "|2019-10-01 00:00:...|      view|   1005135|2053013555631882655|electronics.smart...|   apple|1747.79|535871217|c6bd7419-2748-4c5...|\n",
    "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data_rdd.count()` 를 실행하여 rdd로 해당 데이터의 로우카운트를 해보자.\n",
    "\n",
    "실행결과는 `res3: Long = 42448765` 와 같이 나온다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
