{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "#### 1. 참고자료\n",
    "\n",
    "1) https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/emr.html#EMR.Client.run_job_flow\n",
    "\n",
    "2) https://lamanus.kr/57\n",
    "\n",
    "#### 2. Lambda function 구현하기\n",
    "\n",
    "step 1) Lambda 코드 작성 전에 '환경변수' -> '편집' 클릭\n",
    "\n",
    "아래와 같이 내 계정의 access key와 secret access key 변수를 설정해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2.png\" width=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2) 람다함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "aws_key = os.environ['AWS_KEY']\n",
    "aws_skey = os.environ['AWS_SKEY']\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    print(\"Creating EMR\")\n",
    "    session = boto3.session.Session(region_name='us-west-2') \n",
    "    emr_client = session.client('emr', aws_access_key_id = aws_key, aws_secret_access_key = aws_skey)\n",
    "    \n",
    "    cluster_id = emr_client.run_job_flow(\n",
    "        Name='pms-emr-test', \n",
    "        LogUri='s3://lhw-s3-test/log-folder/', \n",
    "        ReleaseLabel='emr-5.28.0', \n",
    "        Applications=[\n",
    "            {'Name': 'Hadoop'}, \n",
    "            {'Name': 'Hive'}, \n",
    "            {'Name': 'Spark'}],\n",
    "        Instances={\n",
    "            'InstanceGroups': [\n",
    "                {\n",
    "                    'Name': 'Master nodes', \n",
    "                    'Market': 'SPOT', \n",
    "                    'InstanceRole': 'MASTER', \n",
    "                    'InstanceType': 'm5.xlarge', \n",
    "                    'InstanceCount': 1, \n",
    "                    'EbsConfiguration': {\n",
    "                        'EbsBlockDeviceConfigs': [\n",
    "                            {\n",
    "                                'VolumeSpecification':{\n",
    "                                    'VolumeType': 'gp2', \n",
    "                                    'SizeInGB': 400\n",
    "                                },\n",
    "                                'VolumesPerInstance': 2\n",
    "                            },\n",
    "                        ],\n",
    "                        'EbsOptimized': True,\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'Name': 'Slave nodes', \n",
    "                    'Market': 'SPOT', \n",
    "                    'InstanceRole': 'CORE', \n",
    "                    'InstanceType': 'm5.xlarge', \n",
    "                    'InstanceCount': 4, \n",
    "                    'EbsConfiguration': {\n",
    "                        'EbsBlockDeviceConfigs': [\n",
    "                            {\n",
    "                                'VolumeSpecification': {\n",
    "                                    'VolumeType': 'gp2',\n",
    "                                    'SizeInGB': 500\n",
    "                                },\n",
    "                                'VolumesPerInstance': 2\n",
    "                            },\n",
    "                        ],\n",
    "                        'EbsOptimized': True,\n",
    "                    },\n",
    "                    'AutoScalingPolicy': { \n",
    "                        'Constraints': {\n",
    "                            'MinCapacity': 2, \n",
    "                            'MaxCapacity': 20\n",
    "                        },\n",
    "                        'Rules': [\n",
    "                            {\n",
    "                                'Name': 'Compute-scale-up', \n",
    "                                'Description': 'scale up on YARNMemory',\n",
    "                                'Action': {\n",
    "                                    'SimpleScalingPolicyConfiguration': {\n",
    "                                        'AdjustmentType': 'CHANGE_IN_CAPACITY', \n",
    "                                        'ScalingAdjustment': 1, \n",
    "                                        'CoolDown': 300\n",
    "                                    }\n",
    "                                },\n",
    "                                'Trigger': {\n",
    "                                    'CloudWatchAlarmDefinition' :{\n",
    "                                        'ComparisonOperator': 'LESS_THAN', \n",
    "                                        'EvaluationPeriods': 120, \n",
    "                                        'MetricName': 'YARNMemoryAvailablePercentage', \n",
    "                                        'Namespace': 'AWS/ElasticMapReduce', \n",
    "                                        'Period': 300, \n",
    "                                        'Statistic': 'AVERAGE', \n",
    "                                        'Threshold': 20, \n",
    "                                        'Unit': 'PERCENT'\n",
    "                                    }\n",
    "                                }\n",
    "                            },\n",
    "                            {\n",
    "                                'Name': 'Compute-scale-down',\n",
    "                                'Description':'scale down on YARNMemory', \n",
    "                                'Action': {\n",
    "                                    'SimpleScalingPolicyConfiguration' : {\n",
    "                                        'AdjustmentType': 'CHANGE_IN_CAPACITY', \n",
    "                                        'ScalingAdjustment': -1, \n",
    "                                        'CoolDown': 300\n",
    "                                    }\n",
    "                                },\n",
    "                                'Trigger':{\n",
    "                                    'CloudWatchAlarmDefinition': {\n",
    "                                        'ComparisonOperator': 'LESS_THAN', \n",
    "                                        'EvaluationPeriods': 125, \n",
    "                                        'MetricName': 'YARNMemoryAvailablePercentage',\n",
    "                                        'Namespace': 'AWS/ElasticMapReduce', \n",
    "                                        'Period': 250,\n",
    "                                        'Statistic': 'AVERAGE', \n",
    "                                        'Threshold': 85, \n",
    "                                        'Unit': 'PERCENT'\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            'KeepJobFlowAliveWhenNoSteps': False,\n",
    "            'TerminationProtected': False,\n",
    "            'Ec2KeyName': 'pms_oregon_key',\n",
    "            'Ec2SubnetId': 'subnet-03ab1e1a1ea3165e5', \n",
    "            'EmrManagedMasterSecurityGroup': 'sg-017a3f873088b6641',\n",
    "            'EmrManagedSlaveSecurityGroup':  'sg-0ebcd86afff99eece'\n",
    "        },\n",
    "        Steps=[\n",
    "            {\n",
    "                'Name': 'spark-submit',\n",
    "                'ActionOnFailure': 'TERMINATE_CLUSTER',\n",
    "                'HadoopJarStep': {\n",
    "                    ## command-runner.jar은 기본적으로 내장된 파일임. 이걸 통해서 spark-submit 단계를 추가할 수 있게 된다.\n",
    "                    'Jar': 'command-runner.jar',\n",
    "                    'Args': ['spark-submit',\n",
    "                             '--master', 'yarn', '--deploy-mode', 'client',\n",
    "                             '--class', 'main class',\n",
    "                             's3://lhw-s3-test/emrtest.py'\n",
    "                            ]\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'Name': 'spark-submit2',\n",
    "                'ActionOnFailure': 'TERMINATE_CLUSTER',\n",
    "                'HadoopJarStep': {\n",
    "                'Jar': 'command-runner.jar',\n",
    "                'Args': ['spark-submit',\n",
    "                         '--master', 'yarn', '--deploy-mode', 'client',\n",
    "                         '--class', 'main class',\n",
    "                         's3://lhw-s3-test/emrtest2.py'\n",
    "                        ]\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        AutoScalingRole='EMR_AutoScaling_DefaultRole', \n",
    "        VisibleToAllUsers=True, \n",
    "        JobFlowRole='EMR_EC2_DefaultRole', \n",
    "        ServiceRole= 'EMR_DefaultRole', \n",
    "        EbsRootVolumeSize=100, \n",
    "        Tags=[\n",
    "            {\n",
    "                'Key': 'NAME', \n",
    "                'Value': 'pms-emr-test',\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return cluster_id['JobFlowId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 로컬 피시(리눅스 환경)에서 boto3를 이용한 EMR spark job 자동화 구현하기\n",
    "\n",
    "step 1) 로컬피시에서 환경변수 액세스키와 시큐리티 액세스키를 아래 그림과 같이 설정하여 잡아주기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1.jpg\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2) python을 이용한 EMR 구동 및 spark job 부여 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating EMR\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'j-28VEZVYPPWHNT'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "\n",
    "# aws_key = os.environ['AWS_KEY']\n",
    "# aws_skey = os.environ['AWS_SKEY']\n",
    "# 보안상 위와같이 환경변수를 지정해서 사용해야 함.\n",
    "\n",
    "aws_key ='*****'\n",
    "aws_skey ='*****'\n",
    "\n",
    "\n",
    "## 람다에서 쓰일경우(람다핸들러) = def lambda_handler(event, context): 로 바꿔서 쓰면됨\n",
    "def EMR_handler():\n",
    "    print(\"Creating EMR\")\n",
    "    session = boto3.session.Session(region_name='us-west-2') \n",
    "    emr_client = session.client('emr', aws_access_key_id = aws_key, aws_secret_access_key = aws_skey)\n",
    "    \n",
    "    cluster_id = emr_client.run_job_flow(\n",
    "        Name='pms-emr-test', \n",
    "        LogUri='s3://lhw-s3-test/log-folder/', \n",
    "        ReleaseLabel='emr-5.28.0', \n",
    "        Applications=[\n",
    "            {'Name': 'Hadoop'}, \n",
    "            {'Name': 'Hive'}, \n",
    "            {'Name': 'Spark'}],\n",
    "        Instances={\n",
    "            'InstanceGroups': [\n",
    "                {\n",
    "                    'Name': 'Master nodes', \n",
    "                    'Market': 'SPOT', \n",
    "                    'InstanceRole': 'MASTER', \n",
    "                    'InstanceType': 'm5.xlarge', \n",
    "                    'InstanceCount': 1, \n",
    "                    'EbsConfiguration': {\n",
    "                        'EbsBlockDeviceConfigs': [\n",
    "                            {\n",
    "                                'VolumeSpecification':{\n",
    "                                    'VolumeType': 'gp2', \n",
    "                                    'SizeInGB': 400\n",
    "                                },\n",
    "                                'VolumesPerInstance': 2\n",
    "                            },\n",
    "                        ],\n",
    "                        'EbsOptimized': True,\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'Name': 'Slave nodes', \n",
    "                    'Market': 'SPOT', \n",
    "                    'InstanceRole': 'CORE', \n",
    "                    'InstanceType': 'm5.xlarge', \n",
    "                    'InstanceCount': 4, \n",
    "                    'EbsConfiguration': {\n",
    "                        'EbsBlockDeviceConfigs': [\n",
    "                            {\n",
    "                                'VolumeSpecification': {\n",
    "                                    'VolumeType': 'gp2',\n",
    "                                    'SizeInGB': 500\n",
    "                                },\n",
    "                                'VolumesPerInstance': 2\n",
    "                            },\n",
    "                        ],\n",
    "                        'EbsOptimized': True,\n",
    "                    },\n",
    "                    'AutoScalingPolicy': { \n",
    "                        'Constraints': {\n",
    "                            'MinCapacity': 2, \n",
    "                            'MaxCapacity': 20\n",
    "                        },\n",
    "                        'Rules': [\n",
    "                            {\n",
    "                                'Name': 'Compute-scale-up', \n",
    "                                'Description': 'scale up on YARNMemory',\n",
    "                                'Action': {\n",
    "                                    'SimpleScalingPolicyConfiguration': {\n",
    "                                        'AdjustmentType': 'CHANGE_IN_CAPACITY', \n",
    "                                        'ScalingAdjustment': 1, \n",
    "                                        'CoolDown': 300\n",
    "                                    }\n",
    "                                },\n",
    "                                'Trigger': {\n",
    "                                    'CloudWatchAlarmDefinition' :{\n",
    "                                        'ComparisonOperator': 'LESS_THAN', \n",
    "                                        'EvaluationPeriods': 120, \n",
    "                                        'MetricName': 'YARNMemoryAvailablePercentage', \n",
    "                                        'Namespace': 'AWS/ElasticMapReduce', \n",
    "                                        'Period': 300, \n",
    "                                        'Statistic': 'AVERAGE', \n",
    "                                        'Threshold': 20, \n",
    "                                        'Unit': 'PERCENT'\n",
    "                                    }\n",
    "                                }\n",
    "                            },\n",
    "                            {\n",
    "                                'Name': 'Compute-scale-down',\n",
    "                                'Description':'scale down on YARNMemory', \n",
    "                                'Action': {\n",
    "                                    'SimpleScalingPolicyConfiguration' : {\n",
    "                                        'AdjustmentType': 'CHANGE_IN_CAPACITY', \n",
    "                                        'ScalingAdjustment': -1, \n",
    "                                        'CoolDown': 300\n",
    "                                    }\n",
    "                                },\n",
    "                                'Trigger':{\n",
    "                                    'CloudWatchAlarmDefinition': {\n",
    "                                        'ComparisonOperator': 'LESS_THAN', \n",
    "                                        'EvaluationPeriods': 125, \n",
    "                                        'MetricName': 'YARNMemoryAvailablePercentage',\n",
    "                                        'Namespace': 'AWS/ElasticMapReduce', \n",
    "                                        'Period': 250,\n",
    "                                        'Statistic': 'AVERAGE', \n",
    "                                        'Threshold': 85, \n",
    "                                        'Unit': 'PERCENT'\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            'KeepJobFlowAliveWhenNoSteps': False,\n",
    "            'TerminationProtected': False,\n",
    "            'Ec2KeyName': 'pms_oregon_key',\n",
    "            'Ec2SubnetId': 'subnet-03ab1e1a1ea3165e5', \n",
    "            'EmrManagedMasterSecurityGroup': 'sg-017a3f873088b6641',\n",
    "            'EmrManagedSlaveSecurityGroup':  'sg-0ebcd86afff99eece'\n",
    "        },\n",
    "        Steps=[\n",
    "            {\n",
    "                'Name': 'spark-submit',\n",
    "                'ActionOnFailure': 'TERMINATE_CLUSTER',\n",
    "                'HadoopJarStep': {\n",
    "                    ## command-runner.jar은 기본적으로 내장된 파일임. 이걸 통해서 spark-submit 단계를 추가할 수 있게 된다.\n",
    "                    'Jar': 'command-runner.jar',\n",
    "                    'Args': ['spark-submit',\n",
    "                             '--master', 'yarn', '--deploy-mode', 'client',\n",
    "                             '--class', 'main class',\n",
    "                             's3://lhw-s3-test/emrtest.py'\n",
    "                            ]\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'Name': 'spark-submit2',\n",
    "                'ActionOnFailure': 'TERMINATE_CLUSTER',\n",
    "                'HadoopJarStep': {\n",
    "                'Jar': 'command-runner.jar',\n",
    "                'Args': ['spark-submit',\n",
    "                         '--master', 'yarn', '--deploy-mode', 'client',\n",
    "                         '--class', 'main class',\n",
    "                         's3://lhw-s3-test/emrtest2.py'\n",
    "                        ]\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        AutoScalingRole='EMR_AutoScaling_DefaultRole', \n",
    "        VisibleToAllUsers=True, \n",
    "        JobFlowRole='EMR_EC2_DefaultRole', \n",
    "        ServiceRole= 'EMR_DefaultRole', \n",
    "        EbsRootVolumeSize=100, \n",
    "        Tags=[\n",
    "            {\n",
    "                'Key': 'NAME', \n",
    "                'Value': 'pms-emr-test',\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return cluster_id['JobFlowId']\n",
    "\n",
    "EMR_handler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. emrtest.py 코드내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df_csv = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"s3a://lhw-s3-test/source_temp/*.csv\")\n",
    "\n",
    "udf_year = udf(lambda record:record[0:4],StringType())\n",
    "udf_month = udf(lambda record:record[5:7],StringType())\n",
    "udf_day = udf(lambda record:record[8:10],StringType())\n",
    "\n",
    "new_df_csv = df_csv.withColumn('year',udf_year('event_time').cast(IntegerType()))\n",
    "new_df_csv = new_df_csv.withColumn('month',udf_month('event_time').cast(IntegerType()))\n",
    "new_df_csv = new_df_csv.withColumn('day',udf_day('event_time').cast(IntegerType()))\n",
    "\n",
    "new_df_csv.write.partitionBy(\"year\",\"month\",\"day\").save(\"s3a://lhw-s3-test/destination_temp/\", format='parquet', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. emrtest2.py 코드내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df_parquet = spark.read.format(\"parquet\").option(\"header\", \"true\").load(\"s3a://lhw-s3-test/destination_temp/\")\n",
    "\n",
    "df_parquet.repartitionByRange(1,'year').write.save(\"s3a://lhw-s3-test/destination_temp2/\", format='csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
