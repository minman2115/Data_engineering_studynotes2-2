{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "Data_Engineering_TIL(20200731)\n",
    "\n",
    "** 학습 시 참고자료 \n",
    "\n",
    "1) AWS EC2로 Hadoop Cluster 구축하기\n",
    "\n",
    "URL : https://yahwang.github.io/posts/62 \n",
    "\n",
    "2) [:EN]HADOOP 101: MULTI-NODE INSTALLATION USING AWS EC2[:KO]HADOOP 101: 멀티노드 설치 AWS EC2[:]\n",
    "\n",
    "URL : https://codethief.io/ko/hadoop101/\n",
    "\n",
    "\n",
    "#### 1. 구축환경\n",
    "\n",
    "- NameNode 1 & DataNode 2 & client 1\n",
    "\n",
    "ex)\n",
    "\n",
    "pms-hadoop-namenode, pms-hadoop-datanode, pms-hadoop-client\n",
    "\n",
    "\n",
    "- EC2 Instance : Ubuntu 18.04 / m5.xlarge / 30GB\n",
    "\n",
    "** client만 amazon linux 2 / t3.micro / 8GB\n",
    "\n",
    "\n",
    "- Security Group\n",
    "\n",
    "Type   Protocol  Port range  Source\n",
    "\n",
    "All TCP\tTCP\t0 - 65535\t[local pc 아이피주소]\n",
    "\n",
    "All TCP\tTCP\t0 - 65535\t[이 보안그룹의 ID]\n",
    "\n",
    "#### 2. 설치과정\n",
    "\n",
    "step 0) 사전준비\n",
    "\n",
    "ubuntu 18버전으로 datanode 생성한다.\n",
    "\n",
    "마찬가지로 amazon linux 2 버전으로 client 1대를 생성하고, 거기에 하둡 클러스터에서 쓸 키페어를 싸이버덕으로 넣어준다.\n",
    "\n",
    "그런 다음에 클라이언트로 접속해서 아래와 같은 명령어로 키페어의 권한을 변경해주고 네임노드에 키페어를 복사해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-1-10-247 ~]$ ls -l\n",
    "-rw-rw-r-- 1 ec2-user ec2-user 1674 Jul 31 05:38 pms-hadoop-test.pem\n",
    "\n",
    "# private key는 소유자만 사용하도록 권한 설정부터 해야 사용할 수 있다.\n",
    "[ec2-user@ip-10-1-10-247 ~]$ sudo chmod 400 pms-hadoop-test.pem\n",
    "\n",
    "## ls -l로 확인하면 -r------- 로 변한다.\n",
    "[ec2-user@ip-10-1-10-247 ~]$ ll\n",
    "-r-------- 1 ec2-user ec2-user 1674 Jul 31 05:38 pms-hadoop-test.pem\n",
    "\n",
    "# private key를 Instance로 ssh를 통해 복사\n",
    "[ec2-user@ip-10-1-10-247 ~]$ scp -i pms-hadoop-test.pem pms-hadoop-test.pem ubuntu@[namenode 퍼블릭 IP]:~/.ssh\n",
    "The authenticity of host '13.125.191.160 (13.125.191.160)' can't be established.\n",
    "ECDSA key fingerprint is SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.\n",
    "ECDSA key fingerprint is MD5:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.\n",
    "Are you sure you want to continue connecting (yes/no)? yes\n",
    "Warning: Permanently added '13.125.191.160' (ECDSA) to the list of known hosts.\n",
    "pms-hadoop-hadoop.pem                                                            100% 1674     2.3MB/s   00:00\n",
    "\n",
    "# private key로 Instance 연결\n",
    "[ec2-user@ip-10-1-10-247 ~]$ ssh -i pms-hadoop-test.pem ubuntu@13.125.191.160\n",
    "Welcome to Ubuntu 18.04.4 LTS (GNU/Linux 5.3.0-1023-aws x86_64)\n",
    "\n",
    " * Documentation:  https://help.ubuntu.com\n",
    " * Management:     https://landscape.canonical.com\n",
    " * Support:        https://ubuntu.com/advantage\n",
    "\n",
    "  System information as of Fri Jul 31 05:52:22 UTC 2020\n",
    "\n",
    "  System load:  0.0                Processes:           122\n",
    "  Usage of /:   10.5% of 29.02GB   Users logged in:     1\n",
    "  Memory usage: 2%                 IP address for ens5: 10.1.10.226\n",
    "  Swap usage:   0%\n",
    "\n",
    "\n",
    "0 packages can be updated.\n",
    "0 updates are security updates.\n",
    "\n",
    "\n",
    "*** System restart required ***\n",
    "Last login: Fri Jul 31 05:48:51 2020 from 58.151.93.17\n",
    "ubuntu@ip-10-1-10-121:~$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 1) 위와 같이 네임노드로 접속하여 호스트네임을 namenode라고 변경해준다.\n",
    "\n",
    "참고자료 : https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/set-hostname.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo hostnamectl set-hostname namenode\n",
    "sudo reboot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2) 다시 네임노드로 돌아와서 아래의 명령어를 실행하여 apt를 업데이트해주고. 자바와 하둡을 설치해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt update -y \n",
    "sudo apt dist-upgrade -y\n",
    "sudo apt install openjdk-8-jdk -y\n",
    "wget http://apache.mirror.cdnetworks.com/hadoop/common/hadoop-2.9.2/hadoop-2.9.2.tar.gz -P ~/Downloads\n",
    "sudo tar zxvf ~/Downloads/hadoop-* -C /usr/local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 3) bashrc에 환경설정 추가\n",
    "\n",
    "** 참고사항 : 환경변수 설정방법\n",
    "\n",
    "- 1회성 설정 방법\n",
    "\n",
    "ex) export <변수명>=<값> : export JAVA_HOME=/user/lib/java-7-openjdk-amd64/\n",
    "\n",
    "- 영구 설정 방법\n",
    "\n",
    "/etc/bash.bashrc 파일에 export <변수명>=<값>을 작성하고, source /etc/bash.bashrc 명령을 통해 적용 해 주면 된다.\n",
    "\n",
    "영구적인 설정방법으로 bashrc 환경변수를 아래와 같이 설정해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubuntu@datanode:/etc$ sudo mv /usr/local/hadoop-* /usr/local/hadoop\n",
    "\n",
    "ubuntu@datanode:/etc$ sudo vim /etc/bash.bashrc\n",
    "# 가장하단의 빈칸에 아래와 같은 내용을 추가\n",
    "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
    "# 참고사항 : JAVA 경로 확인 readlink -f $(which java)\n",
    "export HADOOP_HOME=/usr/local/hadoop\n",
    "export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop\n",
    "export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n",
    "\n",
    "# 저장\n",
    "ubuntu@datanode:/etc$ source /etc/bash.bashrc\n",
    "\n",
    "# 하둡 폴더 소유자 변경\n",
    "ubuntu@datanode:/etc$ sudo chown -R ubuntu $HADOOP_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 4) Hadoop 기본 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubuntu@namenode:~$ cd $HADOOP_CONF_DIR\n",
    "\n",
    "ubuntu@namenode:/usr/local/hadoop/etc/hadoop$ sudo vim /usr/local/hadoop/etc/hadoop/hadoop-env.sh\n",
    "# JAVA를 환경변수에 이미 설정했지만 hadoop이 인식못하는 것을 방지하기 위해서 추가로 설정한다.\n",
    "# export JAVA_HOME 부분을 찾아서 아래와 같이 고친다.\n",
    "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
    "\n",
    "ubuntu@namenode:/usr/local/hadoop/etc/hadoop$ sudo vim /usr/local/hadoop/etc/hadoop/core-site.xml\n",
    "# node에게 namenode의 정보를 알려준다.\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>fs.defaultFS</name>\n",
    "    <value>hdfs://[namenode의 퍼블릭 DNS주소]:9000</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "\n",
    "ubuntu@namenode:/usr/local/hadoop/etc/hadoop$ sudo vim /usr/local/hadoop/etc/hadoop/yarn-site.xml\n",
    "<configuration>\n",
    "<!-- Site specific YARN configuration properties -->\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.aux-services</name>\n",
    "    <value>mapreduce_shuffle</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.resourcemanager.hostname</name>\n",
    "    <value>[namenode의 퍼블릭 DNS주소]</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "\n",
    "ubuntu@namenode:/usr/local/hadoop/etc/hadoop$ sudo cp mapred-site.xml.template mapred-site.xml\n",
    "ubuntu@namenode:/usr/local/hadoop/etc/hadoop$ sudo vim /usr/local/hadoop/etc/hadoop/mapred-site.xml\n",
    "# jobtracker는 yarn을 사용하지 않을 경우를 대비한 용도 \n",
    "<configuration> \n",
    "  <property>\n",
    "    <name>mapreduce.jobtracker.address</name>\n",
    "    <value>[namenode의 퍼블릭 DNS주소]:54311</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.framework.name</name>\n",
    "    <value>yarn</value>\n",
    "  </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 5) 데이터노드 생성을 위해서 네임노드에 대해서 현재까지 작업한 내용을 AMI 이미지를 뜬다.\n",
    "\n",
    "그리고 생성한 이미지로 데이터노드를 만들어준다.\n",
    "\n",
    "그런 다음에 생성한 두대의 데이터노드로 각각 접속해서 호스트 네임을 datanode로 바꿔주고 재부팅한다.\n",
    "\n",
    "step 6) 클라이언트에서 모든 하둡클러스터 노드와 SSH 연결하기\n",
    "\n",
    "클라이언트로 접속해서 아래와 같은 명령어를 실행신다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-1-10-247 ~]$ sudo mv ~/pms-hadoop-test.pem ~/.ssh\n",
    "[ec2-user@ip-10-1-10-247 ~]$ sudo vim ~/.ssh/config\n",
    "Host namenode\n",
    "  HostName [네임노드 퍼블릭 DNS주소]\n",
    "  User ubuntu\n",
    "  IdentityFile ~/.ssh/pms-hadoop-test.pem\n",
    "\n",
    "Host datanode1\n",
    "  HostName [데이터노드1 퍼블릭 DNS주소]\n",
    "  User ubuntu\n",
    "  IdentityFile ~/.ssh/pms-hadoop-test.pem\n",
    "    \n",
    "Host datanode2\n",
    "  HostName [데이터노드2 퍼블릭 DNS주소]\n",
    "  User ubuntu\n",
    "  IdentityFile ~/.ssh/pms-hadoop-test.pem\n",
    "\n",
    "# 저장 후 namenode에도 이 파일을 복사한다.\n",
    "[ec2-user@ip-10-1-10-247 ~]$ scp ~/.ssh/config namenode:~/.ssh/config\n",
    "config                                                                           100%  197   542.1KB/s   00:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 7) 네임노드가 비밀번호 없이 데이터 노드에 접근할 수 있도록 설정\n",
    "\n",
    "네임노드로 접속해서 아래와 같은 커맨드를 실행시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubuntu@namenode:~$ ssh-keygen -f ~/.ssh/id_rsa -t rsa -P \"\"\n",
    "Generating public/private rsa key pair.\n",
    "Your identification has been saved in /home/ubuntu/.ssh/id_rsa.\n",
    "Your public key has been saved in /home/ubuntu/.ssh/id_rsa.pub.\n",
    "The key fingerprint is:\n",
    "SHA256:xxxxxxxxxxxxxxxxxxxxxxxx ubuntu@namenode\n",
    "The key's randomart image is:\n",
    "+---[RSA 2048]----+\n",
    "|  xxxxxxxxxxxxx   |\n",
    "+----[SHA256]-----+\n",
    "\n",
    "ubuntu@namenode:~$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
    "\n",
    "# datanode에 public key를 인증\n",
    "ubuntu@namenode:~$ ssh datanode1 'cat >> ~/.ssh/authorized_keys' < ~/.ssh/id_rsa.pub\n",
    "The authenticity of host '13.124.220.29 (13.124.220.29)' can't be established.\n",
    "ECDSA key fingerprint is SHA256:xxxxxxxxxxxxxxxxxxxxxxxx.\n",
    "Are you sure you want to continue connecting (yes/no)? yes\n",
    "Warning: Permanently added '13.124.220.29' (ECDSA) to the list of known hosts.\n",
    "    \n",
    "ubuntu@namenode:~$ ssh datanode2 'cat >> ~/.ssh/authorized_keys' < ~/.ssh/id_rsa.pub\n",
    "The authenticity of host '13.124.220.29 (13.124.220.29)' can't be established.\n",
    "ECDSA key fingerprint is SHA256:xxxxxxxxxxxxxxxxxxxxxxxx.\n",
    "Are you sure you want to continue connecting (yes/no)? yes\n",
    "Warning: Permanently added '13.124.220.29' (ECDSA) to the list of known hosts.\n",
    "    \n",
    "ubuntu@namenode:~$ sudo vim /etc/hosts\n",
    "127.0.0.1 localhost\n",
    "[네임노드 프라이빗 아이피] namenode\n",
    "[데이터노드 프라이빗 아이피] datanode1\n",
    "[데이터노드 프라이빗 아이피] datanode2\n",
    "\n",
    "## 네임노드가 데이터노드에 비밀번호 없이 접속할 수 있도록 ssh 적용이 잘 되었는지 아래와 같이 확인\n",
    "\n",
    "ubuntu@namenode:~/.ssh$ ssh localhost\n",
    "The authenticity of host 'localhost (127.0.0.1)' can't be established.\n",
    "ECDSA key fingerprint is SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.\n",
    "Are you sure you want to continue connecting (yes/no)? yes\n",
    "Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.\n",
    "Welcome to Ubuntu 18.04.4 LTS (GNU/Linux 5.3.0-1032-aws x86_64)\n",
    "\n",
    " * Documentation:  https://help.ubuntu.com\n",
    " * Management:     https://landscape.canonical.com\n",
    " * Support:        https://ubuntu.com/advantage\n",
    "\n",
    "  System information as of Sun Aug  2 13:09:03 UTC 2020\n",
    "\n",
    "  System load:  0.0                Processes:           113\n",
    "  Usage of /:   10.5% of 29.02GB   Users logged in:     1\n",
    "  Memory usage: 1%                 IP address for ens5: 10.1.10.19\n",
    "  Swap usage:   0%\n",
    "\n",
    "\n",
    "0 packages can be updated.\n",
    "0 updates are security updates.\n",
    "\n",
    "\n",
    "Last login: Sun Aug  2 13:06:33 2020 from 13.125.215.118\n",
    "ubuntu@namenode:~$ exit\n",
    "logout\n",
    "Connection to localhost closed.\n",
    "ubuntu@namenode:~/.ssh$ ssh namenode\n",
    "The authenticity of host 'ec2-13-125-235-220.ap-northeast-2.compute.amazonaws.com (10.1.10.19)' can't be established.\n",
    "ECDSA key fingerprint is SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.\n",
    "Are you sure you want to continue connecting (yes/no)? yes\n",
    "Warning: Permanently added 'ec2-13-125-235-220.ap-northeast-2.compute.amazonaws.com,10.1.10.19' (ECDSA) to the list of known hosts.\n",
    "Welcome to Ubuntu 18.04.4 LTS (GNU/Linux 5.3.0-1032-aws x86_64)\n",
    "\n",
    " * Documentation:  https://help.ubuntu.com\n",
    " * Management:     https://landscape.canonical.com\n",
    " * Support:        https://ubuntu.com/advantage\n",
    "\n",
    "  System information as of Sun Aug  2 13:09:15 UTC 2020\n",
    "\n",
    "  System load:  0.0                Processes:           114\n",
    "  Usage of /:   10.5% of 29.02GB   Users logged in:     1\n",
    "  Memory usage: 1%                 IP address for ens5: 10.1.10.19\n",
    "  Swap usage:   0%\n",
    "\n",
    "\n",
    "0 packages can be updated.\n",
    "0 updates are security updates.\n",
    "\n",
    "\n",
    "Last login: Sun Aug  2 13:09:03 2020 from 127.0.0.1\n",
    "ubuntu@namenode:~$ exit\n",
    "logout\n",
    "Connection to ec2-13-125-235-220.ap-northeast-2.compute.amazonaws.com closed.\n",
    "ubuntu@namenode:~/.ssh$ ssh datanode1\n",
    "Welcome to Ubuntu 18.04.4 LTS (GNU/Linux 5.3.0-1032-aws x86_64)\n",
    "\n",
    " * Documentation:  https://help.ubuntu.com\n",
    " * Management:     https://landscape.canonical.com\n",
    " * Support:        https://ubuntu.com/advantage\n",
    "\n",
    "  System information as of Sun Aug  2 13:09:21 UTC 2020\n",
    "\n",
    "  System load:  0.0                Processes:           113\n",
    "  Usage of /:   10.5% of 29.02GB   Users logged in:     1\n",
    "  Memory usage: 1%                 IP address for ens5: 10.1.10.127\n",
    "  Swap usage:   0%\n",
    "\n",
    "\n",
    "0 packages can be updated.\n",
    "0 updates are security updates.\n",
    "\n",
    "\n",
    "Last login: Sun Aug  2 13:04:19 2020 from 1.239.132.21\n",
    "ubuntu@datanode1:~$ exit\n",
    "logout\n",
    "Connection to ec2-3-35-53-51.ap-northeast-2.compute.amazonaws.com closed.\n",
    "ubuntu@namenode:~/.ssh$ ssh datanode2\n",
    "Welcome to Ubuntu 18.04.4 LTS (GNU/Linux 5.3.0-1032-aws x86_64)\n",
    "\n",
    " * Documentation:  https://help.ubuntu.com\n",
    " * Management:     https://landscape.canonical.com\n",
    " * Support:        https://ubuntu.com/advantage\n",
    "\n",
    "  System information as of Sun Aug  2 13:09:27 UTC 2020\n",
    "\n",
    "  System load:  0.08               Processes:           124\n",
    "  Usage of /:   10.5% of 29.02GB   Users logged in:     1\n",
    "  Memory usage: 1%                 IP address for ens5: 10.1.10.39\n",
    "  Swap usage:   0%\n",
    "\n",
    "\n",
    "0 packages can be updated.\n",
    "0 updates are security updates.\n",
    "\n",
    "\n",
    "Last login: Sun Aug  2 13:04:25 2020 from 1.239.132.21\n",
    "ubuntu@datanode2:~$ exit\n",
    "logout\n",
    "Connection to ec2-3-35-18-245.ap-northeast-2.compute.amazonaws.com closed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 8) 네임노드의 하둡 config 설정\n",
    "\n",
    "네임노드로 접속해서 아래와 같이 하둡 컨피그를 설정해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubuntu@namenode:/usr/local/hadoop/etc/hadoop$ sudo vim $HADOOP_CONF_DIR/hdfs-site.xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>3</value>\n",
    "  </property>\n",
    "  # namenode 데이터를 저장하는 폴더\n",
    "  <property>\n",
    "    <name>dfs.namenode.name.dir</name>\n",
    "    <value>file:///usr/local/hadoop/data/hdfs/namenode</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "\n",
    "ubuntu@namenode:/usr/local/hadoop/etc/hadoop$ sudo mkdir -p $HADOOP_HOME/data/hdfs/namenode\n",
    "ubuntu@namenode:/usr/local/hadoop/etc/hadoop$ sudo chmod 777 $HADOOP_HOME/data/hdfs/namenode\n",
    "\n",
    "# masters는 secondary namenode의 위치를 지정한다.\n",
    "# 여기서는 namenode와 같은 instance에 만들기 때문에 namenode의 hostname을 입력한다.\n",
    "ubuntu@namenode:/usr/local/hadoop/etc/hadoop$ sudo vim $HADOOP_CONF_DIR/masters\n",
    "namenode    \n",
    "\n",
    "# slaves는 datanode들의 위치를 지정한다.\n",
    "ubuntu@namenode:/usr/local/hadoop/etc/hadoop$ sudo vim $HADOOP_CONF_DIR/slaves\n",
    "# localhost는 지우고 hostname으로 작성\n",
    "datanode1\n",
    "datanode2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 9) 데이터노드의 하둡 config 설정\n",
    "\n",
    "각각의 데이터노드로 접속하여 아래와 같이 하둡 컨피그를 설정해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediate data를 저장하기 위한 폴더를 설정해야 한다.\n",
    "# 이 폴더를 설정하지 않으면, ‘exitCode=-1000. No space available in any of the local directories’ 이런 오류가 생길 수 있다.\n",
    "ubuntu@datanode:/usr/local/hadoop/etc/hadoop$ sudo vim $HADOOP_CONF_DIR/yarn-site.xml\n",
    "<configuration>\n",
    "<!-- Site specific YARN configuration properties -->\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.aux-services</name>\n",
    "    <value>mapreduce_shuffle</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.resourcemanager.hostname</name>\n",
    "    <value>[namenode의 퍼블릭 DNS주소]</value>\n",
    "  </property>\n",
    "  # 아래내용 추가\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.local-dirs</name>\n",
    "    <value>file:///usr/local/hadoop/yarn/local</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "\n",
    "ubuntu@datanode:/usr/local/hadoop/etc/hadoop$ mkdir -p /usr/local/hadoop/yarn/local\n",
    "ubuntu@datanode:/usr/local/hadoop/etc/hadoop$ sudo chmod 777 /usr/local/hadoop/yarn/local\n",
    "    \n",
    "ubuntu@datanode:/usr/local/hadoop/etc/hadoop$ sudo vim $HADOOP_CONF_DIR/hdfs-site.xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>3</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.datanode.data.dir</name>\n",
    "    <value>file:///usr/local/hadoop/data/hdfs/datanode</value>\n",
    "  </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 10) Hadoop Cluster 구동 및 정상작동 확인\n",
    "\n",
    "namenode에서 아래와 같은 명령어로 하둡을 실행하고, 정상작동 여부를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# namenode format으로 오류없이 shutdown 표시가 전시되면 된다.\n",
    "ubuntu@namenode:~/.ssh$ hdfs namenode -format\n",
    "20/08/02 13:15:46 INFO namenode.NameNode: STARTUP_MSG:\n",
    "/************************************************************\n",
    "STARTUP_MSG: Starting NameNode\n",
    "STARTUP_MSG:   host = namenode/10.1.10.19\n",
    "STARTUP_MSG:   args = [-format]\n",
    "STARTUP_MSG:   version = 2.9.2\n",
    "STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.9.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.9.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.9.2.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.9.2-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.9.2.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.9.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.9.2-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.2-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.2-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.2.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.9.2-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.9.2.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.2.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.9.2.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.9.2.jar:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/yarn/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsch-0.1.54.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop/share/hadoop/yarn/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/json-smart-1.3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.9.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.9.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.9.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.9.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-2.9.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.9.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.9.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.9.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.9.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.9.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.9.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.9.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.9.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.9.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.9.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.5.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.9.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.9.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.9.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.9.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.9.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.9.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.9.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.2-tests.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar\n",
    "STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 826afbeae31ca687bc2f8471dc841b66ed2c6704; compiled by 'ajisaka' on 2018-11-13T12:42Z\n",
    "STARTUP_MSG:   java = 1.8.0_252\n",
    "************************************************************/\n",
    "20/08/02 13:15:46 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
    "20/08/02 13:15:46 INFO namenode.NameNode: createNameNode [-format]\n",
    "Formatting using clusterid: CID-1fb18375-939c-45e7-95aa-2b654c02cddf\n",
    "20/08/02 13:15:46 INFO namenode.FSEditLog: Edit logging is async:true\n",
    "20/08/02 13:15:46 INFO namenode.FSNamesystem: KeyProvider: null\n",
    "20/08/02 13:15:46 INFO namenode.FSNamesystem: fsLock is fair: true\n",
    "20/08/02 13:15:46 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
    "20/08/02 13:15:46 INFO namenode.FSNamesystem: fsOwner             = ubuntu (auth:SIMPLE)\n",
    "20/08/02 13:15:46 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
    "20/08/02 13:15:46 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
    "20/08/02 13:15:46 INFO namenode.FSNamesystem: HA Enabled: false\n",
    "20/08/02 13:15:46 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
    "20/08/02 13:15:46 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
    "20/08/02 13:15:46 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
    "20/08/02 13:15:46 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
    "20/08/02 13:15:46 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Aug 02 13:15:46\n",
    "20/08/02 13:15:46 INFO util.GSet: Computing capacity for map BlocksMap\n",
    "20/08/02 13:15:46 INFO util.GSet: VM type       = 64-bit\n",
    "20/08/02 13:15:46 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB\n",
    "20/08/02 13:15:46 INFO util.GSet: capacity      = 2^21 = 2097152 entries\n",
    "20/08/02 13:15:46 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\n",
    "20/08/02 13:15:46 WARN conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS\n",
    "20/08/02 13:15:46 WARN conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\n",
    "20/08/02 13:15:46 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
    "20/08/02 13:15:46 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
    "20/08/02 13:15:46 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
    "20/08/02 13:15:46 INFO blockmanagement.BlockManager: defaultReplication         = 3\n",
    "20/08/02 13:15:46 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
    "20/08/02 13:15:46 INFO blockmanagement.BlockManager: minReplication             = 1\n",
    "20/08/02 13:15:46 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
    "20/08/02 13:15:46 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\n",
    "20/08/02 13:15:46 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
    "20/08/02 13:15:46 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
    "20/08/02 13:15:46 INFO namenode.FSNamesystem: Append Enabled: true\n",
    "20/08/02 13:15:46 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215\n",
    "20/08/02 13:15:46 INFO util.GSet: Computing capacity for map INodeMap\n",
    "20/08/02 13:15:46 INFO util.GSet: VM type       = 64-bit\n",
    "20/08/02 13:15:46 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB\n",
    "20/08/02 13:15:46 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
    "20/08/02 13:15:46 INFO namenode.FSDirectory: ACLs enabled? false\n",
    "20/08/02 13:15:46 INFO namenode.FSDirectory: XAttrs enabled? true\n",
    "20/08/02 13:15:46 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
    "20/08/02 13:15:46 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false\n",
    "20/08/02 13:15:46 INFO util.GSet: Computing capacity for map cachedBlocks\n",
    "20/08/02 13:15:46 INFO util.GSet: VM type       = 64-bit\n",
    "20/08/02 13:15:46 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB\n",
    "20/08/02 13:15:46 INFO util.GSet: capacity      = 2^18 = 262144 entries\n",
    "20/08/02 13:15:46 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
    "20/08/02 13:15:46 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
    "20/08/02 13:15:46 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
    "20/08/02 13:15:46 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
    "20/08/02 13:15:46 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
    "20/08/02 13:15:46 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
    "20/08/02 13:15:46 INFO util.GSet: VM type       = 64-bit\n",
    "20/08/02 13:15:46 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB\n",
    "20/08/02 13:15:46 INFO util.GSet: capacity      = 2^15 = 32768 entries\n",
    "20/08/02 13:15:46 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1427593057-10.1.10.19-1596374146724\n",
    "20/08/02 13:15:46 INFO common.Storage: Storage directory /usr/local/hadoop/data/hdfs/namenode has been successfully formatted.\n",
    "20/08/02 13:15:46 INFO namenode.FSImageFormatProtobuf: Saving image file /usr/local/hadoop/data/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\n",
    "20/08/02 13:15:46 INFO namenode.FSImageFormatProtobuf: Image file /usr/local/hadoop/data/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 325 bytes saved in 0 seconds .\n",
    "20/08/02 13:15:46 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
    "20/08/02 13:15:46 INFO namenode.NameNode: SHUTDOWN_MSG:\n",
    "/************************************************************\n",
    "SHUTDOWN_MSG: Shutting down NameNode at namenode/10.1.10.19\n",
    "************************************************************/\n",
    "\n",
    "# HDFS 구동\n",
    "# sudo start-dfs.sh와 같이 루트권한으로 실행하게 되면 노드 통신간 퍼미션 디나이 오류가 발생하니 주의해야한다.\n",
    "ubuntu@namenode:~$ /usr/local/hadoop/sbin/start-dfs.sh\n",
    "Starting namenodes on [ec2-13-125-235-220.ap-northeast-2.compute.amazonaws.com]\n",
    "ec2-13-125-235-220.ap-northeast-2.compute.amazonaws.com: starting namenode, logging to /usr/local/hadoop/logs/hadoop-ubuntu-namenode-namenode.out\n",
    "datanode1: starting datanode, logging to /usr/local/hadoop/logs/hadoop-ubuntu-datanode-datanode1.out\n",
    "datanode2: starting datanode, logging to /usr/local/hadoop/logs/hadoop-ubuntu-datanode-datanode2.out\n",
    "Starting secondary namenodes [0.0.0.0]\n",
    "The authenticity of host '0.0.0.0 (0.0.0.0)' can't be established.\n",
    "ECDSA key fingerprint is SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.\n",
    "Are you sure you want to continue connecting (yes/no)? yes\n",
    "0.0.0.0: Warning: Permanently added '0.0.0.0' (ECDSA) to the list of known hosts.\n",
    "0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-ubuntu-secondarynamenode-namenode.out\n",
    "    \n",
    "# hadoop yarn 구동\n",
    "# sudo start-yarn.sh와 같이 루트권한으로 실행하게 되면 노드 통신간 퍼미션 디나이 오류가 발생하니 주의해야한다.\n",
    "ubuntu@namenode:~$ /usr/local/hadoop/sbin/start-yarn.sh\n",
    "starting yarn daemons\n",
    "starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-ubuntu-resourcemanager-namenode.out\n",
    "datanode1: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-ubuntu-nodemanager-datanode1.out\n",
    "datanode2: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-ubuntu-nodemanager-datanode2.out\n",
    "\n",
    "# history 서버 구동\n",
    "ubuntu@namenode:~$ /usr/local/hadoop/sbin/mr-jobhistory-daemon.sh start historyserver\n",
    "starting historyserver, logging to /usr/local/hadoop/logs/mapred-ubuntu-historyserver-namenode.out\n",
    "\n",
    "# 자바 프로세스로도 확인\n",
    "ubuntu@namenode:~$ jps\n",
    "2656 JobHistoryServer\n",
    "1942 NameNode\n",
    "2200 SecondaryNameNode\n",
    "2729 Jps\n",
    "2366 ResourceManager\n",
    "\n",
    "\n",
    "\n",
    "##--데이터노드에서도 jps 명령어를 실행하면 아래와 같이 자바 프로세스가 돌고 있을것이다.--##\n",
    "\n",
    "ubuntu@datanode1:~$ jps\n",
    "2018 Jps\n",
    "1701 DataNode\n",
    "1895 NodeManager\n",
    "\n",
    "##-----------------------------------------------------------------------------------##\n",
    "\n",
    "\n",
    "# HDFS 어드민 리포트 확인\n",
    "ubuntu@namenode:/usr/local/hadoop$ hdfs dfsadmin -report\n",
    "Configured Capacity: 62317690880 (58.04 GB)\n",
    "Present Capacity: 55728148480 (51.90 GB)\n",
    "DFS Remaining: 55728099328 (51.90 GB)\n",
    "DFS Used: 49152 (48 KB)\n",
    "DFS Used%: 0.00%\n",
    "Under replicated blocks: 0\n",
    "Blocks with corrupt replicas: 0\n",
    "Missing blocks: 0\n",
    "Missing blocks (with replication factor 1): 0\n",
    "Pending deletion blocks: 0\n",
    "    \n",
    "-------------------------------------------------\n",
    "\n",
    "Live datanodes (2):\n",
    "    \n",
    "Name: 10.1.10.127:50010 (datanode1)\n",
    "Hostname: ip-10-1-10-127.ap-northeast-2.compute.internal\n",
    "Decommission Status : Normal\n",
    "Configured Capacity: 31158845440 (29.02 GB)\n",
    "DFS Used: 24576 (24 KB)\n",
    "Non DFS Used: 3277991936 (3.05 GB)\n",
    "DFS Remaining: 27864051712 (25.95 GB)\n",
    "DFS Used%: 0.00%\n",
    "DFS Remaining%: 89.43%\n",
    "Configured Cache Capacity: 0 (0 B)\n",
    "Cache Used: 0 (0 B)\n",
    "Cache Remaining: 0 (0 B)\n",
    "Cache Used%: 100.00%\n",
    "Cache Remaining%: 0.00%\n",
    "Xceivers: 1\n",
    "Last contact: Sun Aug 02 13:23:23 UTC 2020\n",
    "Last Block Report: Sun Aug 02 13:17:41 UTC 2020\n",
    "\n",
    "Name: 10.1.10.39:50010 (datanode2)\n",
    "Hostname: ip-10-1-10-39.ap-northeast-2.compute.internal\n",
    "Decommission Status : Normal\n",
    "Configured Capacity: 31158845440 (29.02 GB)\n",
    "DFS Used: 24576 (24 KB)\n",
    "Non DFS Used: 3277996032 (3.05 GB)\n",
    "DFS Remaining: 27864047616 (25.95 GB)\n",
    "DFS Used%: 0.00%\n",
    "DFS Remaining%: 89.43%\n",
    "Configured Cache Capacity: 0 (0 B)\n",
    "Cache Used: 0 (0 B)\n",
    "Cache Remaining: 0 (0 B)\n",
    "Cache Used%: 100.00%\n",
    "Cache Remaining%: 0.00%\n",
    "Xceivers: 1\n",
    "Last contact: Sun Aug 02 13:23:24 UTC 2020\n",
    "Last Block Report: Sun Aug 02 13:17:51 UTC 2020\n",
    "            \n",
    "## 또는 HADOOP UI(:50070)과 YARN UI(:8088)을 통해서도 하둡이 돌아가는 것을 확인할 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 11) 간단한 hadoop job 테스트\n",
    "\n",
    "네임노드에서 아래와 같은 명령어로 테스트를 진행해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# home directory 생성\n",
    "ubuntu@namenode:/usr/local/hadoop$ hdfs dfs -mkdir -p /user/ubuntu\n",
    "\n",
    "# create random-data\n",
    "ubuntu@namenode:/usr/local/hadoop$ hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-e*.jar teragen 500000 random-data\n",
    "20/08/02 13:28:22 INFO client.RMProxy: Connecting to ResourceManager at ec2-13-125-235-220.ap-northeast-2.amazonaws.com/10.1.10.19:8032\n",
    "20/08/02 13:28:23 INFO terasort.TeraGen: Generating 500000 using 2\n",
    "20/08/02 13:28:23 INFO mapreduce.JobSubmitter: number of splits:2\n",
    "20/08/02 13:28:23 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled iated. Instead, use yarn.system-metrics-publisher.enabled\n",
    "20/08/02 13:28:23 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1596374353473_0001\n",
    "20/08/02 13:28:24 INFO impl.YarnClientImpl: Submitted application application_1596374353473_0001\n",
    "20/08/02 13:28:24 INFO mapreduce.Job: The url to track the job: http://ec2-13-125-235-220.ap-northeast-2.amazonaws.com:8088/proxy/application_1596374353473_0001/\n",
    "20/08/02 13:28:24 INFO mapreduce.Job: Running job: job_1596374353473_0001\n",
    "20/08/02 13:28:35 INFO mapreduce.Job: Job job_1596374353473_0001 running in uber mode : false\n",
    "20/08/02 13:28:35 INFO mapreduce.Job:  map 0% reduce 0%\n",
    "20/08/02 13:28:39 INFO mapreduce.Job:  map 50% reduce 0%\n",
    "20/08/02 13:28:46 INFO mapreduce.Job:  map 100% reduce 0%\n",
    "20/08/02 13:28:46 INFO mapreduce.Job: Job job_1596374353473_0001 completed successfully\n",
    "20/08/02 13:28:46 INFO mapreduce.Job: Counters: 31\n",
    "        File System Counters\n",
    "                FILE: Number of bytes read=0\n",
    "                FILE: Number of bytes written=396576\n",
    "                FILE: Number of read operations=0\n",
    "                FILE: Number of large read operations=0\n",
    "                FILE: Number of write operations=0\n",
    "                HDFS: Number of bytes read=167\n",
    "                HDFS: Number of bytes written=50000000\n",
    "                HDFS: Number of read operations=8\n",
    "                HDFS: Number of large read operations=0\n",
    "                HDFS: Number of write operations=4\n",
    "        Job Counters\n",
    "                Launched map tasks=2\n",
    "                Other local map tasks=2\n",
    "                Total time spent by all maps in occupied slots (ms)=10786\n",
    "                Total time spent by all reduces in occupied slots (ms)=0\n",
    "                Total time spent by all map tasks (ms)=10786\n",
    "                Total vcore-milliseconds taken by all map tasks=10786\n",
    "                Total megabyte-milliseconds taken by all map tasks=11044864\n",
    "        Map-Reduce Framework\n",
    "                Map input records=500000\n",
    "                Map output records=500000\n",
    "                Input split bytes=167\n",
    "                Spilled Records=0\n",
    "                Failed Shuffles=0\n",
    "                Merged Map outputs=0\n",
    "                GC time elapsed (ms)=117\n",
    "                CPU time spent (ms)=2330\n",
    "                Physical memory (bytes) snapshot=394739712\n",
    "                Virtual memory (bytes) snapshot=3880120320\n",
    "                Total committed heap usage (bytes)=282066944\n",
    "        org.apache.hadoop.examples.terasort.TeraGen$Counters\n",
    "                CHECKSUM=1074598070305752\n",
    "        File Input Format Counters\n",
    "                Bytes Read=0\n",
    "        File Output Format Counters\n",
    "                Bytes Written=50000000\n",
    "            \n",
    "# sort job         \n",
    "ubuntu@namenode:/usr/local/hadoop$ hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-e*.jar terasort random-data sorted-data\n",
    "20/08/02 13:28:54 INFO terasort.TeraSort: starting\n",
    "20/08/02 13:28:55 INFO input.FileInputFormat: Total input files to process : 2\n",
    "Spent 73ms computing base-splits.\n",
    "Spent 2ms computing TeraScheduler splits.\n",
    "Computing input splits took 76ms\n",
    "Sampling 2 splits of 2\n",
    "Making 1 from 100000 sampled records\n",
    "Computing parititions took 447ms\n",
    "Spent 525ms computing partitions.\n",
    "20/08/02 13:28:56 INFO client.RMProxy: Connecting to ResourceManager at ec2-13-125-235-220.ap-northeast-2.amazonaws.com/10.1.10.19:8032\n",
    "20/08/02 13:28:56 INFO mapreduce.JobSubmitter: number of splits:2\n",
    "20/08/02 13:28:56 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled iated. Instead, use yarn.system-metrics-publisher.enabled\n",
    "20/08/02 13:28:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1596374353473_0002\n",
    "20/08/02 13:28:56 INFO impl.YarnClientImpl: Submitted application application_1596374353473_0002\n",
    "20/08/02 13:28:56 INFO mapreduce.Job: The url to track the job: http://ec2-13-125-235-220.ap-northeast-2.amazonaws.com:8088/proxy/application_1596374353473_0002/\n",
    "20/08/02 13:28:56 INFO mapreduce.Job: Running job: job_1596374353473_0002\n",
    "20/08/02 13:29:01 INFO mapreduce.Job: Job job_1596374353473_0002 running in uber mode : false\n",
    "20/08/02 13:29:01 INFO mapreduce.Job:  map 0% reduce 0%\n",
    "20/08/02 13:29:07 INFO mapreduce.Job:  map 100% reduce 0%\n",
    "20/08/02 13:29:12 INFO mapreduce.Job:  map 100% reduce 100%\n",
    "20/08/02 13:29:13 INFO mapreduce.Job: Job job_1596374353473_0002 completed successfully\n",
    "20/08/02 13:29:14 INFO mapreduce.Job: Counters: 49\n",
    "        File System Counters\n",
    "                FILE: Number of bytes read=52000006\n",
    "                FILE: Number of bytes written=104599170\n",
    "                FILE: Number of read operations=0\n",
    "                FILE: Number of large read operations=0\n",
    "                FILE: Number of write operations=0\n",
    "                HDFS: Number of bytes read=50000338\n",
    "                HDFS: Number of bytes written=50000000\n",
    "                HDFS: Number of read operations=9\n",
    "                HDFS: Number of large read operations=0\n",
    "                HDFS: Number of write operations=2\n",
    "        Job Counters\n",
    "                Launched map tasks=2\n",
    "                Launched reduce tasks=1\n",
    "                Data-local map tasks=2\n",
    "                Total time spent by all maps in occupied slots (ms)=7650\n",
    "                Total time spent by all reduces in occupied slots (ms)=2807\n",
    "                Total time spent by all map tasks (ms)=7650\n",
    "                Total time spent by all reduce tasks (ms)=2807\n",
    "                Total vcore-milliseconds taken by all map tasks=7650\n",
    "                Total vcore-milliseconds taken by all reduce tasks=2807\n",
    "                Total megabyte-milliseconds taken by all map tasks=7833600\n",
    "                Total megabyte-milliseconds taken by all reduce tasks=2874368\n",
    "        Map-Reduce Framework\n",
    "                Map input records=500000\n",
    "                Map output records=500000\n",
    "                Map output bytes=51000000\n",
    "                Map output materialized bytes=52000012\n",
    "                Input split bytes=338\n",
    "                Combine input records=0\n",
    "                Combine output records=0\n",
    "                Reduce input groups=500000\n",
    "                Reduce shuffle bytes=52000012\n",
    "                Reduce input records=500000\n",
    "                Reduce output records=500000\n",
    "                Spilled Records=1000000\n",
    "                Shuffled Maps =2\n",
    "                Failed Shuffles=0\n",
    "                Merged Map outputs=2\n",
    "                GC time elapsed (ms)=304\n",
    "                CPU time spent (ms)=6570\n",
    "                Physical memory (bytes) snapshot=904011776\n",
    "                Virtual memory (bytes) snapshot=5790863360\n",
    "                Total committed heap usage (bytes)=588251136\n",
    "        Shuffle Errors\n",
    "                BAD_ID=0\n",
    "                CONNECTION=0\n",
    "                IO_ERROR=0\n",
    "                WRONG_LENGTH=0\n",
    "                WRONG_MAP=0\n",
    "                WRONG_REDUCE=0\n",
    "        File Input Format Counters\n",
    "                Bytes Read=50000000\n",
    "        File Output Format Counters\n",
    "                Bytes Written=50000000\n",
    "20/08/02 13:29:14 INFO terasort.TeraSort: done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 참고사항\n",
    "\n",
    "instance를 중지시킬 경우에는 모든 프로세스를 아래와 같이 먼저 종료해줘야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop-yarn.sh\n",
    "stop-dfs.sh\n",
    "mr-jobhistory-daemon.sh stop historyserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약에 datanode가 제대로 실행되지 않은 경우에 재구축 방법은 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# namenode에서 아래와 같은 명령어 실행\n",
    "rm -rf /usr/local/hadoop/data/hdfs/namenode/*\n",
    "# 위에 명령어 실행 후 reboot 불필요\n",
    "\n",
    "# datanode에서 아래와 같은 명령어 실행\n",
    "rm -rf /usr/local/hadoop/data/hdfs/datanode/*\n",
    "rm -rf /usr/local/hadoop/yarn/local/*\n",
    "sudo reboot\n",
    "# 위의 명령어를 실행하고 나서 다시 hdfs namenode -format부터 실행해본다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
