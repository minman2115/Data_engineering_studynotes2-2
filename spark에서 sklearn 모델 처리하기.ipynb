{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "Data_Engineering_TIL(20201026)\n",
    "\n",
    "### [실습 시 참고자료]\n",
    "\n",
    "블로그글 'Running Scikit-learn models in Apache Pyspark'을 읽고 실습한 내용입니다.\n",
    "\n",
    "** URL : https://medium.com/@the.data.yoga/running-scikit-models-in-apache-pyspark-4a2ac5e693c4\n",
    "\n",
    "\n",
    "### [실습전 참고사항]\n",
    "\n",
    "1) Spark Shuffle 이란\n",
    "\n",
    "Shuffling is a mechanism Spark uses to redistribute the data across different executors and even across machines. Spark shuffling triggers when we perform certain transformation operations like gropByKey(), reducebyKey(), join() on RDD and DataFrame.\n",
    "\n",
    "Spark Shuffle is an expensive operation since it involves the following\n",
    "\n",
    "- Disk I/O\n",
    "\n",
    "\n",
    "- Involves data serialization and deserialization\n",
    "\n",
    "\n",
    "- Network I/O\n",
    "\n",
    "When creating an RDD, Spark doesn't necessarily store the data for all keys in a partition since at the time of creation there is no way we can set the key for data set.\n",
    "\n",
    "2) spark broadcast 이란\n",
    "\n",
    "In Spark RDD and DataFrame, Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, spark distributes broadcast variables to the machine using efficient broadcast algorithms to reduce communication costs.\n",
    "\n",
    "\n",
    "### [실습목표]\n",
    "\n",
    "\n",
    "Scikit-learn model을 Pyspark 엔진을 이용해서 prediction 해본다.\n",
    "\n",
    "\n",
    "### [실습내용]\n",
    "\n",
    "#### step 1) training model with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "Y = iris.target\n",
    "\n",
    "logreg = LogisticRegression(C=1e5)\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg.fit(X, Y)\n",
    "#prediction\n",
    "logreg.predict(X[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 2) save model to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(logreg, open( \"model save path\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 3) make pyspark wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(s_l, s_w, p_l, p_w):\n",
    "    '''\n",
    "    accepts the four features as parameters and returns the predicted score as output\n",
    "    '''\n",
    "    \n",
    "    #open picked model\n",
    "    serialized_model = open(\"model save path\", \"rb\")\n",
    "    model = pickle.load(serialized_model)\n",
    "    serialized_model.close()\n",
    "    \n",
    "    #call predict method for model\n",
    "    return model.predict([s_l, s_w, p_l, p_w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 4) Turning the python function to a pyspark UDF\n",
    "\n",
    "Option 1 (Simply registering python function to pyspark as UDF )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "udf_predictor = udf(predictor, FloatType())\n",
    "\n",
    "#apply the udf to dataframe\n",
    "df_prediction = df.withColumn(\"prediction\",udf_predictor(df.sepal_length,df.sepal_width,df.petal_length,df.petal_width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2(Broadcast the model to spark executors and predict)\n",
    "\n",
    "In order to remove the inefficiency in option 1, this technique allows reading the model once from the disk and send(broadcast) the model to all spark executors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open picked model\n",
    "serialized_model = open(\"model save path\", \"rb\")\n",
    "model = pickle.load(serialized_model)\n",
    "serialized_model.close()\n",
    "\n",
    "#broadcast model to spark executors using spark context(sc)\n",
    "sc.broadcast(model)\n",
    "\n",
    "#update prediction method\n",
    "def predictor(s_l, s_w, p_l, p_w):\n",
    "    #call predict method for model\n",
    "    return model.predict([s_l, s_w, p_l, p_w])\n",
    "\n",
    "#register python method as spark UDF and call over dataframe\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "udf_predictor = udf(predictor, FloatType())\n",
    "\n",
    "#apply the udf to dataframe\n",
    "df_prediction = df.withColumn(\"prediction\",udf_predictor(df.sepal_length,df.sepal_width, df.petal_length, df.petal_width))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
