{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "Data_Engineering_TIL(20201016)\n",
    "\n",
    "[실습내용]\n",
    "\n",
    "step 1) Airflow server 구동\n",
    "\n",
    "https://minman2115.github.io/DE_TIL141/ 참고\n",
    "\n",
    "step 2) Airflow server가 사용할 google smtp app 계정설정 (Gmail 앱비밀번호 생성하기)\n",
    "\n",
    "- Google Home에서 자신의 프로필 > Google 계정 관리 클릭한다.\n",
    "\n",
    "\n",
    "- 왼쪽 사이드 바에서 `보안` 클릭한다.\n",
    "\n",
    "\n",
    "- Google에 로그인 항목에서 `2단계 인증` 사용으로 선택한다.\n",
    "\n",
    "\n",
    "- 앱 비밀번호를 생성한다.\n",
    "\n",
    "\n",
    "- 노란색 박스안의 16자리 비밀번호를 메모장에 복사해둔다.\n",
    "\n",
    "\n",
    "step 2) task 실패시 email 발송을 위한 airflow config 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-1-10-227 ~]$ cd ~/airflow\n",
    "\n",
    "[ec2-user@ip-10-1-10-227 airflow]$ ls\n",
    "airflow.cfg  airflow.db  airflow-webserver.pid  logs  unittests.cfg\n",
    "\n",
    "[ec2-user@ip-10-1-10-227 airflow]$ sudo vim airflow.cfg\n",
    "# 아래와 같이 smtp 부분의 내용을 변경해준다.\n",
    "[smtp]\n",
    "smtp_host = smtp.gmail.com\n",
    "smtp_starttls = True\n",
    "smtp_ssl = False\n",
    "smtp_user = [YOUR_EMAIL_ADDRESS]\n",
    "smtp_password = [16_DIGIT_APP_PASSWORD] # 위에서 메모장에 복사해둔 16자리 Gmail 앱 비밀번호\n",
    "smtp_port = 587\n",
    "smtp_mail_from = [YOUR_EMAIL_ADDRESS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 3) DAG 생성 및 등록 \n",
    "\n",
    "task 가 실행중에 문제가 발생할 수 있도록 코드를 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-1-10-4 airflow]$ mkdir dags\n",
    "\n",
    "[ec2-user@ip-10-1-10-4 airflow]$ cd dags\n",
    "\n",
    "[ec2-user@ip-10-1-10-4 dags]$ sudo vim test.py\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "from airflow.contrib.operators.emr_create_job_flow_operator import EmrCreateJobFlowOperator\n",
    "from airflow.contrib.operators.emr_add_steps_operator import EmrAddStepsOperator\n",
    "from airflow.contrib.sensors.emr_step_sensor import EmrStepSensor\n",
    "from airflow.contrib.operators.emr_terminate_job_flow_operator import EmrTerminateJobFlowOperator\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'minman',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2020,10,14, 1, 55),\n",
    "    'email': ['[my_email_address]'],\n",
    "    'email_on_failure': True\n",
    "}\n",
    "\n",
    "dag = DAG('emr_job_flow_test',\n",
    "         default_args=default_args,\n",
    "         schedule_interval= None, # 스케쥴 인터벌을 None으로 바꾸고, 트리거로만 실행하도록 함 (온디멘드 실행)\n",
    "         catchup=False\n",
    ")\n",
    "\n",
    "JOB_FLOW_OVERRIDES = {\n",
    "    'Name' : 'pms-EMRtest-test',\n",
    "    'LogUri' : 's3://[bucket_name]/',\n",
    "    'ReleaseLabel' : 'emr-5.28.1',\n",
    "    'Instances' : {\n",
    "            'Ec2KeyName': '[key_name]',\n",
    "            'Ec2SubnetId': 'subnet-xxxxxxxxxxxxxxxxxx',\n",
    "            'EmrManagedMasterSecurityGroup': 'sg-xxxxxxxxxxxxxxxxxxxxx',\n",
    "            'EmrManagedSlaveSecurityGroup': 'sg-xxxxxxxxxxxxxxxxxxxx',\n",
    "            'KeepJobFlowAliveWhenNoSteps': True,\n",
    "            'TerminationProtected': False,\n",
    "            'InstanceGroups': [{\n",
    "                'InstanceRole': 'MASTER',\n",
    "                \"InstanceCount\": 1,\n",
    "                    \"InstanceType\": 'm5.2xlarge',\n",
    "                    \"Market\": \"SPOT\",\n",
    "                    \"Name\": \"Master\"\n",
    "                }, {\n",
    "                    'InstanceRole': 'CORE',\n",
    "                    \"InstanceCount\": 1,\n",
    "                    \"InstanceType\": 'm5.2xlarge',\n",
    "                    \"Market\": \"SPOT\",\n",
    "                    \"Name\": \"Core\",\n",
    "                }, {\n",
    "                    'InstanceRole': 'TASK',\n",
    "                    \"InstanceCount\": 1,\n",
    "                    \"InstanceType\": 'm5.2xlarge',\n",
    "                    \"Market\": \"SPOT\",\n",
    "                    \"Name\": \"Core\",\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    'Applications':[{'Name': 'Spark'},{'Name': 'Hadoop'},{'Name': 'Hive'}],\n",
    "    'JobFlowRole':'EMR_EC2_DefaultRole',\n",
    "    'ServiceRole':'EMR_DefaultRole',\n",
    "    'StepConcurrencyLevel': 10,\n",
    "    'Tags' : [{'Key': 'name', 'Value': 'pms-EMR-test'},\n",
    "              {'Key': 'expiry-date', 'Value': '2020-10-13'},\n",
    "              {'Key': 'owner', 'Value': 'pms'}],\n",
    "    'BootstrapActions':[\n",
    "            {\n",
    "                'Name': 'Maximize Spark Default Config',\n",
    "                'ScriptBootstrapAction': {'Path': 's3://[bucket_name]/maximize-spark-default-config.sh'}\n",
    "                # https://github.com/aws-samples/emr-bootstrap-actions/blob/master/spark/maximize-spark-default-config\n",
    "            }\n",
    "    ],\n",
    "    \"VisibleToAllUsers\": True\n",
    "}\n",
    "\n",
    "cluster_creator = EmrCreateJobFlowOperator(\n",
    "   task_id='create_job_flow',\n",
    "   job_flow_overrides=JOB_FLOW_OVERRIDES,\n",
    "   aws_conn_id='aws_default',\n",
    "   emr_conn_id='emr_default',\n",
    "   dag=dag\n",
    ")\n",
    "\n",
    "PRE_STEP = [\n",
    "    {\n",
    "        'Name': 'spark_job_01',\n",
    "        #'ActionOnFailure': 'TERMINATE_CLUSTER',\n",
    "        'HadoopJarStep': {\n",
    "            'Jar': 'command-runner.jar',\n",
    "            'Args': ['spark-submit',\n",
    "                      '--deploy-mode', 'cluster',\n",
    "                      '--master', 'yarn',\n",
    "                      's3://[bucket_name]/spark_job_07.py'] # Error를 발생하는 코드를 삽입한 python script\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "step_adder_pre_step = EmrAddStepsOperator(\n",
    "    task_id='pre_step',\n",
    "    job_flow_id=\"{{ task_instance.xcom_pull('create_job_flow', key='return_value') }}\",\n",
    "    aws_conn_id='aws_default',\n",
    "    steps=PRE_STEP,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "step_checker_pre = EmrStepSensor(\n",
    "        task_id='watch_step_of_pre',\n",
    "        job_flow_id=\"{{ task_instance.xcom_pull('create_job_flow', key='return_value') }}\",\n",
    "        step_id=\"{{ task_instance.xcom_pull('pre_step', key='return_value')[0] }}\",\n",
    "        aws_conn_id='aws_default',\n",
    "        dag=dag\n",
    " )\n",
    "\n",
    "ACTUAL_STEP = [\n",
    "    {\n",
    "        'Name': 'spark_job_03',\n",
    "         # 'ActionOnFailure': 'TERMINATE_CLUSTER',\n",
    "        'HadoopJarStep': {\n",
    "            'Jar': 'command-runner.jar',\n",
    "            'Args': ['spark-submit',\n",
    "                      '--deploy-mode', 'cluster',\n",
    "                      '--master', 'yarn',\n",
    "                      's3://[bucket_name]/spark_job_03.py']\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "step_adder_actual_step = EmrAddStepsOperator(\n",
    "    task_id='actual_step',\n",
    "    job_flow_id=\"{{ task_instance.xcom_pull('create_job_flow', key='return_value') }}\",\n",
    "    aws_conn_id='aws_default',\n",
    "    steps=ACTUAL_STEP,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "step_checker_actual = EmrStepSensor(\n",
    "        task_id='watch_step_of_actual',\n",
    "        job_flow_id=\"{{ task_instance.xcom_pull('create_job_flow', key='return_value') }}\",\n",
    "        step_id=\"{{ task_instance.xcom_pull('actual_step', key='return_value')[0] }}\",\n",
    "        aws_conn_id='aws_default',\n",
    "        dag=dag\n",
    ")\n",
    "\n",
    "cluster_remover = EmrTerminateJobFlowOperator(\n",
    "    task_id='remove_cluster',\n",
    "    job_flow_id=\"{{ task_instance.xcom_pull('create_job_flow', key='return_value') }}\",\n",
    "    aws_conn_id='aws_default',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "cluster_creator >> step_adder_pre_step >> step_checker_pre >> cluster_remover\n",
    "cluster_creator >> step_adder_actual_step >> step_checker_actual >> cluster_remover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실행하면 EMR step에서 spark-submit --deploy-mode cluster --master yarn s3://pms-bucket-test/spark_job_07.py가 fail날 것이다.\n",
    "\n",
    "Airflow webUI에 접속해서 생성한 emr_job_flow_test DAG 메뉴의 Graph view를 보면 해당 task가 실행되는 것을 체크하는 watch_step_of_pre task가 빨간불이 뜨면서 failed가 뜬 것을 확인할 수 있다.\n",
    "\n",
    "그러면 등록한 메일에 아래와 같이 메일이 날라온 것을 확인할 수 있다.\n",
    "\n",
    "메일 제목 : Airflow alert: <TaskInstance: emr_job_flow_test.watch_step_of_pre 2020-10-16T04:15:46.233306+00:00 [failed]>\n",
    "\n",
    "내용 : \n",
    "\n",
    "Try 1 out of 1\n",
    "\n",
    "Exception:\n",
    "\n",
    "EMR job failed for reason None with message Exception in thread \"main\" org.apache.spark.SparkException: Application \n",
    "application_1602822022591_0001 finished with failed status and log file s3://pms-bucket-test/j-3CTW0D7GR53SD/steps/s-308IQQ3TXMGKO/stderr.gz\n",
    "\n",
    "Log: Link\n",
    "\n",
    "Host: ip-10-1-10-227.ap-northeast-2.compute.internal\n",
    "\n",
    "Log file: /home/ec2-user/airflow/logs/emr_job_flow_test/watch_step_of_pre/2020-10-16T04:15:46.233306+00:00.log\n",
    "\n",
    "Mark success: Link\n",
    "\n",
    "[참고사항]\n",
    "\n",
    "- 위의 코드에서 spark_job_03.py 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ABC data test\").getOrCreate()\n",
    "\n",
    "_list = ['D','E','F']\n",
    "\n",
    "for elem in _list:\n",
    "    \n",
    "    df = spark.read.option(\"header\",\"true\").parquet(\"s3a://mybucket/testdata/merge_data/*.parquet\")\n",
    "    \n",
    "    df = df.withColumnRenamed(\"event_time\",\"event_time_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"event_type\",\"event_type_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"product_id\",\"product_id_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"category_id\",\"category_id_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"category_code\",\"category_code_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"brand\",\"brand_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"price\",\"price_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"user_id\",\"user_id_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"year\",\"year_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"user_session\",\"user_session_{}\".format(elem))\\\n",
    "           .withColumn(\"id\", F.monotonically_increasing_id())\n",
    "\n",
    "    df.write.option(\"header\", \"true\").csv(\"s3a://mybucket/{}\".format(elem))\n",
    "    \n",
    "df.show()\n",
    "\n",
    "print(\"record count : \", df.count())\n",
    "\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 코드에서 spark_job_07.py 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print) admka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### # 만약에 특정 Task에서만 실패했을때 메일링을 하려고 한다면 아래와 같이 스크립트를 짜면된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-1-10-4 dags]$ sudo vim test.py\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "from airflow.contrib.operators.emr_create_job_flow_operator import EmrCreateJobFlowOperator\n",
    "from airflow.contrib.operators.emr_add_steps_operator import EmrAddStepsOperator\n",
    "from airflow.contrib.sensors.emr_step_sensor import EmrStepSensor\n",
    "from airflow.contrib.operators.emr_terminate_job_flow_operator import EmrTerminateJobFlowOperator\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'minman',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2020,10,14, 1, 55),\n",
    "    'email': ['[my_email_address]'],\n",
    "    # email_on_failure 를 False로 안해주면 아래 메일링과 함께 중복으로 메일링이 오게됨\n",
    "    'email_on_failure': False \n",
    "}\n",
    "\n",
    "dag = DAG('emr_job_flow_test',\n",
    "         default_args=default_args,\n",
    "         schedule_interval= None, # 스케쥴 인터벌을 None으로 바꾸고, 트리거로만 실행하도록 함 (온디멘드 실행)\n",
    "         catchup=False\n",
    ")\n",
    "\n",
    "JOB_FLOW_OVERRIDES = {\n",
    "    'Name' : 'pms-EMRtest-test',\n",
    "    'LogUri' : 's3://[bucket_name]/',\n",
    "    'ReleaseLabel' : 'emr-5.28.1',\n",
    "    'Instances' : {\n",
    "            'Ec2KeyName': '[key_name]',\n",
    "            'Ec2SubnetId': 'subnet-xxxxxxxxxxxxxxxxxx',\n",
    "            'EmrManagedMasterSecurityGroup': 'sg-xxxxxxxxxxxxxxxxxxxxx',\n",
    "            'EmrManagedSlaveSecurityGroup': 'sg-xxxxxxxxxxxxxxxxxxxx',\n",
    "            'KeepJobFlowAliveWhenNoSteps': True,\n",
    "            'TerminationProtected': False,\n",
    "            'InstanceGroups': [{\n",
    "                'InstanceRole': 'MASTER',\n",
    "                \"InstanceCount\": 1,\n",
    "                    \"InstanceType\": 'm5.2xlarge',\n",
    "                    \"Market\": \"SPOT\",\n",
    "                    \"Name\": \"Master\"\n",
    "                }, {\n",
    "                    'InstanceRole': 'CORE',\n",
    "                    \"InstanceCount\": 1,\n",
    "                    \"InstanceType\": 'm5.2xlarge',\n",
    "                    \"Market\": \"SPOT\",\n",
    "                    \"Name\": \"Core\",\n",
    "                }, {\n",
    "                    'InstanceRole': 'TASK',\n",
    "                    \"InstanceCount\": 1,\n",
    "                    \"InstanceType\": 'm5.2xlarge',\n",
    "                    \"Market\": \"SPOT\",\n",
    "                    \"Name\": \"Core\",\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    'Applications':[{'Name': 'Spark'},{'Name': 'Hadoop'},{'Name': 'Hive'}],\n",
    "    'JobFlowRole':'EMR_EC2_DefaultRole',\n",
    "    'ServiceRole':'EMR_DefaultRole',\n",
    "    'StepConcurrencyLevel': 10,\n",
    "    'Tags' : [{'Key': 'name', 'Value': 'pms-EMR-test'},\n",
    "              {'Key': 'expiry-date', 'Value': '2020-10-13'},\n",
    "              {'Key': 'owner', 'Value': 'pms'}],\n",
    "    'BootstrapActions':[\n",
    "            {\n",
    "                'Name': 'Maximize Spark Default Config',\n",
    "                'ScriptBootstrapAction': {'Path': 's3://[bucket_name]/maximize-spark-default-config.sh'}\n",
    "                # https://github.com/aws-samples/emr-bootstrap-actions/blob/master/spark/maximize-spark-default-config\n",
    "            }\n",
    "    ],\n",
    "    \"VisibleToAllUsers\": True\n",
    "}\n",
    "\n",
    "cluster_creator = EmrCreateJobFlowOperator(\n",
    "   task_id='create_job_flow',\n",
    "   job_flow_overrides=JOB_FLOW_OVERRIDES,\n",
    "   aws_conn_id='aws_default',\n",
    "   emr_conn_id='emr_default',\n",
    "   dag=dag\n",
    ")\n",
    "\n",
    "PRE_STEP = [\n",
    "    {\n",
    "        'Name': 'spark_job_01',\n",
    "        #'ActionOnFailure': 'TERMINATE_CLUSTER',\n",
    "        'HadoopJarStep': {\n",
    "            'Jar': 'command-runner.jar',\n",
    "            'Args': ['spark-submit',\n",
    "                      '--deploy-mode', 'cluster',\n",
    "                      '--master', 'yarn',\n",
    "                      's3://[bucket_name]/spark_job_07.py'] # Error를 발생하는 코드를 삽입한 python script\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "step_adder_pre_step = EmrAddStepsOperator(\n",
    "    task_id='pre_step',\n",
    "    job_flow_id=\"{{ task_instance.xcom_pull('create_job_flow', key='return_value') }}\",\n",
    "    aws_conn_id='aws_default',\n",
    "    steps=PRE_STEP,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "step_checker_pre = EmrStepSensor(\n",
    "        task_id='watch_step_of_pre',\n",
    "        job_flow_id=\"{{ task_instance.xcom_pull('create_job_flow', key='return_value') }}\",\n",
    "        step_id=\"{{ task_instance.xcom_pull('pre_step', key='return_value')[0] }}\",\n",
    "        aws_conn_id='aws_default',\n",
    "        dag=dag\n",
    " )\n",
    "\n",
    "ACTUAL_STEP = [\n",
    "    {\n",
    "        'Name': 'spark_job_03',\n",
    "         # 'ActionOnFailure': 'TERMINATE_CLUSTER',\n",
    "        'HadoopJarStep': {\n",
    "            'Jar': 'command-runner.jar',\n",
    "            'Args': ['spark-submit',\n",
    "                      '--deploy-mode', 'cluster',\n",
    "                      '--master', 'yarn',\n",
    "                      's3://[bucket_name]/spark_job_03.py']\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "step_adder_actual_step = EmrAddStepsOperator(\n",
    "    task_id='actual_step',\n",
    "    job_flow_id=\"{{ task_instance.xcom_pull('create_job_flow', key='return_value') }}\",\n",
    "    aws_conn_id='aws_default',\n",
    "    steps=ACTUAL_STEP,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "step_checker_actual = EmrStepSensor(\n",
    "        task_id='watch_step_of_actual',\n",
    "        job_flow_id=\"{{ task_instance.xcom_pull('create_job_flow', key='return_value') }}\",\n",
    "        step_id=\"{{ task_instance.xcom_pull('actual_step', key='return_value')[0] }}\",\n",
    "        aws_conn_id='aws_default',\n",
    "        dag=dag\n",
    ")\n",
    "\n",
    "cluster_remover = EmrTerminateJobFlowOperator(\n",
    "    task_id='remove_cluster',\n",
    "    job_flow_id=\"{{ task_instance.xcom_pull('create_job_flow', key='return_value') }}\",\n",
    "    aws_conn_id='aws_default',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# prestep 실패시 메일링\n",
    "TaskFailed = EmailOperator (\n",
    "    dag=dag,\n",
    "    trigger_rule=TriggerRule.ONE_FAILED,\n",
    "    task_id=\"TaskFailed\",\n",
    "    to=[\"[email_address]\"],\n",
    "    subject=\"Task Failed\",\n",
    "    html_content='<h3>One of Task Failed\" </h3>')\n",
    "\n",
    "TaskFailed.set_upstream([step_adder_pre_step])\n",
    "\n",
    "# emr 생성 성공시 메일링\n",
    "TaskSucceded = EmailOperator (\n",
    "    dag=dag,\n",
    "    trigger_rule=TriggerRule.ONE_SUCCESS,\n",
    "    task_id=\"TaskSucceded\",\n",
    "    to=[\"[email_address]\"],\n",
    "    subject=\"Task Succeded\",\n",
    "    html_content='<h3>One of Task Succeded\" </h3>')\n",
    "\n",
    "TaskSucceded.set_upstream([cluster_creator])\n",
    "\n",
    "cluster_creator >> step_adder_pre_step >> step_checker_pre >> cluster_remover\n",
    "cluster_creator >> step_adder_actual_step >> step_checker_actual >> cluster_remover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TriggerRule의 attribute\n",
    "\n",
    "all_success: (기본값) 모두 성공\n",
    "\n",
    "all_failed: 모든 상위 task가 failed 또는 upstream_failed 상태\n",
    "\n",
    "all_done: 모든 task 실행을 완료\n",
    "\n",
    "one_failed: 하나 이상의 task가 실패하자마자 실행되며 모든 task가 완료 될 때까지 기다리지 않음\n",
    "\n",
    "one_success: 하나 이상의 task가 성공하자마자 실행되며 모든 task가 완료 될 때까지 기다리지 않음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
