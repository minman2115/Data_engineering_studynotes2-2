{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "Data_Engineering_TIL_(20200225)\n",
    "\n",
    "학습 시 참고자료(출처) : \n",
    "\n",
    "\n",
    "- 블로그 글 \"Spark core concepts explained\"을 읽고 공부한 내용을 정리한 노트입니다.\n",
    "\n",
    "\n",
    "- URL : https://luminousmen.com/post/spark-core-concepts-explained\n",
    "\n",
    "\n",
    "### 1. 개요\n",
    "\n",
    "스파크 아키텍처는 아래 두가지 메인 개념이 기본이라고 할 수 있다.\n",
    "\n",
    "- Resilient Distributed Dataset (RDD)\n",
    "\n",
    "\n",
    "- Directed Acyclic Graph (DAG)\n",
    "\n",
    "### 2. Resilient Distributed Dataset (RDD)\n",
    "\n",
    "![1](https://user-images.githubusercontent.com/41605276/103171172-4fb85080-488d-11eb-98ce-c7166a7aff1b.png)\n",
    "\n",
    "Example :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1582606619430_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-32-67-152.us-west-2.compute.internal:20888/proxy/application_1582606619430_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-32-91-54.us-west-2.compute.internal:8042/node/containerlogs/container_1582606619430_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[1] at RDD at PythonRDD.scala:53"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(20))  # create RDD\n",
    "\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
     ]
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2](https://user-images.githubusercontent.com/41605276/103171211-a6258f00-488d-11eb-901d-8bda8cac77b0.png)\n",
    "\n",
    "Let's check the number of partitions and data on them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    }
   ],
   "source": [
    "# get current number of paritions\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]"
     ]
    }
   ],
   "source": [
    "# collect data on driver based on partitions\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3](https://user-images.githubusercontent.com/41605276/103171238-e1c05900-488d-11eb-880f-76fc7ddafa18.png)\n",
    "\n",
    "![4](https://user-images.githubusercontent.com/41605276/75301191-74165b80-587d-11ea-9adb-a3397fb88cbc.png)\n",
    "\n",
    "![5](https://user-images.githubusercontent.com/41605276/103171260-25b35e00-488e-11eb-8658-fef655953ba9.png)\n",
    "\n",
    "![6](https://user-images.githubusercontent.com/41605276/103171282-598e8380-488e-11eb-99a2-d3a3fd10fed6.png)\n",
    "\n",
    "![7](https://user-images.githubusercontent.com/41605276/75324721-e6596100-58ba-11ea-823c-bbcafe75c9bd.png)\n",
    "\n",
    "![8](https://user-images.githubusercontent.com/41605276/103171367-06690080-488f-11eb-88c5-e4c2dce12ca4.png)\n",
    "\n",
    "Let's use filter transformation on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(2) PythonRDD[1] at RDD at PythonRDD.scala:53 []\\n |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 []'"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(20))\n",
    "filteredRDD = rdd.filter(lambda x: x > 10)\n",
    "\n",
    "## to see the execution graph; only one stage is created\n",
    "print(filteredRDD.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 12, 13, 14, 15, 16, 17, 18, 19]"
     ]
    }
   ],
   "source": [
    "filteredRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![9](https://user-images.githubusercontent.com/41605276/103171409-66f83d80-488f-11eb-8c65-5fbb2177ba47.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## group data based on mod\n",
    "groupedRDD = filteredRDD.groupBy(lambda x: x % 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(2) PythonRDD[6] at RDD at PythonRDD.scala:53 []\\n |  MapPartitionsRDD[5] at mapPartitions at PythonRDD.scala:133 []\\n |  ShuffledRDD[4] at partitionBy at NativeMethodAccessorImpl.java:0 []\\n +-(2) PairwiseRDD[3] at groupBy at <stdin>:2 []\\n    |  PythonRDD[2] at groupBy at <stdin>:2 []\\n    |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 []'"
     ]
    }
   ],
   "source": [
    "## two separate stages are created, because of the shuffle\n",
    "print(groupedRDD.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Actions\n",
    "\n",
    "Actions are applied when it is necessary to materialize the result — save the data to disk, write the data to a database or output a part of the data to the console. The `collect` operation that we have used so far is also an action — it collects data.\n",
    "\n",
    "The actions are not lazy — they will actually trigger the data processing. Actions are RDD operations that produce values that are not RDD.\n",
    "\n",
    "`reduce` 연산을 이용하여 filtering한 데이터를 sum하여 가져와보는 예시를 보자 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135"
     ]
    }
   ],
   "source": [
    "filteredRDD.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * DAG\n",
    "\n",
    "Unlike Hadoop, where the user has to break down all the operations into smaller tasks and chain them together in order to use MapReduce, Spark defines tasks that can be computed in parallel with the partitioned data on the cluster. With these defined tasks, Spark builds a logical flow of operations that can be represented as a directional and acyclic graph, also known as DAG (Directed Acyclic Graph), where the node represents an RDD partition and the edge represents a data transformation. Spark builds the execution plan implicitly from the application provided by Spark.\n",
    "\n",
    "![10](https://user-images.githubusercontent.com/41605276/75401498-abe8d600-5945-11ea-8455-cbc1a8df36ad.png)\n",
    "\n",
    "DAGScheduler computes a DAG of stages for each job. A stage consists of tasks based on input data partitions. DAGScheduler merges some transformations together, for example, many `map` operators can be combined into one stage. The end result of DAGScheduler is an optimal set of stages in the form of TaskSet. Then the stages are passed to TaskScheduler. The number of stage tasks depends on the number of partitions. TaskScheduler launches tasks through cluster manager. TaskScheduler doesn't know about the dependencies of stages.\n",
    "\n",
    "RDDs can determine preferred locations for processing partitions. DAGScheduler places computation so that it will be as close to the data as possible (data locality)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
