{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "Data_Engineering_TIL(20201211)\n",
    "\n",
    "study_program : 티아카데미 컨테이너 오케스트레이션 쿠버네티스 살펴보기\n",
    "\n",
    "(URL : https://tacademy.skplanet.com/frontMain.action)\n",
    "\n",
    "실습 시 참고한 URL : https://github.com/subicura/workshop-k8s-basic\n",
    "\n",
    "\n",
    "- 아래와 같이 쿠버네티스 이해를 위한 주요 개념에 대해 실습해본다.\n",
    "\n",
    "1) kubectl\n",
    "\n",
    "2) pod\n",
    "\n",
    "3) replicaset\n",
    "\n",
    "4) deployment\n",
    "\n",
    "5) service\n",
    "\n",
    "6) ingress\n",
    "\n",
    "\n",
    "- 실습환경 셋팅\n",
    "\n",
    "ec2를 생성하고 ssh 접속해서 아래와 같이 명령어를 실행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-0-1-231 ~]$ sudo yum update -y\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ sudo yum install jq -y\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ sudo amazon-linux-extras install docker\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ sudo service docker start\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ sudo usermod -a -G docker ec2-user\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ exit\n",
    "\n",
    "# ssh 재접속\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ docker info\n",
    "Client:\n",
    " Debug Mode: false\n",
    "\n",
    "Server:\n",
    " Containers: 0\n",
    "  Running: 0\n",
    "  Paused: 0\n",
    "  Stopped: 0\n",
    " Images: 0\n",
    " Server Version: 19.03.13-ce\n",
    " Storage Driver: overlay2\n",
    "  Backing Filesystem: xfs\n",
    "  Supports d_type: true\n",
    "  Native Overlay Diff: true\n",
    " Logging Driver: json-file\n",
    " Cgroup Driver: cgroupfs\n",
    " Plugins:\n",
    "  Volume: local\n",
    "  Network: bridge host ipvlan macvlan null overlay\n",
    "  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\n",
    " Swarm: inactive\n",
    " Runtimes: runc\n",
    " Default Runtime: runc\n",
    " Init Binary: docker-init\n",
    " containerd version: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    " runc version: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    " init version: de40ad0 (expected: fec3683)\n",
    " Security Options:\n",
    "  seccomp\n",
    "   Profile: default\n",
    " Kernel Version: 4.14.193-149.317.amzn2.x86_64\n",
    " Operating System: Amazon Linux 2\n",
    " OSType: linux\n",
    " Architecture: x86_64\n",
    " CPUs: 2\n",
    " Total Memory: 1.909GiB\n",
    " Name: ip-10-0-1-231.ap-northeast-2.compute.internal\n",
    " ID: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    " Docker Root Dir: /var/lib/docker\n",
    " Debug Mode: false\n",
    " Registry: https://index.docker.io/v1/\n",
    " Labels:\n",
    " Experimental: false\n",
    " Insecure Registries:\n",
    "  127.0.0.0/8\n",
    " Live Restore Enabled: false\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ curl -sfL https://get.k3s.io | sh -\n",
    "[INFO]  Finding release for channel stable\n",
    "[INFO]  Using v1.19.4+k3s1 as release\n",
    "[INFO]  Downloading hash https://github.com/rancher/k3s/releases/download/v1.19.4+k3s1/sha256sum-amd64.txt\n",
    "[INFO]  Downloading binary https://github.com/rancher/k3s/releases/download/v1.19.4+k3s1/k3s\n",
    "[INFO]  Verifying binary download\n",
    "[INFO]  Installing k3s to /usr/local/bin/k3s\n",
    "[INFO]  Creating /usr/local/bin/kubectl symlink to k3s\n",
    "[INFO]  Creating /usr/local/bin/crictl symlink to k3s\n",
    "[INFO]  Skipping /usr/local/bin/ctr symlink to k3s, command exists in PATH at /usr/bin/ctr\n",
    "[INFO]  Creating killall script /usr/local/bin/k3s-killall.sh\n",
    "[INFO]  Creating uninstall script /usr/local/bin/k3s-uninstall.sh\n",
    "[INFO]  env: Creating environment file /etc/systemd/system/k3s.service.env\n",
    "[INFO]  systemd: Creating service file /etc/systemd/system/k3s.service\n",
    "[INFO]  systemd: Enabling k3s unit\n",
    "Created symlink from /etc/systemd/system/multi-user.target.wants/k3s.service to /etc/systemd/system/k3s.service.\n",
    "[INFO]  systemd: Starting k3s\n",
    "    \n",
    "[ec2-user@ip-10-0-1-231 ~]$ sudo chown ec2-user:ec2-user /etc/rancher/k3s/k3s.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기본명령어 실습\n",
    "\n",
    "get을 이용해서 각종정보를 조회해본다.\n",
    "\n",
    "get은 object들을 list로 조회한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쿠버네티스 클러스터 현황확인\n",
    "# ec2 한대밖에없어서 하나만 보인다.\n",
    "# 실제 프로덕션 환경에서는 여러대를 클러스터링해서 사용한다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get nodes\n",
    "NAME                                            STATUS   ROLES    AGE   VERSION\n",
    "ip-10-0-1-231.ap-northeast-2.compute.internal   Ready    master   36s   v1.19.4+k3s1\n",
    "\n",
    "# -o wide 옵션을 주어 상세내용을 볼 수도 있다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get nodes -o wide\n",
    "NAME                                            STATUS   ROLES    AGE   VERSION        INTERNAL-IP   EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\n",
    "ip-10-0-1-231.ap-northeast-2.compute.internal   Ready    master   23m   v1.19.4+k3s1   10.0.1.231    <none>        Amazon Linux 2   4.14.193-149.317.amzn2.x86_64   containerd://1.4.1-k3s1\n",
    "\n",
    "# -o yaml 옵션을 주면 완전 자세한 내용을 확인할 수 있다.\n",
    "# 쿠버네티스는 어떤형태의 상태를 관리하는 것이 목적인데 이런 상태를 체크하기에 적합한 옵션이다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get nodes -o yaml\n",
    "    \n",
    "# pod 현황확인\n",
    "# 아직 만든게 없어서 노 리소스로 나온다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get pod\n",
    "No resources found in default namespace.\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get replicaset\n",
    "No resources found in default namespace.\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get deployment\n",
    "No resources found in default namespace.\n",
    "\n",
    "# 쿠버네티스에서 사용하는 서비스가 하나 보인다.\n",
    "# 이거는 kubectl이 api 통신을 하기위해서 필요해서 떠있는 것이다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get service\n",
    "NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
    "kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   9m46s\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get ingress\n",
    "Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress\n",
    "No resources found in default namespace.\n",
    "\n",
    "# 전체 오브젝트는 kubectl api-resources 명령어로 조회가 가능하다.\n",
    "# 아래와 같은 리소스들이 복잡하게 얽혀서 쿠버네티스 클러스터가 동작하는 것이다.\n",
    "# 하지만 실제로 사용자가 사용하는건 위에서 확인한 몇가지 것들 위주다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl api-resources\n",
    "NAME                              SHORTNAMES   APIGROUP                       NAMESPACED   KIND\n",
    "bindings                                                                      true         Binding\n",
    "componentstatuses                 cs                                          false        ComponentStatus\n",
    "configmaps                        cm                                          true         ConfigMap\n",
    "endpoints                         ep                                          true         Endpoints\n",
    "events                            ev                                          true         Event\n",
    "limitranges                       limits                                      true         LimitRange\n",
    "namespaces                        ns                                          false        Namespace\n",
    "nodes                             no                                          false        Node\n",
    "persistentvolumeclaims            pvc                                         true         PersistentVolumeClaim\n",
    "persistentvolumes                 pv                                          false        PersistentVolume\n",
    "pods                              po                                          true         Pod\n",
    "podtemplates                                                                  true         PodTemplate\n",
    "replicationcontrollers            rc                                          true         ReplicationController\n",
    "resourcequotas                    quota                                       true         ResourceQuota\n",
    "secrets                                                                       true         Secret\n",
    "serviceaccounts                   sa                                          true         ServiceAccount\n",
    "services                          svc                                         true         Service\n",
    "mutatingwebhookconfigurations                  admissionregistration.k8s.io   false        MutatingWebhookConfiguration\n",
    "validatingwebhookconfigurations                admissionregistration.k8s.io   false        ValidatingWebhookConfiguration\n",
    "customresourcedefinitions         crd,crds     apiextensions.k8s.io           false        CustomResourceDefinition\n",
    "apiservices                                    apiregistration.k8s.io         false        APIService\n",
    "controllerrevisions                            apps                           true         ControllerRevision\n",
    "daemonsets                        ds           apps                           true         DaemonSet\n",
    "deployments                       deploy       apps                           true         Deployment\n",
    "replicasets                       rs           apps                           true         ReplicaSet\n",
    "statefulsets                      sts          apps                           true         StatefulSet\n",
    "tokenreviews                                   authentication.k8s.io          false        TokenReview\n",
    "localsubjectaccessreviews                      authorization.k8s.io           true         LocalSubjectAccessReview\n",
    "selfsubjectaccessreviews                       authorization.k8s.io           false        SelfSubjectAccessReview\n",
    "selfsubjectrulesreviews                        authorization.k8s.io           false        SelfSubjectRulesReview\n",
    "subjectaccessreviews                           authorization.k8s.io           false        SubjectAccessReview\n",
    "horizontalpodautoscalers          hpa          autoscaling                    true         HorizontalPodAutoscaler\n",
    "cronjobs                          cj           batch                          true         CronJob\n",
    "jobs                                           batch                          true         Job\n",
    "certificatesigningrequests        csr          certificates.k8s.io            false        CertificateSigningRequest\n",
    "leases                                         coordination.k8s.io            true         Lease\n",
    "endpointslices                                 discovery.k8s.io               true         EndpointSlice\n",
    "events                            ev           events.k8s.io                  true         Event\n",
    "ingresses                         ing          extensions                     true         Ingress\n",
    "helmchartconfigs                               helm.cattle.io                 true         HelmChartConfig\n",
    "helmcharts                                     helm.cattle.io                 true         HelmChart\n",
    "addons                                         k3s.cattle.io                  true         Addon\n",
    "nodes                                          metrics.k8s.io                 false        NodeMetrics\n",
    "pods                                           metrics.k8s.io                 true         PodMetrics\n",
    "ingressclasses                                 networking.k8s.io              false        IngressClass\n",
    "ingresses                         ing          networking.k8s.io              true         Ingress\n",
    "networkpolicies                   netpol       networking.k8s.io              true         NetworkPolicy\n",
    "runtimeclasses                                 node.k8s.io                    false        RuntimeClass\n",
    "poddisruptionbudgets              pdb          policy                         true         PodDisruptionBudget\n",
    "podsecuritypolicies               psp          policy                         false        PodSecurityPolicy\n",
    "clusterrolebindings                            rbac.authorization.k8s.io      false        ClusterRoleBinding\n",
    "clusterroles                                   rbac.authorization.k8s.io      false        ClusterRole\n",
    "rolebindings                                   rbac.authorization.k8s.io      true         RoleBinding\n",
    "roles                                          rbac.authorization.k8s.io      true         Role\n",
    "priorityclasses                   pc           scheduling.k8s.io              false        PriorityClass\n",
    "csidrivers                                     storage.k8s.io                 false        CSIDriver\n",
    "csinodes                                       storage.k8s.io                 false        CSINode\n",
    "storageclasses                    sc           storage.k8s.io                 false        StorageClass\n",
    "volumeattachments                              storage.k8s.io                 false        VolumeAttachment\n",
    "\n",
    "# pod, replicaset, deployment, service 한꺼번에 조회\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get all\n",
    "NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
    "service/kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   20m\n",
    "\n",
    "# json으로 나온 결과를 파이프로 받아서 jq라는 얘가 다시 그거를 처리하는 명령어이다.\n",
    "# 현재 이노드의 네임과 cpu 메모리 정보 등을 확인할 수 있다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get nodes -o json | jq \".items[] | {name:.metadata.name} + .status.capacity\"\n",
    "{\n",
    "  \"name\": \"ip-10-0-1-231.ap-northeast-2.compute.internal\",\n",
    "  \"cpu\": \"2\",\n",
    "  \"ephemeral-storage\": \"31444972Ki\",\n",
    "  \"hugepages-1Gi\": \"0\",\n",
    "  \"hugepages-2Mi\": \"0\",\n",
    "  \"memory\": \"2002012Ki\",\n",
    "  \"pods\": \"110\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- describe 명령어 실습\n",
    "\n",
    "상세정보를 조회할때 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get nodes\n",
    "NAME                                            STATUS   ROLES    AGE   VERSION\n",
    "ip-10-0-1-231.ap-northeast-2.compute.internal   Ready    master   39m   v1.19.4+k3s1\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl describe node ip-10-0-1-231.ap-northeast-2.compute.internal\n",
    "Name:               ip-10-0-1-231.ap-northeast-2.compute.internal\n",
    "Roles:              master\n",
    "Labels:             beta.kubernetes.io/arch=amd64\n",
    "                    beta.kubernetes.io/instance-type=k3s\n",
    "                    beta.kubernetes.io/os=linux\n",
    "                    k3s.io/hostname=ip-10-0-1-231.ap-northeast-2.compute.internal\n",
    "                    k3s.io/internal-ip=10.0.1.231\n",
    "                    kubernetes.io/arch=amd64\n",
    "                    kubernetes.io/hostname=ip-10-0-1-231.ap-northeast-2.compute.internal\n",
    "                    kubernetes.io/os=linux\n",
    "                    node-role.kubernetes.io/master=true\n",
    "                    node.kubernetes.io/instance-type=k3s\n",
    "Annotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"xxxxxxxxxxxxxxxxxxxx\"}\n",
    "                    flannel.alpha.coreos.com/backend-type: vxlan\n",
    "                    flannel.alpha.coreos.com/kube-subnet-manager: true\n",
    "                    flannel.alpha.coreos.com/public-ip: 10.0.1.231\n",
    "                    k3s.io/node-args: [\"server\"]\n",
    "                    k3s.io/node-config-hash: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx====\n",
    "                    k3s.io/node-env: {\"K3S_DATA_DIR\":\"/var/lib/rancher/k3s/data/xxxxxxxxxxxxxxxxxxxxxxxxxx\"}\n",
    "                    node.alpha.kubernetes.io/ttl: 0\n",
    "                    volumes.kubernetes.io/controller-managed-attach-detach: true\n",
    "CreationTimestamp:  Fri, 11 Dec 2020 08:09:46 +0000\n",
    "Taints:             <none>\n",
    "Unschedulable:      false\n",
    "Lease:\n",
    "  HolderIdentity:  ip-10-0-1-231.ap-northeast-2.compute.internal\n",
    "  AcquireTime:     <unset>\n",
    "  RenewTime:       Fri, 11 Dec 2020 08:49:26 +0000\n",
    "Conditions:\n",
    "  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n",
    "  ----                 ------  -----------------                 ------------------                ------                       -------\n",
    "  NetworkUnavailable   False   Fri, 11 Dec 2020 08:10:02 +0000   Fri, 11 Dec 2020 08:10:02 +0000   FlannelIsUp                  Flannel is running on this node\n",
    "  MemoryPressure       False   Fri, 11 Dec 2020 08:45:54 +0000   Fri, 11 Dec 2020 08:09:45 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n",
    "  DiskPressure         False   Fri, 11 Dec 2020 08:45:54 +0000   Fri, 11 Dec 2020 08:09:45 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n",
    "  PIDPressure          False   Fri, 11 Dec 2020 08:45:54 +0000   Fri, 11 Dec 2020 08:09:45 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n",
    "  Ready                True    Fri, 11 Dec 2020 08:45:54 +0000   Fri, 11 Dec 2020 08:09:56 +0000   KubeletReady                 kubelet is posting ready status\n",
    "Addresses:\n",
    "  InternalIP:  10.0.1.231\n",
    "  Hostname:    ip-10-0-1-231.ap-northeast-2.compute.internal\n",
    "Capacity:\n",
    "  cpu:                2\n",
    "  ephemeral-storage:  31444972Ki\n",
    "  hugepages-1Gi:      0\n",
    "  hugepages-2Mi:      0\n",
    "  memory:             2002012Ki\n",
    "  pods:               110\n",
    "Allocatable:\n",
    "  cpu:                2\n",
    "  ephemeral-storage:  30589668738\n",
    "  hugepages-1Gi:      0\n",
    "  hugepages-2Mi:      0\n",
    "  memory:             2002012Ki\n",
    "  pods:               110\n",
    "System Info:\n",
    "  Machine ID:                 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "  System UUID:                xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "  Boot ID:                    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "  Kernel Version:             4.14.193-149.317.amzn2.x86_64\n",
    "  OS Image:                   Amazon Linux 2\n",
    "  Operating System:           linux\n",
    "  Architecture:               amd64\n",
    "  Container Runtime Version:  containerd://1.4.1-k3s1\n",
    "  Kubelet Version:            v1.19.4+k3s1\n",
    "  Kube-Proxy Version:         v1.19.4+k3s1\n",
    "PodCIDR:                      10.42.0.0/24\n",
    "PodCIDRs:                     10.42.0.0/24\n",
    "ProviderID:                   k3s://ip-10-0-1-231.ap-northeast-2.compute.internal\n",
    "Non-terminated Pods:          (5 in total)\n",
    "  Namespace                   Name                                      CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n",
    "  ---------                   ----                                      ------------  ----------  ---------------  -------------  ---\n",
    "  kube-system                 local-path-provisioner-7ff9579c6-vkqxc    0 (0%)        0 (0%)      0 (0%)           0 (0%)         39m\n",
    "  kube-system                 metrics-server-7b4f8b595-zpqnm            0 (0%)        0 (0%)      0 (0%)           0 (0%)         39m\n",
    "  kube-system                 coredns-66c464876b-hllht                  100m (5%)     0 (0%)      70Mi (3%)        170Mi (8%)     39m\n",
    "  kube-system                 svclb-traefik-7czk5                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         39m\n",
    "  kube-system                 traefik-5dd496474-brj2s                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         39m\n",
    "Allocated resources:\n",
    "  (Total limits may be over 100 percent, i.e., overcommitted.)\n",
    "  Resource           Requests   Limits\n",
    "  --------           --------   ------\n",
    "  cpu                100m (5%)  0 (0%)\n",
    "  memory             70Mi (3%)  170Mi (8%)\n",
    "  ephemeral-storage  0 (0%)     0 (0%)\n",
    "  hugepages-1Gi      0 (0%)     0 (0%)\n",
    "  hugepages-2Mi      0 (0%)     0 (0%)\n",
    "Events:\n",
    "  Type     Reason                   Age                From        Message\n",
    "  ----     ------                   ----               ----        -------\n",
    "  Normal   Starting                 39m                kubelet     Starting kubelet.\n",
    "  Warning  InvalidDiskCapacity      39m                kubelet     invalid capacity 0 on image filesystem\n",
    "  Normal   NodeHasSufficientPID     39m (x2 over 39m)  kubelet     Node ip-10-0-1-231.ap-northeast-2.compute.internal status is now: NodeHasSufficientPID\n",
    "  Normal   NodeHasSufficientMemory  39m (x2 over 39m)  kubelet     Node ip-10-0-1-231.ap-northeast-2.compute.internal status is now: NodeHasSufficientMemory\n",
    "  Normal   NodeHasNoDiskPressure    39m (x2 over 39m)  kubelet     Node ip-10-0-1-231.ap-northeast-2.compute.internal status is now: NodeHasNoDiskPressure\n",
    "  Normal   NodeAllocatableEnforced  39m                kubelet     Updated Node Allocatable limit across pods\n",
    "  Normal   Starting                 39m                kube-proxy  Starting kube-proxy.\n",
    "  Normal   NodeReady                39m                kubelet     Node ip-10-0-1-231.ap-northeast-2.compute.internal status is now: NodeReady\n",
    "    \n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get service\n",
    "NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
    "kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   50m\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl describe service kubernetes\n",
    "Name:              kubernetes\n",
    "Namespace:         default\n",
    "Labels:            component=apiserver\n",
    "                   provider=kubernetes\n",
    "Annotations:       <none>\n",
    "Selector:          <none>\n",
    "Type:              ClusterIP\n",
    "IP:                10.43.0.1\n",
    "Port:              https  443/TCP\n",
    "TargetPort:        6443/TCP\n",
    "Endpoints:         10.0.1.231:6443\n",
    "Session Affinity:  None\n",
    "Events:            <none>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가장 자주 사용하는 명령어는 아래와 같다.\n",
    "\n",
    "kubectl exec -it <POD_NAME>\n",
    "\n",
    "--> exec 명령어를 이용해서 특정 컨테이너에 접속할때\n",
    "\n",
    "kubectl logs -f <POD_NAME|TYPE/NAME>\n",
    "\n",
    "--> 로그를 확인하거나\n",
    "\n",
    "kubectl apply -f <FILENAME>\n",
    "    \n",
    "--> 사용자가 정의한 상태를 apply\n",
    "\n",
    "kubectl delete -f <FILENAME>\n",
    "    \n",
    "--> 사용자가 정의한 상태를 delete\n",
    "\n",
    "- 쿠버네티스의 배포단위인 pod을 만들어보자\n",
    "\n",
    "프로덕션 환경에서는 yaml파일로 다 정의해놓고 pod을 만드는게 일반적이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whoami 라는 이미지 속성을 넣어서 만들어보자.\n",
    "# 네트워크가 연결되어 있지 않은 웹서비스 형태의 컨테이너를 실행시킨다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl run whoami --image subicura/whoami:1\n",
    "pod/whoami created\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get pods\n",
    "NAME     READY   STATUS    RESTARTS   AGE\n",
    "whoami   1/1     Running   0          39s\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get pods -o wide\n",
    "NAME     READY   STATUS    RESTARTS   AGE   IP          NODE                                            NOMINATED NODE   READINESS GATES\n",
    "whoami   1/1     Running   0          66s   10.42.0.8   ip-10-0-1-231.ap-northeast-2.compute.internal   <none>           <none>\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get pods -o yaml\n",
    "apiVersion: v1\n",
    "items:\n",
    "- apiVersion: v1\n",
    "  kind: Pod\n",
    "  metadata:\n",
    "    creationTimestamp: \"2020-12-11T09:11:26Z\"\n",
    "    labels:\n",
    "      run: whoami\n",
    "    managedFields:\n",
    "    - apiVersion: v1\n",
    "      fieldsType: FieldsV1\n",
    "      fieldsV1:\n",
    "        f:metadata:\n",
    "          f:labels:\n",
    "            .: {}\n",
    "            f:run: {}\n",
    "        f:spec:\n",
    "          f:containers:\n",
    "            k:{\"name\":\"whoami\"}:\n",
    "              .: {}\n",
    "              f:image: {}\n",
    "              f:imagePullPolicy: {}\n",
    "              f:name: {}\n",
    "              f:resources: {}\n",
    "              f:terminationMessagePath: {}\n",
    "              f:terminationMessagePolicy: {}\n",
    "          f:dnsPolicy: {}\n",
    "          f:enableServiceLinks: {}\n",
    "          f:restartPolicy: {}\n",
    "          f:schedulerName: {}\n",
    "          f:securityContext: {}\n",
    "          f:terminationGracePeriodSeconds: {}\n",
    "      manager: kubectl-run\n",
    "      operation: Update\n",
    "      time: \"2020-12-11T09:11:26Z\"\n",
    "    - apiVersion: v1\n",
    "      fieldsType: FieldsV1\n",
    "      fieldsV1:\n",
    "        f:status:\n",
    "          f:conditions:\n",
    "            k:{\"type\":\"ContainersReady\"}:\n",
    "              .: {}\n",
    "              f:lastProbeTime: {}\n",
    "              f:lastTransitionTime: {}\n",
    "              f:status: {}\n",
    "              f:type: {}\n",
    "            k:{\"type\":\"Initialized\"}:\n",
    "              .: {}\n",
    "              f:lastProbeTime: {}\n",
    "              f:lastTransitionTime: {}\n",
    "              f:status: {}\n",
    "              f:type: {}\n",
    "            k:{\"type\":\"Ready\"}:\n",
    "              .: {}\n",
    "              f:lastProbeTime: {}\n",
    "              f:lastTransitionTime: {}\n",
    "              f:status: {}\n",
    "              f:type: {}\n",
    "          f:containerStatuses: {}\n",
    "          f:hostIP: {}\n",
    "          f:phase: {}\n",
    "          f:podIP: {}\n",
    "          f:podIPs:\n",
    "            .: {}\n",
    "            k:{\"ip\":\"xxxxxxxxxxxxxxx\"}:\n",
    "              .: {}\n",
    "              f:ip: {}\n",
    "          f:startTime: {}\n",
    "      manager: k3s\n",
    "      operation: Update\n",
    "      time: \"2020-12-11T09:11:36Z\"\n",
    "    name: whoami\n",
    "    namespace: default\n",
    "    resourceVersion: \"3162\"\n",
    "    selfLink: /api/v1/namespaces/default/pods/whoami\n",
    "    uid: xxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "  spec:\n",
    "    containers:\n",
    "    - image: subicura/whoami:1\n",
    "      imagePullPolicy: IfNotPresent\n",
    "      name: whoami\n",
    "      resources: {}\n",
    "      terminationMessagePath: /dev/termination-log\n",
    "      terminationMessagePolicy: File\n",
    "      volumeMounts:\n",
    "      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n",
    "        name: default-token-bklrx\n",
    "        readOnly: true\n",
    "    dnsPolicy: ClusterFirst\n",
    "    enableServiceLinks: true\n",
    "    nodeName: ip-10-0-1-231.ap-northeast-2.compute.internal\n",
    "    preemptionPolicy: PreemptLowerPriority\n",
    "    priority: 0\n",
    "    restartPolicy: Always\n",
    "    schedulerName: default-scheduler\n",
    "    securityContext: {}\n",
    "    serviceAccount: default\n",
    "    serviceAccountName: default\n",
    "    terminationGracePeriodSeconds: 30\n",
    "    tolerations:\n",
    "    - effect: NoExecute\n",
    "      key: node.kubernetes.io/not-ready\n",
    "      operator: Exists\n",
    "      tolerationSeconds: 300\n",
    "    - effect: NoExecute\n",
    "      key: node.kubernetes.io/unreachable\n",
    "      operator: Exists\n",
    "      tolerationSeconds: 300\n",
    "    volumes:\n",
    "    - name: default-token-bklrx\n",
    "      secret:\n",
    "        defaultMode: 420\n",
    "        secretName: default-token-bklrx\n",
    "  status:\n",
    "    conditions:\n",
    "    - lastProbeTime: null\n",
    "      lastTransitionTime: \"2020-12-11T09:11:26Z\"\n",
    "      status: \"True\"\n",
    "      type: Initialized\n",
    "    - lastProbeTime: null\n",
    "      lastTransitionTime: \"2020-12-11T09:11:36Z\"\n",
    "      status: \"True\"\n",
    "      type: Ready\n",
    "    - lastProbeTime: null\n",
    "      lastTransitionTime: \"2020-12-11T09:11:36Z\"\n",
    "      status: \"True\"\n",
    "      type: ContainersReady\n",
    "    - lastProbeTime: null\n",
    "      lastTransitionTime: \"2020-12-11T09:11:26Z\"\n",
    "      status: \"True\"\n",
    "      type: PodScheduled\n",
    "    containerStatuses:\n",
    "    - containerID: containerd://1c9288f23c00f9dd1eb867afb0e6d4febf05328939370d9dbc2b512de44598ee\n",
    "      image: docker.io/subicura/whoami:1\n",
    "      imageID: docker.io/subicura/whoami@sha256:b560f146bf9cf80ee34bd495ce04dc9661292086781364d36125ef6fc9292fc9\n",
    "      lastState: {}\n",
    "      name: whoami\n",
    "      ready: true\n",
    "      restartCount: 0\n",
    "      started: true\n",
    "      state:\n",
    "        running:\n",
    "          startedAt: \"2020-12-11T09:11:36Z\"\n",
    "    hostIP: 10.0.1.231\n",
    "    phase: Running\n",
    "    podIP: 10.42.0.8\n",
    "    podIPs:\n",
    "    - ip: 10.42.0.8\n",
    "    qosClass: BestEffort\n",
    "    startTime: \"2020-12-11T09:11:26Z\"\n",
    "kind: List\n",
    "metadata:\n",
    "  resourceVersion: \"\"\n",
    "  selfLink: \"\"\n",
    "    \n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get pods -o json\n",
    "{\n",
    "    \"apiVersion\": \"v1\",\n",
    "    \"items\": [\n",
    "        {\n",
    "            \"apiVersion\": \"v1\",\n",
    "            \"kind\": \"Pod\",\n",
    "            \"metadata\": {\n",
    "                \"creationTimestamp\": \"2020-12-11T09:11:26Z\",\n",
    "                \"labels\": {\n",
    "                    \"run\": \"whoami\"\n",
    "                },\n",
    "                \"managedFields\": [\n",
    "                    {\n",
    "                        \"apiVersion\": \"v1\",\n",
    "                        \"fieldsType\": \"FieldsV1\",\n",
    "                        \"fieldsV1\": {\n",
    "                            \"f:metadata\": {\n",
    "                                \"f:labels\": {\n",
    "                                    \".\": {},\n",
    "                                    \"f:run\": {}\n",
    "                                }\n",
    "                            },\n",
    "                            \"f:spec\": {\n",
    "                                \"f:containers\": {\n",
    "                                    \"k:{\\\"name\\\":\\\"whoami\\\"}\": {\n",
    "                                        \".\": {},\n",
    "                                        \"f:image\": {},\n",
    "                                        \"f:imagePullPolicy\": {},\n",
    "                                        \"f:name\": {},\n",
    "                                        \"f:resources\": {},\n",
    "                                        \"f:terminationMessagePath\": {},\n",
    "                                        \"f:terminationMessagePolicy\": {}\n",
    "                                    }\n",
    "                                },\n",
    "                                \"f:dnsPolicy\": {},\n",
    "                                \"f:enableServiceLinks\": {},\n",
    "                                \"f:restartPolicy\": {},\n",
    "                                \"f:schedulerName\": {},\n",
    "                                \"f:securityContext\": {},\n",
    "                                \"f:terminationGracePeriodSeconds\": {}\n",
    "                            }\n",
    "                        },\n",
    "                        \"manager\": \"kubectl-run\",\n",
    "                        \"operation\": \"Update\",\n",
    "                        \"time\": \"2020-12-11T09:11:26Z\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"apiVersion\": \"v1\",\n",
    "                        \"fieldsType\": \"FieldsV1\",\n",
    "                        \"fieldsV1\": {\n",
    "                            \"f:status\": {\n",
    "                                \"f:conditions\": {\n",
    "                                    \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n",
    "                                        \".\": {},\n",
    "                                        \"f:lastProbeTime\": {},\n",
    "                                        \"f:lastTransitionTime\": {},\n",
    "                                        \"f:status\": {},\n",
    "                                        \"f:type\": {}\n",
    "                                    },\n",
    "                                    \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n",
    "                                        \".\": {},\n",
    "                                        \"f:lastProbeTime\": {},\n",
    "                                        \"f:lastTransitionTime\": {},\n",
    "                                        \"f:status\": {},\n",
    "                                        \"f:type\": {}\n",
    "                                    },\n",
    "                                    \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n",
    "                                        \".\": {},\n",
    "                                        \"f:lastProbeTime\": {},\n",
    "                                        \"f:lastTransitionTime\": {},\n",
    "                                        \"f:status\": {},\n",
    "                                        \"f:type\": {}\n",
    "                                    }\n",
    "                                },\n",
    "                                \"f:containerStatuses\": {},\n",
    "                                \"f:hostIP\": {},\n",
    "                                \"f:phase\": {},\n",
    "                                \"f:podIP\": {},\n",
    "                                \"f:podIPs\": {\n",
    "                                    \".\": {},\n",
    "                                    \"k:{\\\"ip\\\":\\\"10.42.0.8\\\"}\": {\n",
    "                                        \".\": {},\n",
    "                                        \"f:ip\": {}\n",
    "                                    }\n",
    "                                },\n",
    "                                \"f:startTime\": {}\n",
    "                            }\n",
    "                        },\n",
    "                        \"manager\": \"k3s\",\n",
    "                        \"operation\": \"Update\",\n",
    "                        \"time\": \"2020-12-11T09:11:36Z\"\n",
    "                    }\n",
    "                ],\n",
    "                \"name\": \"whoami\",\n",
    "                \"namespace\": \"default\",\n",
    "                \"resourceVersion\": \"3162\",\n",
    "                \"selfLink\": \"/api/v1/namespaces/default/pods/whoami\",\n",
    "                \"uid\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "            },\n",
    "            \"spec\": {\n",
    "                \"containers\": [\n",
    "                    {\n",
    "                        \"image\": \"subicura/whoami:1\",\n",
    "                        \"imagePullPolicy\": \"IfNotPresent\",\n",
    "                        \"name\": \"whoami\",\n",
    "                        \"resources\": {},\n",
    "                        \"terminationMessagePath\": \"/dev/termination-log\",\n",
    "                        \"terminationMessagePolicy\": \"File\",\n",
    "                        \"volumeMounts\": [\n",
    "                            {\n",
    "                                \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n",
    "                                \"name\": \"default-token-bklrx\",\n",
    "                                \"readOnly\": true\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                \"dnsPolicy\": \"ClusterFirst\",\n",
    "                \"enableServiceLinks\": true,\n",
    "                \"nodeName\": \"ip-10-0-1-231.ap-northeast-2.compute.internal\",\n",
    "                \"preemptionPolicy\": \"PreemptLowerPriority\",\n",
    "                \"priority\": 0,\n",
    "                \"restartPolicy\": \"Always\",\n",
    "                \"schedulerName\": \"default-scheduler\",\n",
    "                \"securityContext\": {},\n",
    "                \"serviceAccount\": \"default\",\n",
    "                \"serviceAccountName\": \"default\",\n",
    "                \"terminationGracePeriodSeconds\": 30,\n",
    "                \"tolerations\": [\n",
    "                    {\n",
    "                        \"effect\": \"NoExecute\",\n",
    "                        \"key\": \"node.kubernetes.io/not-ready\",\n",
    "                        \"operator\": \"Exists\",\n",
    "                        \"tolerationSeconds\": 300\n",
    "                    },\n",
    "                    {\n",
    "                        \"effect\": \"NoExecute\",\n",
    "                        \"key\": \"node.kubernetes.io/unreachable\",\n",
    "                        \"operator\": \"Exists\",\n",
    "                        \"tolerationSeconds\": 300\n",
    "                    }\n",
    "                ],\n",
    "                \"volumes\": [\n",
    "                    {\n",
    "                        \"name\": \"default-token-bklrx\",\n",
    "                        \"secret\": {\n",
    "                            \"defaultMode\": 420,\n",
    "                            \"secretName\": \"default-token-bklrx\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"status\": {\n",
    "                \"conditions\": [\n",
    "                    {\n",
    "                        \"lastProbeTime\": null,\n",
    "                        \"lastTransitionTime\": \"2020-12-11T09:11:26Z\",\n",
    "                        \"status\": \"True\",\n",
    "                        \"type\": \"Initialized\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"lastProbeTime\": null,\n",
    "                        \"lastTransitionTime\": \"2020-12-11T09:11:36Z\",\n",
    "                        \"status\": \"True\",\n",
    "                        \"type\": \"Ready\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"lastProbeTime\": null,\n",
    "                        \"lastTransitionTime\": \"2020-12-11T09:11:36Z\",\n",
    "                        \"status\": \"True\",\n",
    "                        \"type\": \"ContainersReady\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"lastProbeTime\": null,\n",
    "                        \"lastTransitionTime\": \"2020-12-11T09:11:26Z\",\n",
    "                        \"status\": \"True\",\n",
    "                        \"type\": \"PodScheduled\"\n",
    "                    }\n",
    "                ],\n",
    "                \"containerStatuses\": [\n",
    "                    {\n",
    "                        \"containerID\": \"containerd://1c9288f23c00f9dd1eb867afb0e6d4febf05328939370d9dbc2b512de44598ee\",\n",
    "                        \"image\": \"docker.io/subicura/whoami:1\",\n",
    "                        \"imageID\": \"docker.io/subicura/whoami@sha256:b560f146bf9cf80ee34bd495ce04dc9661292086781364d36125ef6fc9292fc9\",\n",
    "                        \"lastState\": {},\n",
    "                        \"name\": \"whoami\",\n",
    "                        \"ready\": true,\n",
    "                        \"restartCount\": 0,\n",
    "                        \"started\": true,\n",
    "                        \"state\": {\n",
    "                            \"running\": {\n",
    "                                \"startedAt\": \"2020-12-11T09:11:36Z\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"hostIP\": \"10.0.1.231\",\n",
    "                \"phase\": \"Running\",\n",
    "                \"podIP\": \"10.42.0.8\",\n",
    "                \"podIPs\": [\n",
    "                    {\n",
    "                        \"ip\": \"10.42.0.8\"\n",
    "                    }\n",
    "                ],\n",
    "                \"qosClass\": \"BestEffort\",\n",
    "                \"startTime\": \"2020-12-11T09:11:26Z\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"kind\": \"List\",\n",
    "    \"metadata\": {\n",
    "        \"resourceVersion\": \"\",\n",
    "        \"selfLink\": \"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# 아래와 같은 명령어로 생성한 pod에 대해 log를 확인해보자.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get pods\n",
    "NAME     READY   STATUS    RESTARTS   AGE\n",
    "whoami   1/1     Running   0          6m20s\n",
    "\n",
    "# pod에서 실행된 컨테이너의 로그를 확인할 수 있다.\n",
    "# 현재서버에서 돌던 다른서버에서 돌던 알아서 api서버가 모아서 보여주기 때문에 한곳에서 모든 로그를 확인할 수 있다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl logs -f whoami\n",
    "[2020-12-11 09:11:37] INFO  WEBrick 1.3.1\n",
    "[2020-12-11 09:11:37] INFO  ruby 2.3.3 (2016-11-21) [x86_64-linux]\n",
    "== Sinatra (v1.4.7) has taken the stage on 4567 for development with backup from WEBrick\n",
    "[2020-12-11 09:11:37] INFO  WEBrick::HTTPServer#start: pid=7 port=4567\n",
    "^C\n",
    "\n",
    "# 직접 접속해서 들어갈 수도 있다.\n",
    "# 이거 역시 이게 어떤 노드에 존재하는지 몰라도 이 명령어를 실행하면 api가 알아서 프록시 연결로 연결을 해준다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl exec -it whoami sh\n",
    "kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.\n",
    "/usr/src/app #\n",
    "/usr/src/app # ls -al\n",
    "total 16\n",
    "drwxr-xr-x    1 root     root            73 Feb 10  2017 .\n",
    "drwxr-xr-x    1 root     root            17 Feb 10  2017 ..\n",
    "-rw-r--r--    1 root     root           193 Feb 11  2017 Dockerfile\n",
    "-rw-r--r--    1 root     root            44 Feb 11  2017 Gemfile\n",
    "-rw-r--r--    1 root     root           280 Feb 11  2017 Gemfile.lock\n",
    "-rw-r--r--    1 root     root            73 Feb 11  2017 app.rb\n",
    "/usr/src/app # cat app.rb\n",
    "require 'sinatra'\n",
    "require 'socket'\n",
    "\n",
    "get '/' do\n",
    "  Socket.gethostname\n",
    "end\n",
    "\n",
    "/usr/src/app #\n",
    "/usr/src/app # exit\n",
    "\n",
    "# 또한 describe 명령으로 pod의 상세정보를 확인할 수 있다.\n",
    "# 여기서 중요한 내용은 Events이다.\n",
    "# 이벤트 부분을 보면 스케쥴러가 assign 해줬고, 내부적으로 도커이미지가 없으니까 pulling했고,\n",
    "# 컨테이너를 생성하고, 러닝까지 했다는 것을 알 수 있다.\n",
    "# 쿠버네티스를 쓰다보면 뭔가 실행을 했는데 pod이 안뜨는 경우가 있다. 이따 describe pod 명령을 이용하면 좋다.\n",
    "# 이벤트에 어떤 Error가 났는지 다 확인할 수 있다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl describe pod whoami\n",
    "Name:         whoami\n",
    "Namespace:    default\n",
    "Priority:     0\n",
    "Node:         ip-10-0-1-231.ap-northeast-2.compute.internal/10.0.1.231\n",
    "Start Time:   Fri, 11 Dec 2020 09:11:26 +0000\n",
    "Labels:       run=whoami\n",
    "Annotations:  <none>\n",
    "Status:       Running\n",
    "IP:           10.42.0.8\n",
    "IPs:\n",
    "  IP:  10.42.0.8\n",
    "Containers:\n",
    "  whoami:\n",
    "    Container ID:   containerd://1c9288f23c00f9dd1eb867afb0e6d4febf05328939370d9dbc2b512de44598ee\n",
    "    Image:          subicura/whoami:1\n",
    "    Image ID:       docker.io/subicura/whoami@sha256:b560f146bf9cf80ee34bd495ce04dc9661292086781364d36125ef6fc9292fc9\n",
    "    Port:           <none>\n",
    "    Host Port:      <none>\n",
    "    State:          Running\n",
    "      Started:      Fri, 11 Dec 2020 09:11:36 +0000\n",
    "    Ready:          True\n",
    "    Restart Count:  0\n",
    "    Environment:    <none>\n",
    "    Mounts:\n",
    "      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bklrx (ro)\n",
    "Conditions:\n",
    "  Type              Status\n",
    "  Initialized       True\n",
    "  Ready             True\n",
    "  ContainersReady   True\n",
    "  PodScheduled      True\n",
    "Volumes:\n",
    "  default-token-bklrx:\n",
    "    Type:        Secret (a volume populated by a Secret)\n",
    "    SecretName:  default-token-bklrx\n",
    "    Optional:    false\n",
    "QoS Class:       BestEffort\n",
    "Node-Selectors:  <none>\n",
    "Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
    "                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
    "Events:\n",
    "  Type    Reason     Age   From               Message\n",
    "  ----    ------     ----  ----               -------\n",
    "  Normal  Scheduled  29m   default-scheduler  Successfully assigned default/whoami to ip-10-0-1-231.ap-northeast-2.compute.internal\n",
    "  Normal  Pulling    29m   kubelet            Pulling image \"subicura/whoami:1\"\n",
    "  Normal  Pulled     28m   kubelet            Successfully pulled image \"subicura/whoami:1\" in 9.413830827s\n",
    "  Normal  Created    28m   kubelet            Created container whoami\n",
    "  Normal  Started    28m   kubelet            Started container whoami\n",
    "\n",
    "\n",
    "# 생성한 pod을 삭제해보자.\n",
    "# 참고로 deployment라는 것은 pod을 원하는 갯수를 생성을해준다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl delete pod whoami\n",
    "pod \"whoami\" deleted\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get pods\n",
    "No resources found in default namespace.\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get deployment\n",
    "No resources found in default namespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- YAML 파일 예제\n",
    "\n",
    "그러면 위에 명령어 위주로 pod을 실행했는데\n",
    "\n",
    "yaml 형태로 정의해서 실습을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-0-1-231 ~]$ mkdir guide-03\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ mkdir guide-03/task-02\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ sudo vim guide-03/task-02/whoami-pod.yml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: whoami\n",
    "  labels:\n",
    "    type: app\n",
    "spec:\n",
    "  containers:\n",
    "  - name: app\n",
    "    image: subicura/whoami:1\n",
    "            \n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl apply -f guide-03/task-02/whoami-pod.yml\n",
    "pod/whoami created\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get pods\n",
    "NAME     READY   STATUS    RESTARTS   AGE\n",
    "whoami   1/1     Running   0          46s\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get all\n",
    "NAME         READY   STATUS    RESTARTS   AGE\n",
    "pod/whoami   1/1     Running   0          2m54s\n",
    "\n",
    "NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
    "service/kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   110m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pod이 생성되는 과정을 보면 컨테이너를 생성하라고 지시하고, 해당 이미지를 다운로드를 받을 것이다. 그 다음에 컨테이너가 러닝이 되는데 러닝이 된다는 것은 process가 실행이 된다는 것이고, 근데 해당 process는 실행이 되자마자 바로 application을 요청을 받을수는 없을 것이다. 예를 들어서 예를 들어서 웹서비스를 띄운다고 하면 웹이 구동되는 시간이 있기 때문이다. 따라서 컨테이너가 running하는 시간과 최종적으로 application이 running되는 시간 사이에는 약간의 겝이 있다. 아직 app이 running되지 않았는데 네트워크를 연결하게 되면 error가 날 것이다. 따라서 그 웹이 구동되는 겝동안에 상태를 체크할 수 있는 값을 줄 수 있다. app이 실행되고나서도 주기적으로 체크를 하면서 컨테이너가 정상인지 아닌지를 판단을 하고, 정상이 아니면 자체적으로 재시작을 해서 정상여부를 확인을 한다. \n",
    "\n",
    "이를 확인하는게 하나는 app이 실행되고나서 컨테이너가 정상인지 체크하는 livenessProbe라는게 있고, app이 준비가 잘 되었는지 체크하는 readinessProbe 라는게 있다.\n",
    "\n",
    "아래에 작성하는 두 yaml에는 네트워크가 안되도록 작성이 된 내용이다. 따라서 해당 yaml을 실행하면 네트워크가 연결이 안되어서 error가 나는 것을 확인해보고자 한다.\n",
    "\n",
    "- livenessProbe 예제 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-0-1-231 ~]$ sudo vim guide-03/task-02/whoami-pod-lp.yml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: whoami-lp\n",
    "  labels:\n",
    "    type: app\n",
    "spec:\n",
    "  containers:\n",
    "  - name: app\n",
    "    image: subicura/whoami:1\n",
    "    livenessProbe:\n",
    "      httpGet:\n",
    "        path: /not/exist\n",
    "        port: 8080\n",
    "      initialDelaySeconds: 5\n",
    "      timeoutSeconds: 2 # Default 1\n",
    "      periodSeconds: 5 # Defaults 10\n",
    "      failureThreshold: 1 # Defaults 3\n",
    "        \n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl apply -f guide-03/task-02/whoami-pod-lp.yml\n",
    "pod/whoami-lp created\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get all\n",
    "NAME            READY   STATUS    RESTARTS   AGE\n",
    "pod/whoami      1/1     Running   0          17m\n",
    "pod/whoami-lp   1/1     Running   0          27s\n",
    "\n",
    "NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
    "service/kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   125m\n",
    "\n",
    "# Liveness probe 상태가 지속적으로 fail난다는 것을 확인해서 계속 재시도하다가 killing을 시킨것을 알 수 있다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl describe pod/whoami-lp\n",
    "Name:         whoami-lp\n",
    "Namespace:    default\n",
    "Priority:     0\n",
    "Node:         ip-10-0-1-231.ap-northeast-2.compute.internal/10.0.1.231\n",
    "Start Time:   Fri, 11 Dec 2020 10:14:50 +0000\n",
    "Labels:       type=app\n",
    "Annotations:  <none>\n",
    "Status:       Running\n",
    "IP:           10.42.0.10\n",
    "IPs:\n",
    "  IP:  10.42.0.10\n",
    "Containers:\n",
    "  app:\n",
    "    Container ID:   containerd://2fbcea98f85dbd9c513ab0ea994ea07cf12427ca85157ba1a4451797cfbc3ba7\n",
    "    Image:          subicura/whoami:1\n",
    "    Image ID:       docker.io/subicura/whoami@sha256:b560f146bf9cf80ee34bd495ce04dc9661292086781364d36125ef6fc9292fc9\n",
    "    Port:           <none>\n",
    "    Host Port:      <none>\n",
    "    State:          Running\n",
    "      Started:      Fri, 11 Dec 2020 10:16:01 +0000\n",
    "    Last State:     Terminated\n",
    "      Reason:       Error\n",
    "      Exit Code:    137\n",
    "      Started:      Fri, 11 Dec 2020 10:15:26 +0000\n",
    "      Finished:     Fri, 11 Dec 2020 10:16:01 +0000\n",
    "    Ready:          True\n",
    "    Restart Count:  2\n",
    "    Liveness:       http-get http://:8080/not/exist delay=5s timeout=2s period=5s #success=1 #failure=1\n",
    "    Environment:    <none>\n",
    "    Mounts:\n",
    "      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bklrx (ro)\n",
    "Conditions:\n",
    "  Type              Status\n",
    "  Initialized       True\n",
    "  Ready             True\n",
    "  ContainersReady   True\n",
    "  PodScheduled      True\n",
    "Volumes:\n",
    "  default-token-bklrx:\n",
    "    Type:        Secret (a volume populated by a Secret)\n",
    "    SecretName:  default-token-bklrx\n",
    "    Optional:    false\n",
    "QoS Class:       BestEffort\n",
    "Node-Selectors:  <none>\n",
    "Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
    "                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
    "Events:\n",
    "  Type     Reason     Age               From               Message\n",
    "  ----     ------     ----              ----               -------\n",
    "  Normal   Scheduled  77s               default-scheduler  Successfully assigned default/whoami-lp to ip-10-0-1-231.ap-northeast-2.compute.internal\n",
    "  Normal   Pulled     6s (x3 over 77s)  kubelet            Container image \"subicura/whoami:1\" already present on machine\n",
    "  Normal   Created    6s (x3 over 77s)  kubelet            Created container app\n",
    "  Normal   Started    6s (x3 over 76s)  kubelet            Started container app\n",
    "  Warning  Unhealthy  1s (x3 over 71s)  kubelet            Liveness probe failed: Get \"http://10.42.0.10:8080/not/exist\": dial tcp 10.42.0.10:8080: connect: connection refused\n",
    "  Normal   Killing    1s (x3 over 71s)  kubelet            Container app failed liveness probe, will be restarted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- readinessProbe 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-0-1-231 ~]$ sudo vim guide-03/task-02/whoami-pod-rp.yml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: whoami-rp\n",
    "  labels:\n",
    "    type: app\n",
    "spec:\n",
    "  containers:\n",
    "  - name: app\n",
    "    image: subicura/whoami:1\n",
    "    readinessProbe:\n",
    "      httpGet:\n",
    "        path: /not/exist\n",
    "        port: 8080\n",
    "      initialDelaySeconds: 5\n",
    "      timeoutSeconds: 2 # Default 1\n",
    "      periodSeconds: 5 # Defaults 10\n",
    "      failureThreshold: 1 # Defaults 3\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl apply -f guide-03/task-02/whoami-pod-rp.yml\n",
    "pod/whoami-rp created\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get all\n",
    "NAME            READY   STATUS             RESTARTS   AGE\n",
    "pod/whoami      1/1     Running            0          45m\n",
    "pod/whoami-lp   0/1     CrashLoopBackOff   13         28m\n",
    "pod/whoami-rp   0/1     Running            0          8s\n",
    "\n",
    "NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
    "service/kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   153m\n",
    "\n",
    "# pod/whoami-rp   0/1 를 봤을때 아예 뜨지 못하고 있는 모습을 확인할 수 있다.\n",
    "# 상태를 아래와 같이 체크를 해보니 접속을 계속 시도하고 있는데 네트워크 접속이 안되고 있는 것을 알 수 있다.\n",
    "# restart를 지정된 횟수가 넘어가 버리면 killing 된다.\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl describe pod/whoami-rp\n",
    "Name:         whoami-rp\n",
    "Namespace:    default\n",
    "Priority:     0\n",
    "Node:         ip-10-0-1-231.ap-northeast-2.compute.internal/10.0.1.231\n",
    "Start Time:   Fri, 11 Dec 2020 10:42:56 +0000\n",
    "Labels:       type=app\n",
    "Annotations:  <none>\n",
    "Status:       Running\n",
    "IP:           10.42.0.11\n",
    "IPs:\n",
    "  IP:  10.42.0.11\n",
    "Containers:\n",
    "  app:\n",
    "    Container ID:   containerd://bb4127257fb82eb13625166274cc4ba0eb31c21e917efe32f4b62a070ca0eb61\n",
    "    Image:          subicura/whoami:1\n",
    "    Image ID:       docker.io/subicura/whoami@sha256:b560f146bf9cf80ee34bd495ce04dc9661292086781364d36125ef6fc9292fc9\n",
    "    Port:           <none>\n",
    "    Host Port:      <none>\n",
    "    State:          Running\n",
    "      Started:      Fri, 11 Dec 2020 10:42:57 +0000\n",
    "    Ready:          False\n",
    "    Restart Count:  0\n",
    "    Readiness:      http-get http://:8080/not/exist delay=5s timeout=2s period=5s #success=1 #failure=1\n",
    "    Environment:    <none>\n",
    "    Mounts:\n",
    "      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bklrx (ro)\n",
    "Conditions:\n",
    "  Type              Status\n",
    "  Initialized       True\n",
    "  Ready             False\n",
    "  ContainersReady   False\n",
    "  PodScheduled      True\n",
    "Volumes:\n",
    "  default-token-bklrx:\n",
    "    Type:        Secret (a volume populated by a Secret)\n",
    "    SecretName:  default-token-bklrx\n",
    "    Optional:    false\n",
    "QoS Class:       BestEffort\n",
    "Node-Selectors:  <none>\n",
    "Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
    "                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
    "Events:\n",
    "  Type     Reason     Age                   From               Message\n",
    "  ----     ------     ----                  ----               -------\n",
    "  Normal   Scheduled  2m55s                 default-scheduler  Successfully assigned default/whoami-rp to ip-10-0-1-231.ap-northeast-2.compute.internal\n",
    "  Normal   Pulled     2m55s                 kubelet            Container image \"subicura/whoami:1\" already present on machine\n",
    "  Normal   Created    2m55s                 kubelet            Created container app\n",
    "  Normal   Started    2m55s                 kubelet            Started container app\n",
    "  Warning  Unhealthy  61s (x22 over 2m46s)  kubelet            Readiness probe failed: Get \"http://10.42.0.11:8080/not/exist\": dial tcp 10.42.0.11:8080: connect: connection refused"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- health check 예제\n",
    "\n",
    "livenessProbe와 readinessProbe를 전반적으로 체크하는 예제이다.\n",
    "\n",
    "위에 있던 예제들은 일부러 네트워크가 안되도록 yaml을 작성해준 것이고, 아래의 yaml에서 4567 포트는 통신이 가능한 포트로 아래와 같이 yaml을 작성해서 실행하면 정상적으로 실행이 되는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-0-1-231 ~]$ sudo vim guide-03/task-02/whoami-pod-health.yml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: whoami-health\n",
    "  labels:\n",
    "    type: app\n",
    "spec:\n",
    "  containers:\n",
    "  - name: app\n",
    "    image: subicura/whoami:1\n",
    "    livenessProbe:\n",
    "      httpGet:\n",
    "        path: /\n",
    "        port: 4567\n",
    "    readinessProbe:\n",
    "      httpGet:\n",
    "        path: /\n",
    "        port: 4567\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl apply -f guide-03/task-02/whoami-pod-health.yml\n",
    "pod/whoami-health created\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get all\n",
    "NAME                READY   STATUS             RESTARTS   AGE\n",
    "pod/whoami          1/1     Running            0          51m\n",
    "pod/whoami-rp       0/1     Running            0          6m52s\n",
    "pod/whoami-lp       0/1     CrashLoopBackOff   15         34m\n",
    "pod/whoami-health   0/1     Running            0          5s\n",
    "\n",
    "NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
    "service/kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   160m\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl describe pod/whoami-health\n",
    "Name:         whoami-health\n",
    "Namespace:    default\n",
    "Priority:     0\n",
    "Node:         ip-10-0-1-231.ap-northeast-2.compute.internal/10.0.1.231\n",
    "Start Time:   Fri, 11 Dec 2020 10:49:43 +0000\n",
    "Labels:       type=app\n",
    "Annotations:  <none>\n",
    "Status:       Running\n",
    "IP:           10.42.0.12\n",
    "IPs:\n",
    "  IP:  10.42.0.12\n",
    "Containers:\n",
    "  app:\n",
    "    Container ID:   containerd://3f6612c361698a62850363c6a20a51f075ff916d6b1f67639ad72a8ae170e3f8\n",
    "    Image:          subicura/whoami:1\n",
    "    Image ID:       docker.io/subicura/whoami@sha256:b560f146bf9cf80ee34bd495ce04dc9661292086781364d36125ef6fc9292fc9\n",
    "    Port:           <none>\n",
    "    Host Port:      <none>\n",
    "    State:          Running\n",
    "      Started:      Fri, 11 Dec 2020 10:49:44 +0000\n",
    "    Ready:          True\n",
    "    Restart Count:  0\n",
    "    Liveness:       http-get http://:4567/ delay=0s timeout=1s period=10s #success=1 #failure=3\n",
    "    Readiness:      http-get http://:4567/ delay=0s timeout=1s period=10s #success=1 #failure=3\n",
    "    Environment:    <none>\n",
    "    Mounts:\n",
    "      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bklrx (ro)\n",
    "Conditions:\n",
    "  Type              Status\n",
    "  Initialized       True\n",
    "  Ready             True\n",
    "  ContainersReady   True\n",
    "  PodScheduled      True\n",
    "Volumes:\n",
    "  default-token-bklrx:\n",
    "    Type:        Secret (a volume populated by a Secret)\n",
    "    SecretName:  default-token-bklrx\n",
    "    Optional:    false\n",
    "QoS Class:       BestEffort\n",
    "Node-Selectors:  <none>\n",
    "Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
    "                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
    "Events:\n",
    "  Type    Reason     Age   From               Message\n",
    "  ----    ------     ----  ----               -------\n",
    "  Normal  Scheduled  21s   default-scheduler  Successfully assigned default/whoami-health to ip-10-0-1-231.ap-northeast-2.compute.internal\n",
    "  Normal  Pulled     20s   kubelet            Container image \"subicura/whoami:1\" already present on machine\n",
    "  Normal  Created    20s   kubelet            Created container app\n",
    "  Normal  Started    20s   kubelet            Started container app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지는 pod 안에 컨테이너가 하나 있는거만 했는데 pod 하나에 두개 이상을 넣을 수도 있다.\n",
    "\n",
    "- multi-container 예제\n",
    "\n",
    "pod 안에 정의된 컨테이너는 동일한 ip를 사용한다. docker-compose 같은 경우에는 같은 네트워크 대역을 쓸뿐이지 같은 아이피주소를 쓰지는 않았지만 이 pod 안에서 뜨는 여러개의 컨테이너는 동일한 ip를 쓰고 서로다른 포트를 이용을 해서 통신을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-0-1-231 ~]$ sudo vim guide-03/task-02/whoami-pod-redis.yml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: whoami-redis\n",
    "  labels:\n",
    "    type: stack\n",
    "spec:\n",
    "  containers:\n",
    "  - name: app\n",
    "    image: subicura/whoami-redis:1\n",
    "    env:\n",
    "    - name: REDIS_HOST\n",
    "      value: \"localhost\"\n",
    "  - name: db\n",
    "    image: redis\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl apply -f guide-03/task-02/whoami-pod-redis.yml\n",
    "pod/whoami-redis created\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get all\n",
    "NAME                READY   STATUS              RESTARTS   AGE\n",
    "pod/whoami          1/1     Running             0          66m\n",
    "pod/whoami-rp       0/1     Running             0          21m\n",
    "pod/whoami-health   1/1     Running             0          14m\n",
    "pod/whoami-lp       0/1     CrashLoopBackOff    19         49m\n",
    "pod/whoami-redis    0/2     ContainerCreating   0          4s\n",
    "\n",
    "NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
    "service/kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   174m\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl describe pod/whoami-redis\n",
    "Name:         whoami-redis\n",
    "Namespace:    default\n",
    "Priority:     0\n",
    "Node:         ip-10-0-1-231.ap-northeast-2.compute.internal/10.0.1.231\n",
    "Start Time:   Fri, 11 Dec 2020 11:04:20 +0000\n",
    "Labels:       type=stack\n",
    "Annotations:  <none>\n",
    "Status:       Pending\n",
    "IP:\n",
    "IPs:          <none>\n",
    "Containers:\n",
    "  app:\n",
    "    Container ID:\n",
    "    Image:          subicura/whoami-redis:1\n",
    "    Image ID:\n",
    "    Port:           <none>\n",
    "    Host Port:      <none>\n",
    "    State:          Waiting\n",
    "      Reason:       ContainerCreating\n",
    "    Ready:          False\n",
    "    Restart Count:  0\n",
    "    Environment:\n",
    "      REDIS_HOST:  localhost\n",
    "    Mounts:\n",
    "      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bklrx (ro)\n",
    "  db:\n",
    "    Container ID:\n",
    "    Image:          redis\n",
    "    Image ID:\n",
    "    Port:           <none>\n",
    "    Host Port:      <none>\n",
    "    State:          Waiting\n",
    "      Reason:       ContainerCreating\n",
    "    Ready:          False\n",
    "    Restart Count:  0\n",
    "    Environment:    <none>\n",
    "    Mounts:\n",
    "      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bklrx (ro)\n",
    "Conditions:\n",
    "  Type              Status\n",
    "  Initialized       True\n",
    "  Ready             False\n",
    "  ContainersReady   False\n",
    "  PodScheduled      True\n",
    "Volumes:\n",
    "  default-token-bklrx:\n",
    "    Type:        Secret (a volume populated by a Secret)\n",
    "    SecretName:  default-token-bklrx\n",
    "    Optional:    false\n",
    "QoS Class:       BestEffort\n",
    "Node-Selectors:  <none>\n",
    "Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
    "                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
    "Events:\n",
    "  Type    Reason     Age   From               Message\n",
    "  ----    ------     ----  ----               -------\n",
    "  Normal  Scheduled  12s   default-scheduler  Successfully assigned default/whoami-redis to ip-10-0-1-231.ap-northeast-2.compute.internal\n",
    "  Normal  Pulling    12s   kubelet            Pulling image \"subicura/whoami-redis:1\"\n",
    "    \n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get all\n",
    "NAME                READY   STATUS             RESTARTS   AGE\n",
    "pod/whoami          1/1     Running            0          67m\n",
    "pod/whoami-rp       0/1     Running            0          22m\n",
    "pod/whoami-health   1/1     Running            0          15m\n",
    "pod/whoami-lp       0/1     CrashLoopBackOff   19         50m\n",
    "pod/whoami-redis    2/2     Running            0          63s\n",
    "\n",
    "NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
    "service/kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   175m\n",
    "\n",
    "# 이 pod 안에는 컨테이너가 두개 띄어져 있으니까 어떤 service에 대한 log를 보고 싶은지 몰라서 Error가 난다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl logs pod/whoami-redis\n",
    "error: a container name must be specified for pod whoami-redis, choose one of: [app db]\n",
    "        \n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl logs pod/whoami-redis app\n",
    "[2020-12-11 11:04:36] INFO  WEBrick 1.3.1\n",
    "[2020-12-11 11:04:36] INFO  ruby 2.3.8 (2018-10-18) [x86_64-linux-musl]\n",
    "== Sinatra (v2.0.4) has taken the stage on 4567 for development with backup from WEBrick\n",
    "[2020-12-11 11:04:36] INFO  WEBrick::HTTPServer#start: pid=1 port=4567\n",
    "        \n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl logs pod/whoami-redis db\n",
    "1:C 11 Dec 2020 11:04:44.031 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n",
    "1:C 11 Dec 2020 11:04:44.031 # Redis version=6.0.9, bits=64, commit=00000000, modified=0, pid=1, just started\n",
    "1:C 11 Dec 2020 11:04:44.031 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n",
    "1:M 11 Dec 2020 11:04:44.033 * Running mode=standalone, port=6379.\n",
    "1:M 11 Dec 2020 11:04:44.033 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n",
    "1:M 11 Dec 2020 11:04:44.033 # Server initialized\n",
    "1:M 11 Dec 2020 11:04:44.033 * Ready to accept connections\n",
    "\n",
    "# 아래 명령어와 같이 컨테이너에 직접 들어갈수도 있다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl exec -it whoami-redis -c db sh\n",
    "kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.\n",
    "# ls -al\n",
    "total 0\n",
    "drwxr-xr-x 2 redis redis  6 Nov 18 08:28 .\n",
    "drwxr-xr-x 1 root  root  28 Dec 11 11:04 ..\n",
    "# exit\n",
    "\n",
    "# app이라는 컨테이너에 접속했는데 여기서 redis에 접속이 가능한지 테스트를 해보자.\n",
    "# redis는 별도의 컨테이너이기 때문에 접속이 가능한지, 같은 아이피를 사용중인지 체크해본다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl exec -it whoami-redis -c app sh\n",
    "kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.\n",
    "/usr/src/app # apk add curl busybox-extras\n",
    "fetch http://dl-cdn.alpinelinux.org/alpine/v3.8/main/x86_64/APKINDEX.tar.gz\n",
    "fetch http://dl-cdn.alpinelinux.org/alpine/v3.8/community/x86_64/APKINDEX.tar.gz\n",
    "(1/5) Installing busybox-extras (1.28.4-r3)\n",
    "Executing busybox-extras-1.28.4-r3.post-install\n",
    "(2/5) Installing nghttp2-libs (1.39.2-r0)\n",
    "(3/5) Installing libssh2 (1.9.0-r1)\n",
    "(4/5) Installing libcurl (7.61.1-r3)\n",
    "(5/5) Installing curl (7.61.1-r3)\n",
    "Executing busybox-1.28.4-r1.trigger\n",
    "OK: 16 MiB in 35 packages\n",
    "/usr/src/app # curl localhost:4567\n",
    "1\n",
    "/usr/src/app # curl localhost:4567\n",
    "2\n",
    "/usr/src/app # curl localhost:4567\n",
    "3\n",
    "/usr/src/app # curl localhost:4567\n",
    "4\n",
    "/usr/src/app # telnet localhost 6379\n",
    "dbsize\n",
    ":1\n",
    "keys *\n",
    "*1\n",
    "$5\n",
    "count\n",
    "GET count\n",
    "$1\n",
    "4\n",
    "quit\n",
    "+OK\n",
    "Connection closed by foreign host\n",
    "/usr/src/app # exit\n",
    "command terminated with exit code 1\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl describe pod/whoami-redis\n",
    "Name:         whoami-redis\n",
    "Namespace:    default\n",
    "Priority:     0\n",
    "Node:         ip-10-0-1-231.ap-northeast-2.compute.internal/10.0.1.231\n",
    "Start Time:   Fri, 11 Dec 2020 11:04:20 +0000\n",
    "Labels:       type=stack\n",
    "Annotations:  <none>\n",
    "Status:       Running\n",
    "IP:           10.42.0.13\n",
    "IPs:\n",
    "  IP:  10.42.0.13\n",
    "Containers:\n",
    "  app:\n",
    "    Container ID:   containerd://64b0072264f511098ed05a0232b10fa1b3144bb06e1109caafc43c71c3fdfa6a\n",
    "    Image:          subicura/whoami-redis:1\n",
    "    Image ID:       docker.io/subicura/whoami-redis@sha256:4ebb7fcc2adc99f72aaf33f5a7a49c4f516aa413e87177542554573cb93c9454\n",
    "    Port:           <none>\n",
    "    Host Port:      <none>\n",
    "    State:          Running\n",
    "      Started:      Fri, 11 Dec 2020 11:04:35 +0000\n",
    "    Ready:          True\n",
    "    Restart Count:  0\n",
    "    Environment:\n",
    "      REDIS_HOST:  localhost\n",
    "    Mounts:\n",
    "      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bklrx (ro)\n",
    "  db:\n",
    "    Container ID:   containerd://139ed68e3d9ece9956402d818b7e243d52d6d1fdc393316b2d3966822994e17d\n",
    "    Image:          redis\n",
    "    Image ID:       docker.io/library/redis@sha256:e2ae53ef2864fca0a117f9b80f287e2890dd1036271dd363c20b9dfc54a8664c\n",
    "    Port:           <none>\n",
    "    Host Port:      <none>\n",
    "    State:          Running\n",
    "      Started:      Fri, 11 Dec 2020 11:04:44 +0000\n",
    "    Ready:          True\n",
    "    Restart Count:  0\n",
    "    Environment:    <none>\n",
    "    Mounts:\n",
    "      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bklrx (ro)\n",
    "Conditions:\n",
    "  Type              Status\n",
    "  Initialized       True\n",
    "  Ready             True\n",
    "  ContainersReady   True\n",
    "  PodScheduled      True\n",
    "Volumes:\n",
    "  default-token-bklrx:\n",
    "    Type:        Secret (a volume populated by a Secret)\n",
    "    SecretName:  default-token-bklrx\n",
    "    Optional:    false\n",
    "QoS Class:       BestEffort\n",
    "Node-Selectors:  <none>\n",
    "Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
    "                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
    "Events:\n",
    "  Type    Reason     Age   From               Message\n",
    "  ----    ------     ----  ----               -------\n",
    "  Normal  Scheduled  15m   default-scheduler  Successfully assigned default/whoami-redis to ip-10-0-1-231.ap-northeast-2.compute.internal\n",
    "  Normal  Pulling    15m   kubelet            Pulling image \"subicura/whoami-redis:1\"\n",
    "  Normal  Pulled     15m   kubelet            Successfully pulled image \"subicura/whoami-redis:1\" in 13.5701249s\n",
    "  Normal  Created    15m   kubelet            Created container app\n",
    "  Normal  Started    15m   kubelet            Started container app\n",
    "  Normal  Pulling    15m   kubelet            Pulling image \"redis\"\n",
    "  Normal  Pulled     15m   kubelet            Successfully pulled image \"redis\" in 8.099270678s\n",
    "  Normal  Created    15m   kubelet            Created container db\n",
    "  Normal  Started    15m   kubelet            Started container db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 지금까지 실습했던 yaml을 싹 정리해주자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl delete -f ./guide-03/task-02/\n",
    "pod \"whoami-health\" deleted\n",
    "pod \"whoami-lp\" deleted\n",
    "pod \"whoami-redis\" deleted\n",
    "pod \"whoami-rp\" deleted\n",
    "pod \"whoami\" deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Replicaset 실습\n",
    "\n",
    "앞에서 만든 pod을 replicaset으로 만들어보자.\n",
    "\n",
    "아래에서 replicaset은 1개로 할거고, selector는 리플리카셋 입장에서는 해당하는 라벨에 pod이 있는지 없는지를 판단한다. 타입이 app이고 service가 whoami라는 이름의 pod이 1개가 있는지를 체크한다. template는 만약에 타입이 app이고 service가 whoami라는 이름의 pod이 1개가 있는지를 체크를 해봤는데 해당하는 pod이 없으면 template 이해의 명세대로 pod을 만들라는 의미다. pod에 metadata를 보면 타입이 app이고 service가 whoami라는 라벨을 가진 pod을 만들거고 그 pod의 컨테이너는 subicura/whoami:1라는 이름의 pod을 만들겠다는 의미다.\n",
    "\n",
    "실제 현업에서는 Replicas를 거의 쓰지는 않는데 우리가 알아야 하는 이유가 있다. deployment를 현업에서 많이 쓰는데 deployment가 내부적으로 replicas를 사용하고 있기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-0-1-231 ~]$ ls\n",
    "guide-03\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ mkdir guide-03/task-03\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ sudo vim guide-03/task-03/whoami-rs.yml\n",
    "apiVersion: apps/v1\n",
    "kind: ReplicaSet\n",
    "metadata:\n",
    "  name: whoami-rs\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      type: app\n",
    "      service: whoami\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        type: app\n",
    "        service: whoami\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: whoami\n",
    "        image: subicura/whoami:1\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /\n",
    "            port: 4567\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl apply -f guide-03/task-03/whoami-rs.yml\n",
    "replicaset.apps/whoami-rs created\n",
    "\n",
    "# 아까와 유사한데 replicaset.apps/whoami-rs라는 리플리카셋이 하나 생겼다.\n",
    "# 그리고 리플리카셋이 생기면서 앞에서 정의했듯이 타입이 app이고 service가 whoami라는 라벨을 가진 pod을\n",
    "# 찾았는데 없어서 templates 이하내용에 명시된 pod을 생성한 것이다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get all\n",
    "NAME                  READY   STATUS    RESTARTS   AGE\n",
    "pod/whoami-rs-tjbrr   1/1     Running   0          115s\n",
    "\n",
    "NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
    "service/kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   21h\n",
    "\n",
    "NAME                        DESIRED   CURRENT   READY   AGE\n",
    "replicaset.apps/whoami-rs   1         1         1       115s\n",
    "\n",
    "# 아래와 같이 --show-labels 옵션을 주면 서비스는 whoami에 type는 app이라는 라벨정보를 확인할 수 있다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get pods --show-labels\n",
    "NAME              READY   STATUS    RESTARTS   AGE     LABELS\n",
    "whoami-rs-tjbrr   1/1     Running   0          4m41s   service=whoami,type=app\n",
    "\n",
    "# 위에 이 라벨링 정보는 사용자가 임의로 삭제&변경이 가능하다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl label pod/whoami-rs-tjbrr type-\n",
    "pod/whoami-rs-tjbrr labeled\n",
    "\n",
    "# 아래와 같이 pod/whoami-rs-tjbrr 에 대해서 type을 제거하니까\n",
    "# whoami-rs-7d6xg가 강제로 새로 생긴것을 확인할 수 있다.\n",
    "# 리플리카셋은 주기적으로 yaml에 명시되어 있는 pod을 찾기만 한다.\n",
    "# 이게 최초에는 없어서 yaml이 실행되면서 pod이 1개 생성되어 replicaset 1 명시된대로 그대로 유지가 되었는데\n",
    "# 지금 강제로 라벨을 제거하니까 리플리카셋 입장에서는 해당 relicaset이 없어진 것이라고 판단한 것이다.\n",
    "# 그래서 yaml에 명시된 조건에 맞는 pod을 template를 이용해서 또 다른 pod을 생성을 한 것이다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get pods --show-labels\n",
    "NAME              READY   STATUS    RESTARTS   AGE     LABELS\n",
    "whoami-rs-tjbrr   1/1     Running   0          9m33s   service=whoami\n",
    "whoami-rs-7d6xg   1/1     Running   0          11s     service=whoami,type=app\n",
    "\n",
    "\n",
    "# 아래와 같이 pod을 제거를 했을때 pod/whoami-rs-7d6xg을 삭제할 경우\n",
    "# 역시 리플리카셋 입장에서 해당 pod이 없어진 것이기 때문에 template로 새로 하나를\n",
    "# 만드는 것을 확인할 수 있다. 상태를 체크하고 있는 것이다.\n",
    "# 이런 이유로 쿠버네티스가 상태를 관리한다고 할 수 있다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl delete pod/whoami-rs-tjbrr\n",
    "pod \"whoami-rs-tjbrr\" deleted\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get pods\n",
    "NAME              READY   STATUS    RESTARTS   AGE\n",
    "whoami-rs-7d6xg   1/1     Running   0          7m15s\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl delete pod/whoami-rs-7d6xg\n",
    "pod \"whoami-rs-7d6xg\" deleted\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get pods\n",
    "NAME              READY   STATUS    RESTARTS   AGE\n",
    "whoami-rs-zbdpk   1/1     Running   0          41s\n",
    "\n",
    "# 그러면 replicaset 을 3으로 해서 다시 pod을 띄워보자\n",
    "# 아까 yaml에서 replicas 만 1에서 3으로 바꿔준 것이다.\n",
    "# 나머지 내용은 전부 동일하다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ sudo vim guide-03/task-03/whoami-rs.yml\n",
    "apiVersion: apps/v1\n",
    "kind: ReplicaSet\n",
    "metadata:\n",
    "  name: whoami-rs\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      type: app\n",
    "      service: whoami\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        type: app\n",
    "        service: whoami\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: whoami\n",
    "        image: subicura/whoami:1\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /\n",
    "            port: 4567\n",
    "                \n",
    "# 그런다음에 다시 띄우면 큐브컨트롤이 기존것과의 차이점을 인지해서 새로 명령을 내리게 된다. \n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl apply -f guide-03/task-03/whoami-rs.yml\n",
    "replicaset.apps/whoami-rs configured\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get all\n",
    "NAME                  READY   STATUS    RESTARTS   AGE\n",
    "pod/whoami-rs-zbdpk   1/1     Running   0          15m\n",
    "pod/whoami-rs-rhxv2   1/1     Running   0          76s\n",
    "pod/whoami-rs-b7khr   1/1     Running   0          76s\n",
    "\n",
    "NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
    "service/kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   21h\n",
    "\n",
    "NAME                        DESIRED   CURRENT   READY   AGE\n",
    "replicaset.apps/whoami-rs   3         3         3       32m\n",
    "\n",
    "\n",
    "# 위에 실습이 끝나면 아래와 같이 삭제를 해준다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl delete -f guide-03/task-03/\n",
    "replicaset.apps \"whoami-rs\" deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deployment 실습\n",
    "\n",
    "guide-03/task-04/whoami-deploy.yml는 위에서 replicas와 전혀 차이가 없다. kind와 metadata 이름만 다를 뿐이다.\n",
    "\n",
    "Deployment는 버전을 관리하고 rollback 같은 기능이 있을뿐이고 replicaset과 거의 차이가 없다. 예를들어서 이미지의 tag 같은 것들이 바뀌면  Deployment는 이력을 모두 기록한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-0-1-231 ~]$ ls\n",
    "guide-03\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ mkdir guide-03/task-04\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ sudo vim guide-03/task-04/whoami-deploy.yml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: whoami-deploy\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      type: app\n",
    "      service: whoami\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        type: app\n",
    "        service: whoami\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: whoami\n",
    "        image: subicura/whoami:1\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /\n",
    "            port: 4567\n",
    "                \n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl apply -f guide-03/task-04/whoami-deploy.yml\n",
    "deployment.apps/whoami-deploy created\n",
    "\n",
    "# replicas와는 다르게 NAME 세번째에 deployment.apps/whoami-deploy 부분이 새로 생겼다.\n",
    "# 이 deployment.apps/whoami-deploy가 replicaset을 만들었고, replicaset.apps/whoami-deploy-698f4ffd64가 만든 pod 세개가\n",
    "# 추가된 것을 알 수 있다. \n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get all\n",
    "NAME                                 READY   STATUS    RESTARTS   AGE\n",
    "pod/whoami-deploy-698f4ffd64-2ccjr   1/1     Running   0          5s\n",
    "pod/whoami-deploy-698f4ffd64-ldd9t   1/1     Running   0          5s\n",
    "pod/whoami-deploy-698f4ffd64-tpjrd   1/1     Running   0          5s\n",
    "\n",
    "NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
    "service/kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   22h\n",
    "\n",
    "NAME                            READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "deployment.apps/whoami-deploy   3/3     3            3           5s\n",
    "\n",
    "NAME                                       DESIRED   CURRENT   READY   AGE\n",
    "replicaset.apps/whoami-deploy-698f4ffd64   3         3         3       5s\n",
    "\n",
    "# 그러면 왜 deployment가 replicaset을 쓸까\n",
    "# 아래와 같이 yaml에서 subicura/whoami:1을 subicura/whoami:2로 바꿔서 다시 실행해보자.\n",
    "# 그러면 원래 pod은 그대로 있고 subicura/whoami:2가 실행이 된다.\n",
    "# 그러다가 subicura/whoami:2가 실행이 다 완료되면 원래 있던 3개의 pod은 종료가 된다.\n",
    "# 그리고 replicaset이 원래는 1개 였는데 2개가 된것을 알 수 있다.\n",
    "# deployment는 버전을 업그레이드할때 replicaset을 이용을 한다. \n",
    "# 위에서 순서대로 첫번째 replicaset은 버전 1을 관리한 replicaset이고\n",
    "# 아래 replicaset은 버전 2을 관리하는 replicaset이다. 버전 2를 deployment를 이용해서 띄우면\n",
    "# deployment가 처음에는 버전 1을 갖고있는 replicaset 한테 pod 3개를 띄우라고 했다가\n",
    "# pod을 2개를 띄우라고 명령을 한다. 그리고 버전 2을 관리하는 replicaset에게 pod을 1개 띄우라고 한다.\n",
    "# 이게 성공적으로 되면 버전 1을 갖고있는 replicaset에게 pod 두개띄우는걸 한개로 띄우라고하고,\n",
    "# 그런 다음에 버전 2를 갖고 있는 얘한테 pod 1개 갖고 있는걸 2개를 띄우라고 한다. 이런식으로\n",
    "# 기존의 버전과 새로운 버전을 바꿔치기 한다.\n",
    "# deployment가 replicaset을 이용을 하는 것이다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ sudo vim guide-03/task-04/whoami-deploy.yml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: whoami-deploy\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      type: app\n",
    "      service: whoami\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        type: app\n",
    "        service: whoami\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: whoami\n",
    "        image: subicura/whoami:2\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /\n",
    "            port: 4567\n",
    "                \n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl apply -f guide-03/task-04/whoami-deploy.yml\n",
    "deployment.apps/whoami-deploy configured\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get all\n",
    "NAME                                 READY   STATUS              RESTARTS   AGE\n",
    "pod/whoami-deploy-698f4ffd64-2ccjr   1/1     Running             0          4m19s\n",
    "pod/whoami-deploy-698f4ffd64-ldd9t   1/1     Running             0          4m19s\n",
    "pod/whoami-deploy-698f4ffd64-tpjrd   1/1     Running             0          4m19s\n",
    "pod/whoami-deploy-8686bdc7cc-f5ks5   0/1     ContainerCreating   0          7s\n",
    "\n",
    "NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
    "service/kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   22h\n",
    "\n",
    "NAME                            READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "deployment.apps/whoami-deploy   3/3     1            3           4m19s\n",
    "\n",
    "NAME                                       DESIRED   CURRENT   READY   AGE\n",
    "replicaset.apps/whoami-deploy-698f4ffd64   3         3         3       4m19s\n",
    "replicaset.apps/whoami-deploy-8686bdc7cc   1         1         0       7s\n",
    "\n",
    "# 한 3초 있다가 다시 아래와 같이 조회해본다.\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get all\n",
    "NAME                                 READY   STATUS    RESTARTS   AGE\n",
    "pod/whoami-deploy-8686bdc7cc-f5ks5   1/1     Running   0          71s\n",
    "pod/whoami-deploy-8686bdc7cc-vnzbf   1/1     Running   0          64s\n",
    "pod/whoami-deploy-8686bdc7cc-dtmx5   1/1     Running   0          63s\n",
    "\n",
    "NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
    "service/kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   22h\n",
    "\n",
    "NAME                            READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "deployment.apps/whoami-deploy   3/3     3            3           5m23s\n",
    "\n",
    "NAME                                       DESIRED   CURRENT   READY   AGE\n",
    "replicaset.apps/whoami-deploy-8686bdc7cc   3         3         3       71s\n",
    "replicaset.apps/whoami-deploy-698f4ffd64   0         0         0       5m23s\n",
    "\n",
    "\n",
    "# 그러면 위에서 언급한 내용이 실시간으로 어떻게 변화는지 아래와 같이 명령어를 실행해서 확인해보자.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get rs -w\n",
    "NAME                       DESIRED   CURRENT   READY   AGE\n",
    "whoami-deploy-8686bdc7cc   3         3         3       17m\n",
    "whoami-deploy-698f4ffd64   0         0         0       21m\n",
    "\n",
    "\n",
    "## 프로세스가 계속 물고 있기 때문에 새로 터미널을 띄운다.\n",
    "\n",
    "# subicura/whoami:2에서 다시 1로 바꿔서 실행해보자.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ sudo vim guide-03/task-04/whoami-deploy.yml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: whoami-deploy\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      type: app\n",
    "      service: whoami\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        type: app\n",
    "        service: whoami\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: whoami\n",
    "        image: subicura/whoami:1\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /\n",
    "            port: 4567\n",
    "                \n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl apply -f guide-03/task-04/whoami-deploy.yml\n",
    "deployment.apps/whoami-deploy configured\n",
    "                \n",
    "## 아까 kubectl get rs -w 띄워놓은 터미널로 돌아가면\n",
    "\n",
    "# pod 뜨고 죽는 현황을 확인할 수 있다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get rs -w\n",
    "NAME                       DESIRED   CURRENT   READY   AGE\n",
    "whoami-deploy-8686bdc7cc   3         3         3       17m\n",
    "whoami-deploy-698f4ffd64   0         0         0       21m\n",
    "whoami-deploy-698f4ffd64   0         0         0       27m\n",
    "whoami-deploy-698f4ffd64   1         0         0       27m\n",
    "whoami-deploy-698f4ffd64   1         0         0       27m\n",
    "whoami-deploy-698f4ffd64   1         1         0       27m\n",
    "whoami-deploy-698f4ffd64   1         1         1       27m\n",
    "whoami-deploy-8686bdc7cc   2         3         3       22m\n",
    "whoami-deploy-698f4ffd64   2         1         1       27m\n",
    "whoami-deploy-8686bdc7cc   2         3         3       22m\n",
    "whoami-deploy-698f4ffd64   2         1         1       27m\n",
    "whoami-deploy-8686bdc7cc   2         2         2       22m\n",
    "whoami-deploy-698f4ffd64   2         2         1       27m\n",
    "whoami-deploy-698f4ffd64   2         2         2       27m\n",
    "whoami-deploy-8686bdc7cc   1         2         2       22m\n",
    "whoami-deploy-698f4ffd64   3         2         2       27m\n",
    "whoami-deploy-8686bdc7cc   1         2         2       22m\n",
    "whoami-deploy-698f4ffd64   3         2         2       27m\n",
    "whoami-deploy-8686bdc7cc   1         1         1       22m\n",
    "whoami-deploy-698f4ffd64   3         3         2       27m\n",
    "whoami-deploy-698f4ffd64   3         3         3       27m\n",
    "whoami-deploy-8686bdc7cc   0         1         1       22m\n",
    "whoami-deploy-8686bdc7cc   0         1         1       22m\n",
    "whoami-deploy-8686bdc7cc   0         0         0       22m\n",
    "^C\n",
    "\n",
    "# deployment는 pod의 갯수 같은 것을 신경쓰는게 아니라 replicaset의 상태만 관리하고, replicaset에게 위임을 한다.\n",
    "# 그러면 deployment가 버전관리가 가능한데 이걸 실습을 해보자.\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get deployment\n",
    "NAME            READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "whoami-deploy   3/3     3            3           37m\n",
    "\n",
    "# deployment가 replicaset에 명령했던 이력들을 event로 확인할 수 있다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl describe deployment/whoami-deploy\n",
    "Name:                   whoami-deploy\n",
    "Namespace:              default\n",
    "CreationTimestamp:      Sat, 12 Dec 2020 06:27:12 +0000\n",
    "Labels:                 <none>\n",
    "Annotations:            deployment.kubernetes.io/revision: 3\n",
    "Selector:               service=whoami,type=app\n",
    "Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\n",
    "StrategyType:           RollingUpdate\n",
    "MinReadySeconds:        0\n",
    "RollingUpdateStrategy:  25% max unavailable, 25% max surge\n",
    "Pod Template:\n",
    "  Labels:  service=whoami\n",
    "           type=app\n",
    "  Containers:\n",
    "   whoami:\n",
    "    Image:        subicura/whoami:1\n",
    "    Port:         <none>\n",
    "    Host Port:    <none>\n",
    "    Liveness:     http-get http://:4567/ delay=0s timeout=1s period=10s #success=1 #failure=3\n",
    "    Environment:  <none>\n",
    "    Mounts:       <none>\n",
    "  Volumes:        <none>\n",
    "Conditions:\n",
    "  Type           Status  Reason\n",
    "  ----           ------  ------\n",
    "  Available      True    MinimumReplicasAvailable\n",
    "  Progressing    True    NewReplicaSetAvailable\n",
    "OldReplicaSets:  <none>\n",
    "NewReplicaSet:   whoami-deploy-698f4ffd64 (3/3 replicas created)\n",
    "Events:\n",
    "  Type    Reason             Age                From                   Message\n",
    "  ----    ------             ----               ----                   -------\n",
    "  Normal  ScalingReplicaSet  34m                deployment-controller  Scaled up replica set whoami-deploy-8686bdc7cc to 1\n",
    "  Normal  ScalingReplicaSet  33m                deployment-controller  Scaled down replica set whoami-deploy-698f4ffd64 to 2\n",
    "  Normal  ScalingReplicaSet  33m                deployment-controller  Scaled up replica set whoami-deploy-8686bdc7cc to 2\n",
    "  Normal  ScalingReplicaSet  33m                deployment-controller  Scaled down replica set whoami-deploy-698f4ffd64 to 1\n",
    "  Normal  ScalingReplicaSet  33m                deployment-controller  Scaled up replica set whoami-deploy-8686bdc7cc to 3\n",
    "  Normal  ScalingReplicaSet  33m                deployment-controller  Scaled down replica set whoami-deploy-698f4ffd64 to 0\n",
    "  Normal  ScalingReplicaSet  11m                deployment-controller  Scaled up replica set whoami-deploy-698f4ffd64 to 1\n",
    "  Normal  ScalingReplicaSet  11m                deployment-controller  Scaled down replica set whoami-deploy-8686bdc7cc to 2\n",
    "  Normal  ScalingReplicaSet  11m                deployment-controller  Scaled up replica set whoami-deploy-698f4ffd64 to 2\n",
    "  Normal  ScalingReplicaSet  11m                deployment-controller  Scaled down replica set whoami-deploy-8686bdc7cc to 1\n",
    "  Normal  ScalingReplicaSet  11m (x2 over 38m)  deployment-controller  Scaled up replica set whoami-deploy-698f4ffd64 to 3\n",
    "  Normal  ScalingReplicaSet  11m                deployment-controller  Scaled down replica set whoami-deploy-8686bdc7cc to 0\n",
    "  \n",
    "\n",
    "# deployment의 history도 다음과 같은 명령어를 실행해서 확인할 수 있다.\n",
    "# subicura/whoami:1 --> subicura/whoami:2 --> subicura/whoami:1 이렇게 세번 바뀌었기 떄문에\n",
    "# revision이 세개가 있는것이고, 이미지 버전이 똑같은 경우에는 이전 기록을 지우기 때문에\n",
    "# 총 버전이 세개지만 마지막 버전 2개만 보이는 것이다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl rollout history deployment/whoami-deploy\n",
    "deployment.apps/whoami-deploy\n",
    "REVISION  CHANGE-CAUSE\n",
    "2         <none>\n",
    "3         <none>\n",
    "\n",
    "# 특정 revision의 상세내역도 확인할 수 있다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl rollout history deployment/whoami-deploy --revision=2\n",
    "deployment.apps/whoami-deploy with revision #2\n",
    "Pod Template:\n",
    "  Labels:       pod-template-hash=8686bdc7cc\n",
    "        service=whoami\n",
    "        type=app\n",
    "  Containers:\n",
    "   whoami:\n",
    "    Image:      subicura/whoami:2\n",
    "    Port:       <none>\n",
    "    Host Port:  <none>\n",
    "    Liveness:   http-get http://:4567/ delay=0s timeout=1s period=10s #success=1 #failure=3\n",
    "    Environment:        <none>\n",
    "    Mounts:     <none>\n",
    "  Volumes:      <none>\n",
    "    \n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl rollout history deployment/whoami-deploy --revision=3\n",
    "deployment.apps/whoami-deploy with revision #3\n",
    "Pod Template:\n",
    "  Labels:       pod-template-hash=698f4ffd64\n",
    "        service=whoami\n",
    "        type=app\n",
    "  Containers:\n",
    "   whoami:\n",
    "    Image:      subicura/whoami:1\n",
    "    Port:       <none>\n",
    "    Host Port:  <none>\n",
    "    Liveness:   http-get http://:4567/ delay=0s timeout=1s period=10s #success=1 #failure=3\n",
    "    Environment:        <none>\n",
    "    Mounts:     <none>\n",
    "  Volumes:      <none>\n",
    "    \n",
    "# 이런식으로 내가 버전을 하나 올렸는데 그 버전에 문제가 있다면 다시 rollback해야 한다.\n",
    "# 이런 시나리오도 대처가 가능하다. \n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl rollout undo deployment/whoami-deploy\n",
    "deployment.apps/whoami-deploy rolled back\n",
    "\n",
    "# 롤백을 하는데 revision을 롤백하는게 아니라 리비전 4번으로 하고, 이전의 상태를 롤백하는 개념이다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl rollout history deployment/whoami-deploy\n",
    "deployment.apps/whoami-deploy\n",
    "REVISION  CHANGE-CAUSE\n",
    "3         <none>\n",
    "4         <none>\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl rollout history deployment/whoami-deploy --revision=4\n",
    "deployment.apps/whoami-deploy with revision #4\n",
    "Pod Template:\n",
    "  Labels:       pod-template-hash=8686bdc7cc\n",
    "        service=whoami\n",
    "        type=app\n",
    "  Containers:\n",
    "   whoami:\n",
    "    Image:      subicura/whoami:2\n",
    "    Port:       <none>\n",
    "    Host Port:  <none>\n",
    "    Liveness:   http-get http://:4567/ delay=0s timeout=1s period=10s #success=1 #failure=3\n",
    "    Environment:        <none>\n",
    "    Mounts:     <none>\n",
    "  Volumes:      <none>\n",
    "    \n",
    "\n",
    "# deployment는 strategy라는 항목이 있다.\n",
    "# 아까는 3개의 pod이 띄워져 있는 상태에서 버전이 변화하면 pod이 하나가 뜨면 하나가 죽고, 다시 하나가 뜨면 하나가 죽는\n",
    "# 이런 방식인데 이게 만약에 pod이 100개라고 치면 pod이 한방에 50개뜨고 다 뜨면 50개 죽이고 이런식으로 하고 싶을 것이다.\n",
    "# 이런걸 적용할 수 있는 옵션이다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ sudo vim guide-03/task-04/whoami-deploy-strategy.yml\n",
    "apiVersion: apps/v1beta2\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: whoami-deploy\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      type: app\n",
    "      service: whoami\n",
    "  minReadySeconds: 5\n",
    "  strategy:\n",
    "    type: RollingUpdate\n",
    "    rollingUpdate:\n",
    "      maxSurge: 1\n",
    "      maxUnavailable: 1\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        type: app\n",
    "        service: whoami\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: whoami\n",
    "        image: subicura/whoami:1\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /\n",
    "            port: 4567\n",
    "                \n",
    "\n",
    "# 위에 실습이 끝나면 아래와 같이 삭제를 해준다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl delete -f guide-03/task-04/\n",
    "deployment.apps \"whoami-deploy\" deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Service 실습\n",
    "\n",
    "지금까지는 컨테이너를 띄우기만 했지 실제 접속을 해보지는 않았다.\n",
    "\n",
    "service라는 개념이해를 위해 아래의 URL을 참고한다.\n",
    "\n",
    "https://medium.com/google-cloud/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what-922f010849e0\n",
    "\n",
    "\n",
    "docker-compose 같은 경우에는 host에 port를 연결하는 컨테이너가 있었고, host에 연결을 하지 않은 컨테이너가 있었는데 그거를 쿠버네티스에서는 명시적으로 구분을 한다.\n",
    "\n",
    "그래서 clusterIP라는 용어가 있는데 이거는 도커에서 실습한 backend나 mongodb의 포트라고 생각하면 된다. clusterIP는 외부로 노출하는 IP가 아니라 내부적으로 통신할때 쓰는 IP를 말한다. docker-compose로 컨테이너를 띄울때는 아무설정을 하지 않아도 몽고디비 안에 있던 27017을 노출을 시킬 수 있었다. 하지만 쿠버네티스에서는 만약에 몽고디비의 27017을 개방하고 싶다 그러면 clusterIP로 지정을 해줘야 한다. 결론은 어떤 pod에 연결하기 위해서는 해당 pod에 반드시 clusterIP가 부여되어 있어야 한다. \n",
    "\n",
    "그러면 외부에서 접속을 하려면 어떻게 해야하냐. 쿠버네티스에서는 nodeport라는 것을 제공한다. 도커 같은 경우에는 포트를 오픈하면 그 호스트에서만 포트가 열리는데 nodeport같은 경우에는 갖고 있는 모든 노드의 포트가 같이 열리게 된다. 예를 들어서 100개의 노드다 그러면 100개의 노드에 포트가 전부 열리게 된다. 그 100개중에 아무거나 하나접속하면 알아서 내부에 있는 pod을 찾아서 접속하게 설계가 되어 있다.\n",
    "\n",
    "\n",
    "\n",
    "loadbalancer라는 것도 있다. AWS의 로드밸런서라고 보면 된다. nodeport를 열어주면 모든 노드의 해당 포트가 전부 열린다고 했다. loadbalancer를 쓰게 되면 실제로는 loadbalancer만 포트가 열리게 되고, 이게 서비스에 붙게 된다. \n",
    "\n",
    "\n",
    "ingress라는 개념도 있다. 이거는 어떤 도메인과 path를 이용해서 원하는 서비스를 연결해주는 기능이다. ingress 안에 nginx가 있는경우가 많다. 이게 무슨의미냐면 ingress는 쿠버네티스에서 얘기하는 개념이고, 실제 구현체로 nginx도 있고, HAproxy도 있고, AWS ELB 또는 ALB도 ingress로 사용할 수 있다. 사용자가 어떻게 구현하느냐에 따라 다른 형태가 되는 것이다.\n",
    "\n",
    "1) clusterIP 기능을 이용해서 \n",
    "\n",
    "내부서비스 간에 통신하는 것을 실습해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deployment로 redis를 띄울거고 deployment는 type은 db면서 service는 redis인 pod을 찾는다. \n",
    "# 그래서 type은 db면서 service는 redis를 만족하는 pod이 없으면\n",
    "# tempates를 이용해서 pod을 생성한다. \n",
    "# ---밑에부분이 clusterIP에 관한 설정이다. ClusrterIP는 라벨이 type은 db면서 service는 redis인 pod을 바라본다.\n",
    "# clusterIP의 selector가 그 역할을 한다.\n",
    "# 아래와 같이 --- 구분을 통해 분리하는 것이 양식이다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ mkdir guide-03/task-05/\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ sudo vim guide-03/task-05/redis-app.yml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: redis\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      type: db\n",
    "      service: redis\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        type: db\n",
    "        service: redis\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: redis\n",
    "        image: redis\n",
    "        ports:\n",
    "        - containerPort: 6379\n",
    "          protocol: TCP\n",
    "---\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: redis\n",
    "spec:\n",
    "  ports:\n",
    "  - port: 6379\n",
    "    protocol: TCP\n",
    "  selector:\n",
    "    type: db\n",
    "    service: redis\n",
    "\n",
    "# deployment와 service가 생성된 것을 알 수 있다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl apply -f guide-03/task-05/redis-app.yml\n",
    "deployment.apps/redis created\n",
    "service/redis created\n",
    "\n",
    "# deployment가 하나 생성되었고, 이 deployment가 replicaset을 생성했고,\n",
    "# 이 replicaset이 pod을 생성했다. 그리고 여기에 redis라는 서비스가 추가가 된것이다.\n",
    "# redis는 10.43.3.112를 부여받았는데 10.43.3.112로 접근하면 pod/redis-b6fd45ccc-lcg7j에 접근하는 것이다.\n",
    "# 지금은 pod이 하나라서 무조건 pod/redis-b6fd45ccc-lcg7j로 접속하겠지만 만약에 pod이 여러개가 있으면\n",
    "# 10.43.3.112로 접근했을때 내부적으로 로드밸런스처럼 여러개의 pod에 번갈아가면서 접속한다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get all\n",
    "NAME                        READY   STATUS    RESTARTS   AGE\n",
    "pod/redis-b6fd45ccc-lcg7j   1/1     Running   0          27s\n",
    "\n",
    "NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE\n",
    "service/kubernetes   ClusterIP   10.43.0.1     <none>        443/TCP    24h\n",
    "service/redis        ClusterIP   10.43.3.112   <none>        6379/TCP   27s\n",
    "\n",
    "NAME                    READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "deployment.apps/redis   1/1     1            1           27s\n",
    "\n",
    "NAME                              DESIRED   CURRENT   READY   AGE\n",
    "replicaset.apps/redis-b6fd45ccc   1         1         1       27s\n",
    "\n",
    "# 10.42.0.36:6379는 pod/redis-b6fd45ccc-lcg7j의 endpoin이다.\n",
    "# 10.43.3.112로 접근하면 10.42.0.36:6379를 통해 pod/redis-b6fd45ccc-lcg7j에 접근할 수 있다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl describe service/redis\n",
    "Name:              redis\n",
    "Namespace:         default\n",
    "Labels:            <none>\n",
    "Annotations:       <none>\n",
    "Selector:          service=redis,type=db\n",
    "Type:              ClusterIP\n",
    "IP:                10.43.3.112\n",
    "Port:              <unset>  6379/TCP\n",
    "TargetPort:        6379/TCP\n",
    "Endpoints:         10.42.0.36:6379\n",
    "Session Affinity:  None\n",
    "Events:            <none>\n",
    "    \n",
    "# pod을 세개로 늘려보자.\n",
    "# 기존내용에서 replicas: 3만 추가한 것이다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ sudo vim guide-03/task-05/redis-app.yml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: redis\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      type: db\n",
    "      service: redis\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        type: db\n",
    "        service: redis\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: redis\n",
    "        image: redis\n",
    "        ports:\n",
    "        - containerPort: 6379\n",
    "          protocol: TCP\n",
    "---\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: redis\n",
    "spec:\n",
    "  ports:\n",
    "  - port: 6379\n",
    "    protocol: TCP\n",
    "  selector:\n",
    "    type: db\n",
    "    service: redis\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl apply -f guide-03/task-05/redis-app.yml\n",
    "deployment.apps/redis configured\n",
    "service/redis unchanged\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get all\n",
    "NAME                        READY   STATUS    RESTARTS   AGE\n",
    "pod/redis-b6fd45ccc-lcg7j   1/1     Running   0          14m\n",
    "pod/redis-b6fd45ccc-nlpn4   1/1     Running   0          7s\n",
    "pod/redis-b6fd45ccc-vtnff   1/1     Running   0          7s\n",
    "\n",
    "NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE\n",
    "service/kubernetes   ClusterIP   10.43.0.1     <none>        443/TCP    24h\n",
    "service/redis        ClusterIP   10.43.3.112   <none>        6379/TCP   14m\n",
    "\n",
    "NAME                    READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "deployment.apps/redis   3/3     3            3           14m\n",
    "\n",
    "NAME                              DESIRED   CURRENT   READY   AGE\n",
    "replicaset.apps/redis-b6fd45ccc   3         3         3       14m\n",
    "\n",
    "# 10.43.3.112로 접근했을때 바라보는게 아까는 엔드포인트가 하나였는데\n",
    "# 지금은 세개가 되었다. 로드밸런서 처럼 동작할 것이다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl describe service/redis\n",
    "Name:              redis\n",
    "Namespace:         default\n",
    "Labels:            <none>\n",
    "Annotations:       <none>\n",
    "Selector:          service=redis,type=db\n",
    "Type:              ClusterIP\n",
    "IP:                10.43.3.112\n",
    "Port:              <unset>  6379/TCP\n",
    "TargetPort:        6379/TCP\n",
    "Endpoints:         10.42.0.36:6379,10.42.0.37:6379,10.42.0.38:6379\n",
    "Session Affinity:  None\n",
    "Events:            <none>\n",
    "    \n",
    "# 리플리카셋을 다시 1로해서 적용해주자.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ sudo vim guide-03/task-05/redis-app.yml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: redis\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      type: db\n",
    "      service: redis\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        type: db\n",
    "        service: redis\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: redis\n",
    "        image: redis\n",
    "        ports:\n",
    "        - containerPort: 6379\n",
    "          protocol: TCP\n",
    "---\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: redis\n",
    "spec:\n",
    "  ports:\n",
    "  - port: 6379\n",
    "    protocol: TCP\n",
    "  selector:\n",
    "    type: db\n",
    "    service: redis\n",
    "        \n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl apply -f guide-03/task-05/redis-app.yml\n",
    "deployment.apps/redis configured\n",
    "service/redis unchanged\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get all\n",
    "NAME                        READY   STATUS    RESTARTS   AGE\n",
    "pod/redis-b6fd45ccc-lcg7j   1/1     Running   0          17m\n",
    "\n",
    "NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE\n",
    "service/kubernetes   ClusterIP   10.43.0.1     <none>        443/TCP    24h\n",
    "service/redis        ClusterIP   10.43.3.112   <none>        6379/TCP   17m\n",
    "\n",
    "NAME                    READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "deployment.apps/redis   1/1     1            1           17m\n",
    "\n",
    "NAME                              DESIRED   CURRENT   READY   AGE\n",
    "replicaset.apps/redis-b6fd45ccc   1         1         1       17m\n",
    "\n",
    "\n",
    "# redis를 만들었으니까 그거를 바라보는 app을 만들어보자.\n",
    "# 호스트를 redis로 사용을 했다. docker-compose에서 container의 이름이 곧 domain인 것처럼\n",
    "# 동일하게 동작을 한다. 위에서 서비스의 이름을 redis로 지정해줬기 때문에\n",
    "# whoami 입장에서는 redis로 접근을 하면 redis service로 접근을 한다. 서비스 이름이 곧 도메인 이름처럼 쓰인다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ sudo vim guide-03/task-05/whoami-deploy.yml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: whoami\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      type: app\n",
    "      service: whoami\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        type: app\n",
    "        service: whoami\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: whoami\n",
    "        image: subicura/whoami-redis:1\n",
    "        env:\n",
    "        - name: REDIS_HOST\n",
    "          value: \"redis\"\n",
    "        - name: REDIS_PORT\n",
    "          value: \"6379\"\n",
    "            \n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl apply -f guide-03/task-05/whoami-deploy.yml\n",
    "deployment.apps/whoami created\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get all\n",
    "NAME                          READY   STATUS    RESTARTS   AGE\n",
    "pod/redis-b6fd45ccc-lcg7j     1/1     Running   0          24m\n",
    "pod/whoami-676d78bbf8-8m29n   1/1     Running   0          7s\n",
    "\n",
    "NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE\n",
    "service/kubernetes   ClusterIP   10.43.0.1     <none>        443/TCP    25h\n",
    "service/redis        ClusterIP   10.43.3.112   <none>        6379/TCP   24m\n",
    "\n",
    "NAME                     READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "deployment.apps/redis    1/1     1            1           24m\n",
    "deployment.apps/whoami   1/1     1            1           7s\n",
    "\n",
    "NAME                                DESIRED   CURRENT   READY   AGE\n",
    "replicaset.apps/redis-b6fd45ccc     1         1         1       24m\n",
    "replicaset.apps/whoami-676d78bbf8   1         1         1       7s\n",
    "\n",
    "# whoami가 잘떴고 실제 접속이 잘 되는지 테스트를 해보자.\n",
    "# 아래와 같이 redis에 잘 접근하는 것을 확인할 수 있다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl exec -it whoami-676d78bbf8-8m29n sh\n",
    "kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.\n",
    "/usr/src/app # apk add curl busybox-extras\n",
    "fetch http://dl-cdn.alpinelinux.org/alpine/v3.8/main/x86_64/APKINDEX.tar.gz\n",
    "fetch http://dl-cdn.alpinelinux.org/alpine/v3.8/community/x86_64/APKINDEX.tar.gz\n",
    "(1/5) Installing busybox-extras (1.28.4-r3)\n",
    "Executing busybox-extras-1.28.4-r3.post-install\n",
    "(2/5) Installing nghttp2-libs (1.39.2-r0)\n",
    "(3/5) Installing libssh2 (1.9.0-r1)\n",
    "(4/5) Installing libcurl (7.61.1-r3)\n",
    "(5/5) Installing curl (7.61.1-r3)\n",
    "Executing busybox-1.28.4-r1.trigger\n",
    "OK: 16 MiB in 35 packages\n",
    "/usr/src/app # telnet redis 6379\n",
    "keys *\n",
    "*0\n",
    "^C\n",
    "Console escape. Commands are:\n",
    "\n",
    " l      go to line mode\n",
    " c      go to character mode\n",
    " z      suspend telnet\n",
    " e      exit telnet\n",
    "/usr/src/app # curl localhost:4567\n",
    "1\n",
    "/usr/src/app # curl localhost:4567\n",
    "2\n",
    "/usr/src/app # curl localhost:4567\n",
    "3\n",
    "/usr/src/app # curl localhost:4567\n",
    "4\n",
    "/usr/src/app # telnet redis 6379\n",
    "keys *\n",
    "*1\n",
    "$5\n",
    "count\n",
    "GET count\n",
    "$1\n",
    "4\n",
    "^C\n",
    "Console escape. Commands are:\n",
    "\n",
    " l      go to line mode\n",
    " c      go to character mode\n",
    " z      suspend telnet\n",
    " e      exit telnet\n",
    "\n",
    "/usr/src/app # exit\n",
    "[ec2-user@ip-10-0-1-231 ~]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nodeport 실습\n",
    "\n",
    "nodeport 기능을 이용해서 외부와의 통신을해보자\n",
    "\n",
    "app을 바라보는 nodeport를 적용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-0-1-231 ~]$ sudo vim guide-03/task-05/np.yml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: whoami\n",
    "spec:\n",
    "  type: NodePort\n",
    "  ports:\n",
    "  - port: 4567\n",
    "    protocol: TCP\n",
    "  selector:\n",
    "    type: app\n",
    "    service: whoami\n",
    "        \n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl apply -f guide-03/task-05/np.yml\n",
    "service/whoami created\n",
    "\n",
    "# whoami라는 노드포트가 연결이 되었고, 포트를 보니까 내부에 4567이 30135에 물게된다.\n",
    "# 따로지정 안하면 30000이상의 포트중에 랜덤하게 지정을 해준다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get all\n",
    "NAME                          READY   STATUS    RESTARTS   AGE\n",
    "pod/redis-b6fd45ccc-lcg7j     1/1     Running   0          59m\n",
    "pod/whoami-676d78bbf8-8m29n   1/1     Running   0          35m\n",
    "\n",
    "NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE\n",
    "service/kubernetes   ClusterIP   10.43.0.1      <none>        443/TCP          25h\n",
    "service/redis        ClusterIP   10.43.3.112    <none>        6379/TCP         59m\n",
    "service/whoami       NodePort    10.43.85.165   <none>        4567:30135/TCP   23s\n",
    "\n",
    "NAME                     READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "deployment.apps/redis    1/1     1            1           59m\n",
    "deployment.apps/whoami   1/1     1            1           35m\n",
    "\n",
    "NAME                                DESIRED   CURRENT   READY   AGE\n",
    "replicaset.apps/redis-b6fd45ccc     1         1         1       59m\n",
    "replicaset.apps/whoami-676d78bbf8   1         1         1       35m\n",
    "\n",
    "## 웹브라우저를 열고 [ec2-public-ip]:30135 로 접속하면 숫자 5가 전시되는 것을 알 수 있다.\n",
    "## 웹브라우저를 새로고침 할때마다 숫자가 올라가는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ingress 실습\n",
    "\n",
    "ingress는 어떤 도메인을 지정하여 해당 도메인에 접근시 특정 서비스에 접근하도록 하는 기능임.\n",
    "\n",
    "whoami-v1라는 이름으로 ingress를 만들것이다. whoami-v1은 도메인 루트로 접근을 할 것이다.\n",
    "\n",
    "ssl이 아니더라도 redirect를 하지 않겠다는 옵션도 주었다. 그리고 rule을 지정해줘야 하는데 rule은 `v1.whoami.3.35.175.198.sslip.io`이라는 도메인 주소로 접속을 하면 아래의 백앤드 서비스로 접속을 하겠다는 것이다.\n",
    "\n",
    "아래에서 `3.35.175.198`는 EC2의 public ip다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-0-1-231 ~]$ sudo vim guide-03/task-05/in.yml\n",
    "apiVersion: extensions/v1beta1\n",
    "kind: Ingress\n",
    "metadata:\n",
    "  name: whoami-v1\n",
    "  annotations:\n",
    "    ingress.kubernetes.io/rewrite-target: \"/\"\n",
    "    ingress.kubernetes.io/ssl-redirect: \"false\"\n",
    "spec:\n",
    "  rules:\n",
    "  - host: v1.whoami.3.35.175.198.sslip.io\n",
    "    http:\n",
    "      paths:\n",
    "      - path: /\n",
    "        backend:\n",
    "          serviceName: whoami-v1\n",
    "          servicePort: 4567\n",
    "\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl apply -f guide-03/task-05/in.yml\n",
    "Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress\n",
    "ingress.extensions/whoami-v1 created\n",
    "\n",
    "# ingress가 생성된 것을 확인할 수 있고\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get ingress\n",
    "Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress\n",
    "NAME        CLASS    HOSTS                             ADDRESS      PORTS   AGE\n",
    "whoami-v1   <none>   v1.whoami.3.35.175.198.sslip.io   10.0.1.231   80      61s\n",
    "\n",
    "# 웹브라우저에서 v1.whoami.3.35.175.198.sslip.io 로 접속을 해본다.\n",
    "# 아마 404 not found가 뜰 것이다. \n",
    "# 왜냐하면 이 ingress는 whoami-v1 서비스를 바라보게 했는데 whoami-v1 서비스가 없어서 그렇다.\n",
    "# 그러면 whoami-v1 서비스를 만들고 다시 시도해보자.\n",
    "# --- 밑에는 clusterIP 설정하는 부분인데 ingress가 해당하는 ip로 접근하겠다는 설정이다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ sudo vim guide-03/task-05/whoami-v1.yml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: whoami-v1\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      type: app\n",
    "      service: whoami\n",
    "      version: v1\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        type: app\n",
    "        service: whoami\n",
    "        version: v1\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: whoami\n",
    "        image: subicura/whoami:1\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /\n",
    "            port: 4567\n",
    "\n",
    "---\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: whoami-v1\n",
    "spec:\n",
    "  ports:\n",
    "  - port: 4567\n",
    "    protocol: TCP\n",
    "  selector:\n",
    "    type: app\n",
    "    service: whoami\n",
    "    version: v1\n",
    "        \n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl apply -f guide-03/task-05/whoami-v1.yml\n",
    "deployment.apps/whoami-v1 created\n",
    "service/whoami-v1 created\n",
    "\n",
    "# whoami-v1가 만들어진 것을 볼 수 있다.\n",
    "[ec2-user@ip-10-0-1-231 ~]$ kubectl get all\n",
    "NAME                           READY   STATUS    RESTARTS   AGE\n",
    "pod/redis-b6fd45ccc-lcg7j      1/1     Running   0          92m\n",
    "pod/whoami-676d78bbf8-8m29n    1/1     Running   0          68m\n",
    "pod/whoami-v1-54588fd7-fm7hb   1/1     Running   0          27s\n",
    "pod/whoami-v1-54588fd7-wt6lq   1/1     Running   0          27s\n",
    "pod/whoami-v1-54588fd7-n6vk6   1/1     Running   0          27s\n",
    "\n",
    "NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\n",
    "service/kubernetes   ClusterIP   10.43.0.1       <none>        443/TCP          26h\n",
    "service/redis        ClusterIP   10.43.3.112     <none>        6379/TCP         92m\n",
    "service/whoami       NodePort    10.43.85.165    <none>        4567:30135/TCP   33m\n",
    "service/whoami-v1    ClusterIP   10.43.220.236   <none>        4567/TCP         27s\n",
    "\n",
    "NAME                        READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "deployment.apps/redis       1/1     1            1           92m\n",
    "deployment.apps/whoami      1/1     1            1           68m\n",
    "deployment.apps/whoami-v1   3/3     3            3           27s\n",
    "\n",
    "NAME                                 DESIRED   CURRENT   READY   AGE\n",
    "replicaset.apps/redis-b6fd45ccc      1         1         1       92m\n",
    "replicaset.apps/whoami-676d78bbf8    1         1         1       68m\n",
    "replicaset.apps/whoami-v1-54588fd7   3         3         3       27s\n",
    "\n",
    "# 그러면 아까 만든 ingress가 whoami-v1을 바라볼테니까 웹브라우저에서 \n",
    "# 다시한번 v1.whoami.3.35.175.198.sslip.io 로 접속해본다.\n",
    "# 그러면 whoami-v1-54588fd7-n6vk6 와 같은 메세지가 나오는 것을 확인할 수 있다.\n",
    "# 새로 고침을 할때마다 whoami-v1-54588fd7-wt6lq whoami-v1-54588fd7-fm7hb 이런식으로 세개가 번갈아가면서\n",
    "# 나올 것이다. 왜냐하면 pod을 세개 띄웠기 때문에 ingress는 세개의 pod을 바라보는 서비스로 접속을 한거고\n",
    "# 그래서 새로고침을 할때마다 세개의 팟이 내부적으로 번갈아가면서 접속이 되는 것을 확인할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
