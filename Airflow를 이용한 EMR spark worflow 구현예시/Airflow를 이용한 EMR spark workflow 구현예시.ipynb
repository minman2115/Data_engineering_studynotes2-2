{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "[구현목표]\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/41605276/93855558-a0e70000-fcf2-11ea-98e6-07c2d4180f31.png)\n",
    "\n",
    "\n",
    "#### Airflow client(EC2)에서  dag를 실행하면 \n",
    "\n",
    "#### EMR cluster create --> pyspark job submit --> EMR terminate 의 일련의 workload가 구동됨\n",
    "\n",
    "\n",
    "[구현방법]\n",
    "\n",
    "#### step 0) Airflow clinet를 위한 EC2 생성\n",
    "\n",
    "\n",
    "#### step 1) Airflow client(EC2)에 SSH 접속해 다음과 같이 Airflow를 설치한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# airflow 설치를 위한 명령어\n",
    "[ec2-user@ip-10-1-10-239 ~]$ sudo yum update -y\n",
    "[ec2-user@ip-10-1-10-239 ~]$ sudo yum install python3 -y\n",
    "[ec2-user@ip-10-1-10-239 ~]$ sudo yum install gcc python3-devel -y\n",
    "[ec2-user@ip-10-1-10-239 ~]$ sudo pip3 install apache-airflow==1.10.3\n",
    "[ec2-user@ip-10-1-10-239 ~]$ sudo pip3 install werkzeug==0.15.4\n",
    "[ec2-user@ip-10-1-10-239 ~]$ sudo pip3 install boto3\n",
    "[ec2-user@ip-10-1-10-239 ~]$ aws configure\n",
    "AWS Access Key ID [None]: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "AWS Secret Access Key [None]: yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
    "Default region name [None]: ap-northeast-2\n",
    "Default output format [None]: json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 2) Airflow 구동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta DB 구동\n",
    "[ec2-user@ip-10-1-10-239 ~]$ airflow initdb\n",
    "                                                                                                              \n",
    "# webserver 구동\n",
    "[ec2-user@ip-10-1-10-239 ~]$ airflow webserver -p 8080\n",
    "# 위와 같이 웹서버를 구동하고 나서 웹브라우져로 이동한 다음 [ec2 server public ip]:8080 으로 접속하면 airflow WebUI화면을 확인할 수 있다.\n",
    "\n",
    "# 터미널 새 창을 열어서 아래 커맨드 입력                                    \n",
    "# scheduler 구동. scheduler는 DAG들의 스케쥴링 및 실행을 담당한다.\n",
    "[ec2-user@ip-10-1-10-239 ~]$ airflow scheduler                                                                                                                                                                                                                           \n",
    "    \n",
    "# 다시 새로운 터미널창을 열고 아래의 커맨드를 입력\n",
    "# ~/airflow 위치에 airflow 관련 파일이 저장됨\n",
    "[ec2-user@ip-10-1-10-239 ~]$ cd ~/airflow\n",
    "[ec2-user@ip-10-1-10-239 airflow]$ ls\n",
    "airflow.cfg  airflow.db  airflow-webserver.pid  logs  unittests.cfg\n",
    "# airflow.cfg : Airflow 관련 설정, airflow.db : sqlite 데이터베이스\n",
    "\n",
    "# airflow 폴더에 dags라는 폴더가 없다면 아래와 같이 mkdir dags로 생성한다. \n",
    "# dags 디렉토리는 DAG 파일이 저장되는 장소다.\n",
    "[ec2-user@ip-10-1-10-239 airflow]$ mkdir dags\n",
    "[ec2-user@ip-10-1-10-239 airflow]$ ls\n",
    "airflow.cfg  airflow.db  airflow-webserver.pid  dags  logs  unittests.cfg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 3) Airflow UI에 접속하여 'connection' 설정\n",
    "\n",
    "- 웹브라우저를 열어서 `[ec2 public ip]:8080`로 접속\n",
    "\n",
    "\n",
    "- Web UI 상단에 'Admin' --> 'Connections' 클릭\n",
    "\n",
    "\n",
    "- 'aws_default' 좌측에 연필모양의 아이콘 클릭\n",
    "\n",
    "\n",
    "- 'Extra'에 {\"region_name\": \"us-east-1\"}를 {\"region_name\": \"ap-northeast-2\"}로 변경하고 save\n",
    "\n",
    "#### step 4) DAG 구현 및 Airflow 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-1-10-144 airflow]$ cd dags\n",
    "[ec2-user@ip-10-1-10-144 dags]$ sudo vim test.py\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "from airflow.contrib.operators.emr_create_job_flow_operator import EmrCreateJobFlowOperator\n",
    "from airflow.contrib.operators.emr_add_steps_operator import EmrAddStepsOperator\n",
    "from airflow.contrib.sensors.emr_step_sensor import EmrStepSensor\n",
    "from airflow.contrib.operators.emr_terminate_job_flow_operator import EmrTerminateJobFlowOperator\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'minman',\n",
    "    'depends_on_past': False,\n",
    "    # start_date = The first dag start time. keep it STATIC 그리고 UTC 기준으로 작성할것\n",
    "    'start_date': datetime(2020, 9, 17, 7, 30),\n",
    "    'email': ['[email_address]@gmail.com'],\n",
    "    'email_on_failure': True\n",
    "}\n",
    "\n",
    "dag = DAG('emr_job_flow_test',\n",
    "         default_args=default_args,\n",
    "         schedule_interval= dt.timedelta(hours=1),\n",
    "         catchup=False\n",
    ")\n",
    "\n",
    "JOB_FLOW_OVERRIDES = {\n",
    "    'Name' : 'pms-EMRtest-test',\n",
    "    'LogUri' : 's3://[bucket_name]/',\n",
    "    'ReleaseLabel' : 'emr-5.28.1',\n",
    "    'Instances' : {\n",
    "            'Ec2KeyName': '[keypair_name]',\n",
    "            'Ec2SubnetId': 'subnet-xxxxxxxxxxxxxxxxx',\n",
    "            'EmrManagedMasterSecurityGroup': 'sg-xxxxxxxxxxxxxxxxx',\n",
    "            'EmrManagedSlaveSecurityGroup': 'sg-xxxxxxxxxxxxxxxxx',\n",
    "            'KeepJobFlowAliveWhenNoSteps': True,\n",
    "            'TerminationProtected': False,\n",
    "            'InstanceGroups': [{\n",
    "                'InstanceRole': 'MASTER',\n",
    "                \"InstanceCount\": 1,\n",
    "                    \"InstanceType\": 'm5.2xlarge',\n",
    "                    \"Market\": \"SPOT\",\n",
    "                    \"Name\": \"Master\"\n",
    "                }, {\n",
    "                    'InstanceRole': 'CORE',\n",
    "                    \"InstanceCount\": 3,\n",
    "                    \"InstanceType\": 'm5.2xlarge',\n",
    "                    \"Market\": \"SPOT\",\n",
    "                    \"Name\": \"Core\",\n",
    "                }, {\n",
    "                    'InstanceRole': 'TASK',\n",
    "                    \"InstanceCount\": 3,\n",
    "                    \"InstanceType\": 'm5.2xlarge',\n",
    "                    \"Market\": \"SPOT\",\n",
    "                    \"Name\": \"Core\",\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    'Applications':[{'Name': 'Spark'},{'Name': 'Hadoop'},{'Name': 'Hive'}],\n",
    "    'JobFlowRole':'EMR_EC2_DefaultRole',\n",
    "    'ServiceRole':'EMR_DefaultRole',\n",
    "    'Tags' : [{'Key': 'name', 'Value': 'pms-EMR-test'},\n",
    "              {'Key': 'expiry-date', 'Value': '2020-09-16'},\n",
    "              {'Key': 'owner', 'Value': 'pms'}],\n",
    "    'BootstrapActions':[\n",
    "            {\n",
    "                'Name': 'Maximize Spark Default Config',\n",
    "                'ScriptBootstrapAction': {'Path': 's3://[bucket_name]/maximize-spark-default-config.sh'}\n",
    "                # https://github.com/aws-samples/emr-bootstrap-actions/blob/master/spark/maximize-spark-default-config\n",
    "            }\n",
    "    ],\n",
    "    \"VisibleToAllUsers\": True\n",
    "}\n",
    "\n",
    "cluster_creator = EmrCreateJobFlowOperator(\n",
    "   task_id='create_job_flow',\n",
    "   job_flow_overrides=JOB_FLOW_OVERRIDES,\n",
    "   aws_conn_id='aws_default',\n",
    "   emr_conn_id='emr_default',\n",
    "   dag=dag\n",
    ")\n",
    "\n",
    "PRE_STEP = [\n",
    "    {\n",
    "        'Name': 'spark_job_01',\n",
    "        'ActionOnFailure': 'TERMINATE_CLUSTER',\n",
    "        'HadoopJarStep': {\n",
    "            'Jar': 'command-runner.jar',\n",
    "            'Args': ['spark-submit',\n",
    "                      '--deploy-mode', 'cluster',\n",
    "                      '--master', 'yarn',\n",
    "                      's3://[bucket_name]/spark_job_01.py']\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "step_adder_pre_step = EmrAddStepsOperator(\n",
    "    task_id='pre_step',\n",
    "    job_flow_id=\"{{ task_instance.xcom_pull('create_job_flow', key='return_value') }}\",\n",
    "    aws_conn_id='aws_default',\n",
    "    steps=PRE_STEP,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "step_checker_pre = EmrStepSensor(\n",
    "        task_id='watch_step_of_pre',\n",
    "        job_flow_id=\"{{ task_instance.xcom_pull('create_job_flow', key='return_value') }}\",\n",
    "        step_id=\"{{ task_instance.xcom_pull('pre_step', key='return_value')[0] }}\",\n",
    "        aws_conn_id='aws_default',\n",
    "        dag=dag\n",
    " )\n",
    "\n",
    "ACTUAL_STEP = [\n",
    "    {\n",
    "        'Name': 'spark_job_02',\n",
    "        'ActionOnFailure': 'TERMINATE_CLUSTER',\n",
    "        'HadoopJarStep': {\n",
    "            'Jar': 'command-runner.jar',\n",
    "            'Args': ['spark-submit',\n",
    "                      '--deploy-mode', 'cluster',\n",
    "                      '--master', 'yarn',\n",
    "                      's3://[bucket_name]/spark_job_02.py']\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "step_adder_actual_step = EmrAddStepsOperator(\n",
    "    task_id='actual_step',\n",
    "    job_flow_id=\"{{ task_instance.xcom_pull('create_job_flow', key='return_value') }}\",\n",
    "    aws_conn_id='aws_default',\n",
    "    steps=ACTUAL_STEP,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "step_checker_actual = EmrStepSensor(\n",
    "        task_id='watch_step_of_actual',\n",
    "        job_flow_id=\"{{ task_instance.xcom_pull('create_job_flow', key='return_value') }}\",\n",
    "        step_id=\"{{ task_instance.xcom_pull('actual_step', key='return_value')[0] }}\",\n",
    "        aws_conn_id='aws_default',\n",
    "        dag=dag\n",
    ")\n",
    "\n",
    "cluster_remover = EmrTerminateJobFlowOperator(\n",
    "    task_id='remove_cluster',\n",
    "    job_flow_id=\"{{ task_instance.xcom_pull('create_job_flow', key='return_value') }}\",\n",
    "    aws_conn_id='aws_default',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "cluster_creator >> step_adder_pre_step >> step_checker_pre >> step_adder_actual_step >> step_checker_actual >> cluster_remover\n",
    "\n",
    "# 생성한 dag 확인\n",
    "[ec2-user@ip-10-1-10-144 dags]$ airflow list_dags\n",
    "[2020-09-17 02:55:14,141] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
    "[2020-09-17 02:55:14,388] {__init__.py:305} INFO - Filling up the DagBag from /home/ec2-user/airflow/dags\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------\n",
    "DAGS\n",
    "-------------------------------------------------------------------\n",
    "emr_job_flow_test # 방금 생성한 dag\n",
    "example_bash_operator\n",
    "example_branch_dop_operator_v3\n",
    "example_branch_operator\n",
    "example_http_operator\n",
    "example_passing_params_via_test_command\n",
    "example_python_operator\n",
    "example_short_circuit_operator\n",
    "example_skip_dag\n",
    "example_subdag_operator\n",
    "example_subdag_operator.section-1\n",
    "example_subdag_operator.section-2\n",
    "example_trigger_controller_dag\n",
    "example_trigger_target_dag\n",
    "example_xcom\n",
    "latest_only\n",
    "latest_only_with_trigger\n",
    "test_utils\n",
    "tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![python_script_explain](https://user-images.githubusercontent.com/41605276/93855947-4ef2aa00-fcf3-11ea-8f45-bf5f6352152e.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 이해가 잘 안되고 어려웠던 내용 \n",
    "\n",
    "1) scheduling 적용 관련해서 이슈확인 및 해결 필요\n",
    "\n",
    "스케쥴링을 걸면 처음 시도하는 dag 구동이 두번일어남\n",
    "\n",
    "https://issues.apache.org/jira/browse/AIRFLOW-6207 참조\n",
    "\n",
    "2) scheduling 적용 시 start_time과 interval 옵션을 잘 줘야함\n",
    "\n",
    "https://stackoverflow.com/questions/46117211/apache-airflow-dag-executed-twice-before-start-date 링크 참조\n",
    "\n",
    "#### step 5) STEP 3) DAG 스케쥴링 구동 및 결과확인\n",
    "\n",
    "- airflow UI에서 상단 메뉴에서 'DAGs' 클릭\n",
    "\n",
    "\n",
    "- emr_job_flow_test 좌측에 Off 버튼을 클릭하여 On 으로 변경\n",
    "\n",
    "\n",
    "- 스케쥴링이 트리거링 되면서 아래와 같이 우리가 의도한 workflow가 구동되는 것을 확인할 수 있다.\n",
    "\n",
    "![result_check](https://user-images.githubusercontent.com/41605276/93856590-30d97980-fcf4-11ea-9c2b-e866bcb4d9f2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 dag를 실행했을때 airflow scheduler의 로그는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[2020-09-17 06:36:39,692] {jobs.py:1106} INFO - 1 tasks up for execution:\n",
    "        <TaskInstance: emr_job_flow_test.create_job_flow 2020-09-17 05:36:33.885042+00:00 [scheduled]>\n",
    "[2020-09-17 06:36:39,696] {jobs.py:1144} INFO - Figuring out tasks to run in Pool(name=None) with 128 open slots and 1 task instances in queue\n",
    "[2020-09-17 06:36:39,703] {jobs.py:1182} INFO - DAG emr_job_flow_test has 0/16 running and queued tasks\n",
    "[2020-09-17 06:36:39,704] {jobs.py:1223} INFO - Setting the follow tasks to queued state:\n",
    "        <TaskInstance: emr_job_flow_test.create_job_flow 2020-09-17 05:36:33.885042+00:00 [scheduled]>\n",
    "[2020-09-17 06:36:39,713] {jobs.py:1298} INFO - Setting the following 1 tasks to queued state:\n",
    "        <TaskInstance: emr_job_flow_test.create_job_flow 2020-09-17 05:36:33.885042+00:00 [queued]>\n",
    "[2020-09-17 06:36:39,713] {jobs.py:1334} INFO - Sending ('emr_job_flow_test', 'create_job_flow', datetime.datetime(2020, 9, 17, 5, 36, 33, 885042, tzinfo=<Timezone [UTC]>), 1) to executor with priority 6 and queue default\n",
    "[2020-09-17 06:36:39,713] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'emr_job_flow_test', 'create_job_flow', '2020-09-17T05:36:33.885042+00:00', '--local', '-sd', '/home/ec2-user/airflow/dags/test.py']\n",
    "[2020-09-17 06:36:39,715] {sequential_executor.py:45} INFO - Executing command: ['airflow', 'run', 'emr_job_flow_test', 'create_job_flow', '2020-09-17T05:36:33.885042+00:00', '--local', '-sd', '/home/ec2-user/airflow/dags/test.py']\n",
    "[2020-09-17 06:36:40,432] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
    "[2020-09-17 06:36:40,661] {__init__.py:305} INFO - Filling up the DagBag from /home/ec2-user/airflow/dags/test.py\n",
    "[2020-09-17 06:36:40,715] {cli.py:517} INFO - Running <TaskInstance: emr_job_flow_test.create_job_flow 2020-09-17T05:36:33.885042+00:00 [queued]> on host ip-10-1-10-144.ap-northeast-2.compute.internal\n",
    "[2020-09-17 06:36:45,952] {jobs.py:1468} INFO - Executor reports execution of emr_job_flow_test.create_job_flow execution_date=2020-09-17 05:36:33.885042+00:00 exited with status success for try_number 1\n",
    "[2020-09-17 06:37:25,142] {jobs.py:1106} INFO - 1 tasks up for execution:\n",
    "        <TaskInstance: emr_job_flow_test.pre_step 2020-09-17 05:36:33.885042+00:00 [scheduled]>\n",
    "[2020-09-17 06:37:25,144] {jobs.py:1144} INFO - Figuring out tasks to run in Pool(name=None) with 128 open slots and 1 task instances in queue\n",
    "[2020-09-17 06:37:25,145] {jobs.py:1182} INFO - DAG emr_job_flow_test has 0/16 running and queued tasks\n",
    "[2020-09-17 06:37:25,145] {jobs.py:1223} INFO - Setting the follow tasks to queued state:\n",
    "        <TaskInstance: emr_job_flow_test.pre_step 2020-09-17 05:36:33.885042+00:00 [scheduled]>\n",
    "[2020-09-17 06:37:25,152] {jobs.py:1298} INFO - Setting the following 1 tasks to queued state:\n",
    "        <TaskInstance: emr_job_flow_test.pre_step 2020-09-17 05:36:33.885042+00:00 [queued]>\n",
    "[2020-09-17 06:37:25,152] {jobs.py:1334} INFO - Sending ('emr_job_flow_test', 'pre_step', datetime.datetime(2020, 9, 17, 5, 36, 33, 885042, tzinfo=<Timezone [UTC]>), 1) to executor with priority 5 and queue default\n",
    "[2020-09-17 06:37:25,152] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'emr_job_flow_test', 'pre_step', '2020-09-17T05:36:33.885042+00:00', '--local', '-sd', '/home/ec2-user/airflow/dags/test.py']\n",
    "[2020-09-17 06:37:25,152] {sequential_executor.py:45} INFO - Executing command: ['airflow', 'run', 'emr_job_flow_test', 'pre_step', '2020-09-17T05:36:33.885042+00:00', '--local', '-sd', '/home/ec2-user/airflow/dags/test.py']\n",
    "[2020-09-17 06:37:25,736] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
    "[2020-09-17 06:37:25,989] {__init__.py:305} INFO - Filling up the DagBag from /home/ec2-user/airflow/dags/test.py\n",
    "[2020-09-17 06:37:26,061] {cli.py:517} INFO - Running <TaskInstance: emr_job_flow_test.pre_step 2020-09-17T05:36:33.885042+00:00 [queued]> on host ip-10-1-10-144.ap-northeast-2.compute.internal\n",
    "[2020-09-17 06:37:31,371] {jobs.py:1468} INFO - Executor reports execution of emr_job_flow_test.pre_step execution_date=2020-09-17 05:36:33.885042+00:00 exited with status success for try_number 1\n",
    "[2020-09-17 06:38:10,554] {jobs.py:1106} INFO - 1 tasks up for execution:\n",
    "        <TaskInstance: emr_job_flow_test.watch_step_of_pre 2020-09-17 05:36:33.885042+00:00 [scheduled]>\n",
    "[2020-09-17 06:38:10,557] {jobs.py:1144} INFO - Figuring out tasks to run in Pool(name=None) with 128 open slots and 1 task instances in queue\n",
    "[2020-09-17 06:38:10,558] {jobs.py:1182} INFO - DAG emr_job_flow_test has 0/16 running and queued tasks\n",
    "[2020-09-17 06:38:10,558] {jobs.py:1223} INFO - Setting the follow tasks to queued state:\n",
    "        <TaskInstance: emr_job_flow_test.watch_step_of_pre 2020-09-17 05:36:33.885042+00:00 [scheduled]>\n",
    "[2020-09-17 06:38:10,564] {jobs.py:1298} INFO - Setting the following 1 tasks to queued state:\n",
    "        <TaskInstance: emr_job_flow_test.watch_step_of_pre 2020-09-17 05:36:33.885042+00:00 [queued]>\n",
    "[2020-09-17 06:38:10,565] {jobs.py:1334} INFO - Sending ('emr_job_flow_test', 'watch_step_of_pre', datetime.datetime(2020, 9, 17, 5, 36, 33, 885042, tzinfo=<Timezone [UTC]>), 1) to executor with priority 4 and queue default\n",
    "[2020-09-17 06:38:10,565] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'emr_job_flow_test', 'watch_step_of_pre', '2020-09-17T05:36:33.885042+00:00', '--local', '-sd', '/home/ec2-user/airflow/dags/test.py']\n",
    "[2020-09-17 06:38:10,565] {sequential_executor.py:45} INFO - Executing command: ['airflow', 'run', 'emr_job_flow_test', 'watch_step_of_pre', '2020-09-17T05:36:33.885042+00:00', '--local', '-sd', '/home/ec2-user/airflow/dags/test.py']\n",
    "[2020-09-17 06:38:11,264] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
    "[2020-09-17 06:38:11,499] {__init__.py:305} INFO - Filling up the DagBag from /home/ec2-user/airflow/dags/test.py\n",
    "[2020-09-17 06:38:11,554] {cli.py:517} INFO - Running <TaskInstance: emr_job_flow_test.watch_step_of_pre 2020-09-17T05:36:33.885042+00:00 [queued]> on host ip-10-1-10-144.ap-northeast-2.compute.internal\n",
    "[2020-09-17 06:57:18,639] {jobs.py:1468} INFO - Executor reports execution of emr_job_flow_test.watch_step_of_pre execution_date=2020-09-17 05:36:33.885042+00:00 exited with status success for try_number 1\n",
    "[2020-09-17 06:57:57,837] {jobs.py:1106} INFO - 1 tasks up for execution:\n",
    "        <TaskInstance: emr_job_flow_test.actual_step 2020-09-17 05:36:33.885042+00:00 [scheduled]>\n",
    "[2020-09-17 06:57:57,839] {jobs.py:1144} INFO - Figuring out tasks to run in Pool(name=None) with 128 open slots and 1 task instances in queue\n",
    "[2020-09-17 06:57:57,840] {jobs.py:1182} INFO - DAG emr_job_flow_test has 0/16 running and queued tasks\n",
    "[2020-09-17 06:57:57,840] {jobs.py:1223} INFO - Setting the follow tasks to queued state:\n",
    "        <TaskInstance: emr_job_flow_test.actual_step 2020-09-17 05:36:33.885042+00:00 [scheduled]>\n",
    "[2020-09-17 06:57:57,848] {jobs.py:1298} INFO - Setting the following 1 tasks to queued state:\n",
    "        <TaskInstance: emr_job_flow_test.actual_step 2020-09-17 05:36:33.885042+00:00 [queued]>\n",
    "[2020-09-17 06:57:57,848] {jobs.py:1334} INFO - Sending ('emr_job_flow_test', 'actual_step', datetime.datetime(2020, 9, 17, 5, 36, 33, 885042, tzinfo=<Timezone [UTC]>), 1) to executor with priority 3 and queue default\n",
    "[2020-09-17 06:57:57,848] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'emr_job_flow_test', 'actual_step', '2020-09-17T05:36:33.885042+00:00', '--local', '-sd', '/home/ec2-user/airflow/dags/test.py']\n",
    "[2020-09-17 06:57:57,850] {sequential_executor.py:45} INFO - Executing command: ['airflow', 'run', 'emr_job_flow_test', 'actual_step', '2020-09-17T05:36:33.885042+00:00', '--local', '-sd', '/home/ec2-user/airflow/dags/test.py']\n",
    "[2020-09-17 06:57:58,821] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
    "[2020-09-17 06:57:59,057] {__init__.py:305} INFO - Filling up the DagBag from /home/ec2-user/airflow/dags/test.py\n",
    "[2020-09-17 06:57:59,112] {cli.py:517} INFO - Running <TaskInstance: emr_job_flow_test.actual_step 2020-09-17T05:36:33.885042+00:00 [queued]> on host ip-10-1-10-144.ap-northeast-2.compute.internal\n",
    "[2020-09-17 06:58:04,387] {jobs.py:1468} INFO - Executor reports execution of emr_job_flow_test.actual_step execution_date=2020-09-17 05:36:33.885042+00:00 exited with status success for try_number 1\n",
    "[2020-09-17 06:58:43,575] {jobs.py:1106} INFO - 1 tasks up for execution:\n",
    "        <TaskInstance: emr_job_flow_test.watch_step_of_actual 2020-09-17 05:36:33.885042+00:00 [scheduled]>\n",
    "[2020-09-17 06:58:43,579] {jobs.py:1144} INFO - Figuring out tasks to run in Pool(name=None) with 128 open slots and 1 task instances in queue\n",
    "[2020-09-17 06:58:43,581] {jobs.py:1182} INFO - DAG emr_job_flow_test has 0/16 running and queued tasks\n",
    "[2020-09-17 06:58:43,581] {jobs.py:1223} INFO - Setting the follow tasks to queued state:\n",
    "        <TaskInstance: emr_job_flow_test.watch_step_of_actual 2020-09-17 05:36:33.885042+00:00 [scheduled]>\n",
    "[2020-09-17 06:58:43,589] {jobs.py:1298} INFO - Setting the following 1 tasks to queued state:\n",
    "        <TaskInstance: emr_job_flow_test.watch_step_of_actual 2020-09-17 05:36:33.885042+00:00 [queued]>\n",
    "[2020-09-17 06:58:43,589] {jobs.py:1334} INFO - Sending ('emr_job_flow_test', 'watch_step_of_actual', datetime.datetime(2020, 9, 17, 5, 36, 33, 885042, tzinfo=<Timezone [UTC]>), 1) to executor with priority 2 and queue default\n",
    "[2020-09-17 06:58:43,589] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'emr_job_flow_test', 'watch_step_of_actual', '2020-09-17T05:36:33.885042+00:00', '--local', '-sd', '/home/ec2-user/airflow/dags/test.py']\n",
    "[2020-09-17 06:58:43,590] {sequential_executor.py:45} INFO - Executing command: ['airflow', 'run', 'emr_job_flow_test', 'watch_step_of_actual', '2020-09-17T05:36:33.885042+00:00', '--local', '-sd', '/home/ec2-user/airflow/dags/test.py']\n",
    "[2020-09-17 06:58:44,178] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
    "[2020-09-17 06:58:44,406] {__init__.py:305} INFO - Filling up the DagBag from /home/ec2-user/airflow/dags/test.py\n",
    "[2020-09-17 06:58:44,460] {cli.py:517} INFO - Running <TaskInstance: emr_job_flow_test.watch_step_of_actual 2020-09-17T05:36:33.885042+00:00 [queued]> on host ip-10-1-10-144.ap-northeast-2.compute.internal\n",
    "[2020-09-17 07:04:50,400] {jobs.py:1468} INFO - Executor reports execution of emr_job_flow_test.watch_step_of_actual execution_date=2020-09-17 05:36:33.885042+00:00 exited with status success for try_number 1\n",
    "[2020-09-17 07:05:29,581] {jobs.py:1106} INFO - 1 tasks up for execution:\n",
    "        <TaskInstance: emr_job_flow_test.remove_cluster 2020-09-17 05:36:33.885042+00:00 [scheduled]>\n",
    "[2020-09-17 07:05:29,583] {jobs.py:1144} INFO - Figuring out tasks to run in Pool(name=None) with 128 open slots and 1 task instances in queue\n",
    "[2020-09-17 07:05:29,585] {jobs.py:1182} INFO - DAG emr_job_flow_test has 0/16 running and queued tasks\n",
    "[2020-09-17 07:05:29,585] {jobs.py:1223} INFO - Setting the follow tasks to queued state:\n",
    "        <TaskInstance: emr_job_flow_test.remove_cluster 2020-09-17 05:36:33.885042+00:00 [scheduled]>\n",
    "[2020-09-17 07:05:29,592] {jobs.py:1298} INFO - Setting the following 1 tasks to queued state:\n",
    "        <TaskInstance: emr_job_flow_test.remove_cluster 2020-09-17 05:36:33.885042+00:00 [queued]>\n",
    "[2020-09-17 07:05:29,593] {jobs.py:1334} INFO - Sending ('emr_job_flow_test', 'remove_cluster', datetime.datetime(2020, 9, 17, 5, 36, 33, 885042, tzinfo=<Timezone [UTC]>), 1) to executor with priority 1 and queue default\n",
    "[2020-09-17 07:05:29,593] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'emr_job_flow_test', 'remove_cluster', '2020-09-17T05:36:33.885042+00:00', '--local', '-sd', '/home/ec2-user/airflow/dags/test.py']\n",
    "[2020-09-17 07:05:29,594] {sequential_executor.py:45} INFO - Executing command: ['airflow', 'run', 'emr_job_flow_test', 'remove_cluster', '2020-09-17T05:36:33.885042+00:00', '--local', '-sd', '/home/ec2-user/airflow/dags/test.py']\n",
    "[2020-09-17 07:05:30,496] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
    "[2020-09-17 07:05:30,753] {__init__.py:305} INFO - Filling up the DagBag from /home/ec2-user/airflow/dags/test.py\n",
    "[2020-09-17 07:05:30,817] {cli.py:517} INFO - Running <TaskInstance: emr_job_flow_test.remove_cluster 2020-09-17T05:36:33.885042+00:00 [queued]> on host ip-10-1-10-144.ap-northeast-2.compute.internal\n",
    "[2020-09-17 07:05:36,179] {jobs.py:1468} INFO - Executor reports execution of emr_job_flow_test.remove_cluster execution_date=2020-09-17 05:36:33.885042+00:00 exited with status success for try_number 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[참고사항]\n",
    "\n",
    "- maximize-spark-default-config.sh\n",
    "\n",
    "** 출처 :  https://github.com/aws-samples/emr-bootstrap-actions/blob/master/spark/maximize-spark-default-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "# Configures spark-default.conf for dedicate/maximum cluster use\n",
    "#   Set num executors to number to total instance count at time of creation (spark.executor.instances)\n",
    "#   Set vcores per executor to be all the vcores for the instance type of the core nodes (spark.executor.cores)\n",
    "#   Set the memory per executor to the max available for the node (spark.executor.memory)\n",
    "#   Set the default parallelism to the total number of cores available across all nodes at time of cluster creation  (spark.default.parallelism)\n",
    "#\n",
    "# Limitations:\n",
    "#   Assumes a homogenous cluster (all core and task instance groups the same instance type)\n",
    "#   Is not dynamic with cluster resices\n",
    "#\n",
    "set -x\n",
    "#\n",
    "#determine the current region and place into REGION\n",
    "EC2AZ=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)\n",
    "REGION=\"`echo \\\"$EC2AZ\\\" | sed -e 's:\\([0-9][0-9]*\\)[a-z]*\\$:\\\\1:'`\"\n",
    "\n",
    "\n",
    "if [ \"$SparkS3SupportingFilesPath\" == \"\" ]\n",
    "then\n",
    "       if [ \"$REGION\" == \"eu-central-1\" ]\n",
    "       then\n",
    "               SparkS3SupportingFilesPath=s3://eu-central-1.support.elasticmapreduce/spark\n",
    "       else\n",
    "               SparkS3SupportingFilesPath=s3://support.elasticmapreduce/spark\n",
    "       fi\n",
    "fi\n",
    "SparkS3SupportingFilesPath=${SparkS3SupportingFilesPath%/}\n",
    "\n",
    "VCOREREFERENCE=\"$SparkS3SupportingFilesPath/vcorereference.tsv\"\n",
    "CONFIGURESPARK=\"$SparkS3SupportingFilesPath/configure-spark.bash\"\n",
    "#\n",
    "echo \"Configuring Spark default configuration to the max memory and vcore setting given configured number of cores nodes at cluster creation\"\n",
    "\n",
    "#Set the default yarn min allocation to 256 to allow for most optimum memory use\n",
    "/usr/share/aws/emr/scripts/configure-hadoop -y yarn.scheduler.minimum-allocation-mb=256\n",
    "\n",
    "#Gather core node count\n",
    "NUM_NODES=$(grep /mnt/var/lib/info/job-flow.json -e \"instanceCount\" | sed 's/.*instanceCount.*:.\\([0-9]*\\).*/\\1/g')\n",
    "NUM_NODES=$(expr $NUM_NODES - 1)\n",
    "\n",
    "if [ $NUM_NODES -lt 2 ]\n",
    "then\n",
    "    #set back to default to be safe\n",
    "    NUM_NODES=2\n",
    "fi\n",
    "\n",
    "SLAVE_INSTANCE_TYPE=$(grep /mnt/var/lib/info/job-flow.json -e \"slaveInstanceType\" | cut -d'\"' -f4 | sed  's/\\s\\+//g')\n",
    "\n",
    "if [ \"$SLAVE_INSTANCE_TYPE\" == \"\" ]\n",
    "then\n",
    "    SLAVE_INSTANCE_TYPE=\"m3.xlarge\"\n",
    "fi\n",
    "\n",
    "hadoop fs -get $VCOREREFERENCE\n",
    "\n",
    "if [ ! -e \"vcorereference.tsv\" ]\n",
    "then\n",
    "    echo \"Reference file vcorereference.tsv not available, failing quietly.\"\n",
    "    exit 0\n",
    "fi\n",
    "\n",
    "NUM_VCORES=$(grep vcorereference.tsv -e $SLAVE_INSTANCE_TYPE | cut -f2)\n",
    "\n",
    "MAX_YARN_MEMORY=$(grep /home/hadoop/conf/yarn-site.xml -e \"yarn\\.scheduler\\.maximum-allocation-mb\" | sed 's/.*<value>\\(.*\\).*<\\/value>.*/\\1/g')\n",
    "\n",
    "EXEC_MEMORY=$(echo \"($MAX_YARN_MEMORY - 1024 - 384) - ($MAX_YARN_MEMORY - 1024 - 384) * 0.07 \" | bc | cut -d'.' -f1)\n",
    "EXEC_MEMORY+=\"M\"\n",
    "\n",
    "PARALLEL=$(expr $NUM_VCORES \\* $NUM_NODES) \n",
    "\n",
    "#--- Now use configure-spark.bash to set values\n",
    "\n",
    "hadoop fs -get $CONFIGURESPARK\n",
    "\n",
    "bash configure-spark.bash spark.executor.instances=$NUM_NODES spark.executor.cores=$NUM_VCORES spark.executor.memory=$EXEC_MEMORY \n",
    "\n",
    "if [ $PARALLEL -gt 2 ]\n",
    "then\n",
    "    #only set/change this if it looks reasonable\n",
    "    bash configure-spark.bash spark.default.parallelism=$PARALLEL\n",
    "fi\n",
    "\n",
    "\n",
    "exit 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spark_job_01.py (#01 spark job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ABC data test\").getOrCreate()\n",
    "\n",
    "_list = ['A','B','C']\n",
    "\n",
    "for elem in _list:\n",
    "    \n",
    "    df = spark.read.option(\"header\",\"true\").csv(\"s3a://pms-bucket-test/testdata/10G.csv\")\n",
    "    \n",
    "    # 기존의 컬럼에 A 파일의 컬럼인지,B 인지,C 인지 구분자를 붙여주는 전처리과정\n",
    "    df = df.withColumnRenamed(\"event_time\",\"event_time_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"event_type\",\"event_type_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"product_id\",\"product_id_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"category_id\",\"category_id_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"category_code\",\"category_code_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"brand\",\"brand_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"price\",\"price_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"user_id\",\"user_id_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"year\",\"year_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"user_session\",\"user_session_{}\".format(elem))\\\n",
    "           .withColumn(\"id\", F.monotonically_increasing_id()) ## id라는 레코드별 고유넘버 컬럼을 추가해서 id를 기준으로 join할 예정\n",
    "    \n",
    "    # 1개의 csv 파일형태로 전처리한 데이터를 s3에 저장\n",
    "    df.write.option(\"header\", \"true\").csv(\"s3a://pms-bucket-test/{}\".format(elem))\n",
    "    \n",
    "    \n",
    "df.show()\n",
    "\n",
    "print(\"record count : \", df.count())\n",
    "\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spark_job_02.py (#02 spark job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "import pyspark\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# 코드 스타트 시간 체크\n",
    "start = time.time()\n",
    "\n",
    "# 스파크 세션을 생성하여 데이터 프레임을 구동할 수 있도록 정의\n",
    "spark = SparkSession.builder.appName(\"minmantest\").getOrCreate()\n",
    "\n",
    "# A 데이터 로딩, 임시테이블 생성\n",
    "A_df = spark.read.option(\"header\",\"true\").csv(\"s3a://pms-bucket-test/A/*.csv\")\n",
    "A_df.createOrReplaceTempView(\"Atable\")\n",
    "\n",
    "# B 데이터 로딩, 임시테이블 생성\n",
    "B_df = spark.read.option(\"header\",\"true\").csv(\"s3a://pms-bucket-test/B/*.csv\")\n",
    "B_df.createOrReplaceTempView(\"Btable\")\n",
    "\n",
    "# C 데이터 로딩, 임시테이블 생성\n",
    "C_df = spark.read.option(\"header\",\"true\").csv(\"s3a://pms-bucket-test/C/*.csv\")\n",
    "C_df.createOrReplaceTempView(\"Ctable\")\n",
    "\n",
    "# A,B 파일을 join하여 AB라는 데이터 프레임을 생성 후 DISK에 중간저장, AB 임시테이블 생성\n",
    "AB_df = spark.sql(\"SELECT * FROM Atable JOIN Btable USING(id)\").persist(StorageLevel.DISK_ONLY)\n",
    "AB_df.createOrReplaceTempView(\"ABtable\")\n",
    "\n",
    "# AB 데이터 프레임에 C파일 join하여 ABC라는 데이터 프레임 생성 후 DISK에 중간저장, AB 임시테이블 생성\n",
    "ABC_df = spark.sql(\"SELECT * FROM ABtable JOIN Ctable USING(id)\").persist(StorageLevel.DISK_ONLY)\n",
    "ABC_df.createOrReplaceTempView(\"ABCtable\")\n",
    "\n",
    "# ABC 데이터 프레임 레코드 카운트\n",
    "spark.sql(\"SELECT COUNT(*) FROM ABCtable\").show()\n",
    "\n",
    "# 코드 종료시간 체크\n",
    "endTimeQuery = time.clock()\n",
    "\n",
    "# 위의 전체 코드에 대한 소요시간\n",
    "print(\" code running time (sec) : \", time.time() - start,', code finish time (GMT) : ',datetime.now().strftime('%Y-%m-%d %H:%M'))\n",
    "\n",
    "# 데이터 프레임 캐시제거\n",
    "spark.catalog.clearCache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
