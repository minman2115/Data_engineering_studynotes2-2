{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "Data_Engineering_TIL(20201206)\n",
    "\n",
    "study program : T아카데미 - 아파치 스파크 입문과 활용\n",
    "\n",
    "** URL : https://tacademy.skplanet.com/frontMain.action\n",
    "\n",
    "### [학습내용]\n",
    "\n",
    "step 1) 실습환경 구성\n",
    "\n",
    "\n",
    "spark을 띄우기 위해서 spark cluster를 구성해야하는데 시간상 제플린 환경으로 실습한다. 제플린에서 spark context를 띄워서 실습해볼 수 있다.\n",
    "\n",
    "참고로 spark을 설치한다는 것은 spark만 설치하면 끝나는게 아니라 yarn과 Hadoop을 설치를 해주고 연동해줘야 한다.\n",
    "\n",
    "\n",
    "아래와 같은 spec으로 aws ec2 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws ec2 run-instances --image-id ami-03b42693dc6a7dc35 --count 1 --instance-type t3.medium --key-name pms-seoul-key --security-group-ids sg-xxxxxxxxxxxx --subnet-id subnet-xxxxxxxxxxxxx --associate-public-ip-address --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=pms-spark-test},{Key=owner,Value=pms},{Key=expiry-date,Value=2020-12-04}]' --block-device-mappings 'DeviceName=/dev/xvda,Ebs={VolumeSize=30,DeleteOnTermination=true}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런 다음에 해당 ec2로 ssh 접속해서 아래와 같은 명령어 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-0-1-170 ~]$ sudo yum update -y\n",
    "\n",
    "[ec2-user@ip-10-0-1-170 ~]$ sudo amazon-linux-extras install docker -y\n",
    "\n",
    "[ec2-user@ip-10-0-1-170 ~]$ sudo service docker start\n",
    "Redirecting to /bin/systemctl start docker.service\n",
    "\n",
    "[ec2-user@ip-10-0-1-170 ~]$ sudo usermod -a -G docker ec2-user\n",
    "\n",
    "[ec2-user@ip-10-0-1-170 ~]$ exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "터미널 로그아웃 후 다시 ec2로 재접속하고 아래와 같이 명령어를 실행했을때 sudo 명령어를 안줘도 되는지 확인해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-0-1-170 ~]$ docker info\n",
    "Client:\n",
    " Debug Mode: false\n",
    "\n",
    "Server:\n",
    " Containers: 0\n",
    "  Running: 0\n",
    "  Paused: 0\n",
    "  Stopped: 0\n",
    " Images: 0\n",
    " Server Version: 19.03.13-ce\n",
    " Storage Driver: overlay2\n",
    "  Backing Filesystem: xfs\n",
    "  Supports d_type: true\n",
    "  Native Overlay Diff: true\n",
    " Logging Driver: json-file\n",
    " Cgroup Driver: cgroupfs\n",
    " Plugins:\n",
    "  Volume: local\n",
    "  Network: bridge host ipvlan macvlan null overlay\n",
    "  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\n",
    " Swarm: inactive\n",
    " Runtimes: runc\n",
    " Default Runtime: runc\n",
    " Init Binary: docker-init\n",
    " containerd version: c623d1b36f09f8ef6536a057bd658b3aa8632828\n",
    " runc version: ff819c7e9184c13b7c2607fe6c30ae19403a7aff\n",
    " init version: de40ad0 (expected: fec3683)\n",
    " Security Options:\n",
    "  seccomp\n",
    "   Profile: default\n",
    " Kernel Version: 4.14.193-149.317.amzn2.x86_64\n",
    " Operating System: Amazon Linux 2\n",
    " OSType: linux\n",
    " Architecture: x86_64\n",
    " CPUs: 2\n",
    " Total Memory: 3.794GiB\n",
    " Name: ip-10-0-1-170.ap-northeast-2.compute.internal\n",
    " ID: SPCF:Y75R:B22D:BLRW:ELZ5:3O4B:HPDJ:R3JI:DSQM:W74R:TRA2:BXWE\n",
    " Docker Root Dir: /var/lib/docker\n",
    " Debug Mode: false\n",
    " Registry: https://index.docker.io/v1/\n",
    " Labels:\n",
    " Experimental: false\n",
    " Insecure Registries:\n",
    "  127.0.0.0/8\n",
    " Live Restore Enabled: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같은 docker run 명령어를 실행해서 제플린 개발환경을 구성한다.\n",
    "\n",
    "`docker run -p 4040:4040 -p 8080:8080 --privileged=true -v $PWD/logs:/logs -v $PWD/notebook:/notebook -e ZEPPELIN_NOTEBOOK_DIR='/notebook' -e ZEPPELIN_LOG_DIR='/logs' -d apache/zeppelin:0.8.1 /zeppelin/bin/zeppelin.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-0-1-170 ~]$ docker run -p 4040:4040 -p 8080:8080 --privileged=true -v $PWD/logs:/logs -v $PWD/notebook:/notebook \\\n",
    "> -e ZEPPELIN_NOTEBOOK_DIR='/notebook' \\\n",
    "> -e ZEPPELIN_LOG_DIR='/logs' \\\n",
    "> -d apache/zeppelin:0.8.1 \\\n",
    "> /zeppelin/bin/zeppelin.sh\n",
    "Unable to find image 'apache/zeppelin:0.8.1' locally\n",
    "0.8.1: Pulling from apache/zeppelin\n",
    "7b722c1070cd: Pull complete\n",
    "5fbf74db61f1: Pull complete\n",
    "ed41cb72e5c9: Pull complete\n",
    "7ea47a67709e: Pull complete\n",
    "7ba34fd9f5e0: Pull complete\n",
    "8f2f09b83582: Pull complete\n",
    "40260f0a8f69: Pull complete\n",
    "48946af5572c: Pull complete\n",
    "8b38acee7e8d: Pull complete\n",
    "a806f41d7e41: Pull complete\n",
    "7dcaf396dead: Pull complete\n",
    "8db355f40e66: Pull complete\n",
    "Digest: sha256:a3a90ec1579f5171ebac565e739547b885ed75efc1ec7581128ec1033a4496cb\n",
    "Status: Downloaded newer image for apache/zeppelin:0.8.1\n",
    "9276b4344e403414c354ec41816abcd6005e637836d31f691319323b672fe279\n",
    "\n",
    "[ec2-user@ip-10-0-1-170 ~]$ docker ps\n",
    "CONTAINER ID        IMAGE                   COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n",
    "9276b4344e40        apache/zeppelin:0.8.1   \"/usr/bin/tini -- /z…\"   7 seconds ago       Up 5 seconds        0.0.0.0:4040->4040/tcp, 0.0.0.0:8080->8080/tcp   confident_swirles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 정상적으로 도커를 실행하고 나면 웹브라우저를 열고 `[ec2 public ip]:8080` 으로 접속해서 제플린으로 정상 접근가능한지 확인해본다.\n",
    "\n",
    "접속한 다음에 `Create new note`을 클릭해서 spark 노트북을 새로 만들어준다. 그런 다음에 아래 그림과 같이 첫번째 블럭에 spark 문자열을 입력후 실행해본다. 실행하면 sparksession이 구동되는 것을 확인할 수 있다.\n",
    "\n",
    "![1](https://user-images.githubusercontent.com/41605276/101123798-f3b92e00-3638-11eb-8be0-71e23ed84eae.PNG)\n",
    "\n",
    "그런 다음에 웹브라우저를 하나 새로열고 `[ec2 public ip]:4040`를 입력해서 아래 그림과 같이 spark ui에 정상접속하는지 확인해본다.\n",
    "\n",
    "\n",
    "![2](https://user-images.githubusercontent.com/41605276/101123952-4e528a00-3639-11eb-9a9e-889d83083460.PNG)\n",
    "\n",
    "생성한 제플린 노트북에서 아래와 같은 코드로 실습을 해본다.\n",
    "\n",
    "'티아카데미 아파치 스파크 입문과 활용 - 데이터 처리 실습 제플린 파일' 폴더에서 'test.json' 참고할것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark\n",
    "//sparksession driver process,어플리케이션 기준 \n",
    "spark\n",
    "\n",
    "\n",
    "%spark\n",
    "//한개의 컬럼과 1000개의 row(0~999)를 생성한 데이터 프레임을 만든다\n",
    "val myRange = spark.range(1000).toDF(\"number\")\n",
    "\n",
    "\n",
    "%spark\n",
    "//transformation - narrow dependency\n",
    "val divisBy2 = myRange.where(\"number % 2 = 0\")\n",
    "\n",
    "%spark\n",
    "//action\n",
    "divisBy2.count()\n",
    "\n",
    "\n",
    "%sh\n",
    "//데이터 다운로드\n",
    "wget https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/flight-data/csv/2015-summary.csv\n",
    "\n",
    "\n",
    "%sh\n",
    "//데이터 확인\n",
    "head /zeppelin/2015-summary.csv\n",
    "\n",
    "\n",
    "%spark\n",
    "//CSV데이터 데이터 프레임으로 바로 읽기, visit to http://localhost:4040\n",
    "val flightData2015 = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"/zeppelin/2015-summary.csv\")\n",
    "\n",
    "\n",
    "%spark\n",
    "//3건만 가져오기\n",
    "//take같이 action이 발생할때 spark 대시보드에 로그가 남는다.\n",
    "flightData2015.take(3)\n",
    "\n",
    "\n",
    "%spark\n",
    "//실행계획 확인하기\n",
    "flightData2015.sort(\"count\").explain()\n",
    "\n",
    "\n",
    "%spark\n",
    "//count필드로 정렬해서 2개 가져오기\n",
    "flightData2015.sort(\"count\").take(2)\n",
    "\n",
    "\n",
    "%spark\n",
    "//SQL을 사용하기 위해 임시 view만들기\n",
    "// tempview나 temptable은 sparkcontext안에서만 유효하기 때문에 sparkcontext가 종료되면 이것들도 같이 날아간다.\n",
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")\n",
    "\n",
    "\n",
    "%spark\n",
    "//SQL로DEST_COUNTRY_NAME 기준으로 몇건인지 확인하는 spark sql실행하기\n",
    "val sqlWay = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, count(1)\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "%spark\n",
    "//데이터 프레임 통계 보기\n",
    "sqlWay.describe().show()\n",
    "\n",
    "\n",
    "\n",
    "%spark\n",
    "//데이터 프레임으로 실행계획 확인하기\n",
    "val dataFrameWay = flightData2015\n",
    "  .groupBy('DEST_COUNTRY_NAME)\n",
    "  .count.explain\n",
    "\n",
    "\n",
    "%spark\n",
    "//sql과 dataframe실행계획 비교하기           \n",
    "sqlWay.explain\n",
    "\n",
    "\n",
    "%spark\n",
    "//sql질의로 max값 가져와서 take로 한건만 확인하기           \n",
    "spark.sql(\"SELECT max(count) from flight_data_2015\").take(1)\n",
    "\n",
    "\n",
    "%spark\n",
    "//데이터 프레임에서 max함수 이용하여 count의 최대값 가져오기           \n",
    "import org.apache.spark.sql.functions.max\n",
    "\n",
    "flightData2015.select(max(\"count\")).take(1)\n",
    "\n",
    "\n",
    "%spark\n",
    "//spark sql로 DEST_COUNTRY_NAME 을 집계연산을 수행하여 합계를 구하여 큰 순서대로 5건 확인하기           \n",
    "val maxSql = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "%spark\n",
    "//sql결과 확인하기           \n",
    "maxSql.explain\n",
    "maxSql.show()\n",
    "\n",
    "\n",
    "\n",
    "%spark\n",
    "//동일한 쿼리를 dataframe으로 구현해 봅니다.  End-to-End example\n",
    "flightData2015\n",
    ".groupBy(\"DEST_COUNTRY_NAME\")\n",
    ".sum(\"count\")\n",
    ".withColumnRenamed(\"sum(count)\", \"destination_total\")\n",
    ".sort(desc(\"destination_total\"))\n",
    ".limit(5)\n",
    ".explain\n",
    "\n",
    "\n",
    "%spark\n",
    "flightData2015\n",
    ".groupBy(\"DEST_COUNTRY_NAME\")\n",
    ".sum(\"count\")\n",
    ".withColumnRenamed(\"sum(count)\", \"destination_total\")\n",
    ".sort(desc(\"destination_total\"))\n",
    ".limit(5)\n",
    ".show()\n",
    "\n",
    "\n",
    "%spark\n",
    "//임의의 값을 가지고 데이터 프레임 생성하기\n",
    "val data = Seq((\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"),\n",
    "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"),\n",
    "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"),\n",
    "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\"))\n",
    "\n",
    "import spark.sqlContext.implicits._\n",
    "val df = data.toDF(\"Product\",\"Amount\",\"Country\")\n",
    "df.show()\n",
    "\n",
    "%spark\n",
    "//각 제품의 각 국가로 수출 된 총 금액을 얻으려면 제품 별 그룹화, 국가 별 피봇 팅 및 금액 합계를 얻어온다\n",
    "val pivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
    "pivotDF.show()\n",
    "\n",
    "%spark\n",
    "//spark2.0에서 성능향상을 위해 pivot대상열을 선언함 \n",
    "val countries = Seq(\"USA\",\"China\",\"Canada\",\"Mexico\")\n",
    "val pivotDF = df.groupBy(\"Product\").pivot(\"Country\", countries).sum(\"Amount\")\n",
    "pivotDF.show()\n",
    "\n",
    "%spark\n",
    "//성능 향상을 위해 두단계 집계를 사용함 \n",
    "val pivotDF = df.groupBy(\"Product\",\"Country\")\n",
    "      .sum(\"Amount\")\n",
    "      .groupBy(\"Product\")\n",
    "      .pivot(\"Country\")\n",
    "      .sum(\"sum(Amount)\")\n",
    "pivotDF.show()\n",
    "           \n",
    "%spark\n",
    "//stack기능을 이용하여 주요 국가에 대해 unpivot을 수행\n",
    "val unPivotDF = pivotDF.select($\"Product\",\n",
    "expr(\"stack(4, 'Canada', Canada, 'China', China, 'Mexico', Mexico, 'USA', USA) as (Country,Total)\"))\n",
    ".where(\"Total is not null\")\n",
    "unPivotDF.show()\n",
    "           \n",
    "  \n",
    "//%sql로 하면 sparksql 커널로 잡히게 된다.\n",
    "%sql\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참고사항 : sparksession과 sparkcontext\n",
    "\n",
    "1) 둘의 차이점 결론\n",
    "\n",
    "spark.sparkContext = RDD 생성시 사용하는 API 객체로 application 전체의 실행 관련 정보 집약 객체로 스케줄러 등이 포함된 개념이다.\n",
    "\n",
    "spark.sparkSession = DataFrame 생성시 사용하는 API 객체로 sparkcontext에 세션정보가 추가로 포함된 개념이다.\n",
    "\n",
    "RDD를 생성하려면 SparkContext 객체를 생성하고, DataFrame 또는 DataSet을 생성하려면 SparkSession 객체를 생성해야 한다. 그런데 sparksession 객체 안에는 sparkcontext 객체가 포함돼 있기 때문에 RDD를 만들때나 데이터프레임을 만들때나 상관없이 sparksession 객체를 생성하면 된다.\n",
    "\n",
    "2) SparkSession\n",
    "\n",
    "Spark2.0 이후부터는 SparkSession을 사용해서 Spark Dataset, DataFrame API를 사용할 수 있다. 또한 SparkContext에서 사용 가능한 모든 기능은 SparkSession에서도 사용이 가능하다.\n",
    "\n",
    "\n",
    "3) sparkcontext\n",
    "\n",
    "SparkSession를 이용해서 SparkContext()를 사용하고 싶다면 SparkSession에서 sparkContext() 메서드를 사용하면 SparkContext를 사용할 수 있다.SparkConext는 Spark 클러스터에 대한 연결을 나타내며 해당 클러스터에서 RDD, broadcast, accumulator 등의 변수를 사용하여 사용이 가능하다. JVM 당 하나의 SparkContext만 활성화 할 수 있다.\n",
    "\n",
    "- RDD란\n",
    "\n",
    "1) RDDs (Resilient Distributed Datasets) is Data Containers\n",
    "\n",
    "\n",
    "2) All the different processing components in Spark share the same abstraction called RDD\n",
    "\n",
    "\n",
    "spark의 모든 프로세싱은 RDD 기반으로 모든 데이터를 share한다고 이해하면 된다.\n",
    "\n",
    "\n",
    "3) As applications share the RDD abstraction, you can mix different kind of transformations to create new RDDs\n",
    "\n",
    "\n",
    "4) Created by parallelizing a collection or reading a file\n",
    "\n",
    "\n",
    "5) Fault tolerant\n",
    "\n",
    "\n",
    "- RDD 개념이해를 위한 예시 : log mining\n",
    "\n",
    "HDFS에 로그가 쌓여있다고 가정하자. 데이터를 line by line으로 읽는데 'ERROR'가 포함된 문자열을 detect하고 싶은것이다.\n",
    "\n",
    "'ERROR'가 포함된 RDD를 따로 만들고, 거기에서 map작업을 하는데 '\\t' split했을때 두번째 문자열을 message 변수로 가져오고 그거를 캐싱을 해서 거기에서 foo가 몇개이고 bar가 몇개인지 filter해서 count한다.\n",
    "\n",
    "RDD가 변환시킬때마다 transformation이 일어난다. 변환될때마다 transformed RDD가 계속 생성된다. 그리고 그거를 count를 해서 action을 하게되면 driver로 이벤트가 날아가게 된다.\n",
    "\n",
    "![3](https://user-images.githubusercontent.com/41605276/101245378-cd39e680-374f-11eb-8f5e-077fb7ceef86.png)\n",
    "\n",
    "위에 그림에서 driver는 자바에서 메인함수를 가지는 어플리케이션이라고 할 수 있다. driver에 code가 submit이 되면 클러스터안에 워커한테 task들이 분배가 될것이다. 그리고 cache를 위에서 사용했는데 worker의 메모리 영역에서 일부영역을 캐시영역으로 활용하게 된다. 그리고 spark의 task들이 완료되면 결과값을 driver로 던져주게 된다.\n",
    "\n",
    "\n",
    "- RDD fault tolerance가 뭐냐\n",
    "\n",
    "HDFS는 매번 읽고 쓰고를 하는데 읽고 쓰고 하는 만약에 iteration한 job이 있는데 이게 중간에 갑자기 문제가 생겨서 다시 이 job을 재개한다면 mapreduce는 다시 디스크를 읽어서 중간산출물을 다시 읽게 된다. 그런다음에 다시 computing을 하게 된다. 그러면 spark 같이 인메모리 구조에서는 이런 recomputing을 어떻게 하냐. RDD는 리니지 트레킹이라는 개념이 있다. 아래 그림과 같이 데이터가 변형이 가해질때마다 RDD가 새로 생성이 된다. 그래서 연산중간에 문제가 생기면 바로이전 단계 RDD를 트레킹해서 리트라이 할 수 있도록 한다.\n",
    "\n",
    "\"RDDs track the transformations used to build them (their lineage) to recompute lost data\"\n",
    "\n",
    "![4](https://user-images.githubusercontent.com/41605276/101246141-c2358500-3754-11eb-93ba-1c2601812b08.PNG)\n",
    "\n",
    "\n",
    "- scala vs java\n",
    "\n",
    "spark은 jvm 기반이기 때문에 자바 스칼라 모두 가능하고, 스칼라 언어의 간결성과 함수형 프로그래밍의 장점을 살리고자 주로 스칼라를 spark에서는 사용한다. spark-submit하고 spark job의 프로세싱은 jvm 위에서 모두 실행이 된다.\n",
    "\n",
    "![5](https://user-images.githubusercontent.com/41605276/101246292-beeec900-3755-11eb-89ac-9f12896b4311.PNG)\n",
    "\n",
    "\n",
    "- 참고로 spark 데이터프레임만 사용한다면 java, scala, python 언어의 성능차이는 최근에는 거의 없다고 할 수 있다.\n",
    "\n",
    "\n",
    "- scala cheat sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//variables\n",
    "var x: Int = 7\n",
    "    \n",
    "var x = 7 \n",
    "// type inferred\n",
    "\n",
    "val y = \"hi\"\n",
    "// read-only\n",
    "\n",
    "// Functions\n",
    "def square(x: Int): Int = x*x\n",
    "def square(x: Int): Int = {\n",
    "    x*x // last line returned\n",
    "}\n",
    "    \n",
    "// Collections and closures\n",
    "val nums = Array(1, 2, 3)\n",
    "nums.map((x: Int) => x + 2) \n",
    "// => Array(3, 4,5)\n",
    "\n",
    "nums.map(x => x + 2) \n",
    "// => same\n",
    "\n",
    "nums.map(_ + 2) \n",
    "// => same\n",
    "\n",
    "nums.reduce((x, y) => x + y) \n",
    "// => 6\n",
    "\n",
    "nums.reduce(_ + _) \n",
    "// => 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RDD operations\n",
    "\n",
    "spark은 최적의 DAG를 action 시점에서 찾기 위해서 lazy operation을 한다.      ** DAG : 특정 job이 돌기위해서 실행되는 프로세스\n",
    "\n",
    "사용자가 이것저것 transformation을 하더라도 실제로 action단계에서 transformation의 실행계획이 최적화 되는것이다.\n",
    "\n",
    "1) Transformations (e.g. map, filter, groupBy,join)\n",
    "\n",
    "Lazy operations to build RDDs from other RDDs\n",
    "\n",
    "2) Actions (e.g. count, collect, save)\n",
    "\n",
    "Return a result or write it to storage\n",
    "\n",
    "![6](https://user-images.githubusercontent.com/41605276/101247531-7affc200-375d-11eb-84d8-3db3cdcfd382.PNG)\n",
    "\n",
    "action이 수행되고 결과값은 driver로 return하거나 특정 스토리지에 저장할 수 있다.\n",
    "\n",
    "\n",
    "- RDD와 Dataframe\n",
    "\n",
    "RDDs provide a low level interface into Spark\n",
    "\n",
    "DataFrames have a schema\n",
    "\n",
    "DataFrames are cached and optimized by Spark\n",
    "\n",
    "DataFrames are built on top of the RDDs and the core Spark API\n",
    "\n",
    "![7](https://user-images.githubusercontent.com/41605276/101247772-b484fd00-375e-11eb-814d-a6d29aa1c1fe.PNG)\n",
    "\n",
    "- spark Dataframe 샘플코드\n",
    "\n",
    "코드가 상당히 직관적이고 쉽다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create a new DataFrame that contains “students”\n",
    "students = users.filter(users.age < 21)\n",
    "\n",
    "//Alternatively, using Pandas-like syntax\n",
    "students = users[users.age < 21]\n",
    "\n",
    "//Count the number of students users by gender\n",
    "students.groupBy(\"gender\").count()\n",
    "\n",
    "// Join young students with another DataFrame called logs\n",
    "students.join(logs, logs.userId == users.userId,\"left_outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD와 Dataframe 샘플코드 비교\n",
    "\n",
    "RDD같은 경우에는 \\t으로 split을 한다음에 몇번째 array의 무슨값인지 다 지정을 해줘야 하는데 Dataframe은 스키마를 갖고 있기 때문에 Dataframe은 그런식으로 지정을 해줄 필요가 없다.\n",
    "\n",
    "1) RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile(...).split(\"\\t\")\n",
    "data.map(lambda x: (x[0], [int(x[1]), 1]))\n",
    ".reduceByKey(lambda x, y: [x[0] + y[0], x[1] + y[1]])\n",
    ".map(lambda x: [x[0], x[1][0] / x[1][1]])\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlCtx.table(\"people\").groupBy(\"name\").agg(\"name\",avg(\"age\")).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
