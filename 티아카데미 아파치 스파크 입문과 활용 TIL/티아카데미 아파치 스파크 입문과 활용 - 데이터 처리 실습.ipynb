{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "Data_Engineering_TIL(20201204)\n",
    "\n",
    "study program : T아카데미 - 아파치 스파크 입문과 활용\n",
    "\n",
    "** URL : https://tacademy.skplanet.com/frontMain.action\n",
    "\n",
    "### [학습내용]\n",
    "\n",
    "step 1) 실습환경 구성\n",
    "\n",
    "\n",
    "spark을 띄우기 위해서 spark cluster를 구성해야하는데 시간상 제플린 환경으로 실습한다. 제플린에서 spark context를 띄워서 실습해볼 수 있다.\n",
    "\n",
    "참고로 spark을 설치한다는 것은 spark만 설치하면 끝나는게 아니라 yarn과 Hadoop을 설치를 해주고 연동해줘야 한다.\n",
    "\n",
    "\n",
    "아래와 같은 spec으로 aws ec2 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws ec2 run-instances --image-id ami-03b42693dc6a7dc35 --count 1 --instance-type t3.medium --key-name pms-seoul-key --security-group-ids sg-xxxxxxxxxxxx --subnet-id subnet-xxxxxxxxxxxxx --associate-public-ip-address --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=pms-spark-test},{Key=owner,Value=pms},{Key=expiry-date,Value=2020-12-04}]' --block-device-mappings 'DeviceName=/dev/xvda,Ebs={VolumeSize=30,DeleteOnTermination=true}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런 다음에 해당 ec2로 ssh 접속해서 아래와 같은 명령어 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-0-1-170 ~]$ sudo yum update -y\n",
    "\n",
    "[ec2-user@ip-10-0-1-170 ~]$ sudo amazon-linux-extras install docker -y\n",
    "Installing docker\n",
    "Loaded plugins: extras_suggestions, langpacks, priorities, update-motd\n",
    "Cleaning repos: amzn2-core amzn2extra-docker\n",
    "12 metadata files removed\n",
    "4 sqlite files removed\n",
    "0 metadata files removed\n",
    "Loaded plugins: extras_suggestions, langpacks, priorities, update-motd\n",
    "amzn2-core                                                                                    | 3.7 kB  00:00:00\n",
    "amzn2extra-docker                                                                             | 3.0 kB  00:00:00\n",
    "(1/5): amzn2-core/2/x86_64/group_gz                                                           | 2.5 kB  00:00:00\n",
    "(2/5): amzn2-core/2/x86_64/updateinfo                                                         | 309 kB  00:00:00\n",
    "(3/5): amzn2extra-docker/2/x86_64/updateinfo                                                  |   76 B  00:00:00\n",
    "(4/5): amzn2extra-docker/2/x86_64/primary_db                                                  |  74 kB  00:00:00\n",
    "(5/5): amzn2-core/2/x86_64/primary_db                                                         |  47 MB  00:00:00\n",
    "Resolving Dependencies\n",
    "--> Running transaction check\n",
    "---> Package docker.x86_64 0:19.03.13ce-1.amzn2 will be installed\n",
    "--> Processing Dependency: runc >= 1.0.0 for package: docker-19.03.13ce-1.amzn2.x86_64\n",
    "--> Processing Dependency: containerd >= 1.3.2 for package: docker-19.03.13ce-1.amzn2.x86_64\n",
    "--> Processing Dependency: pigz for package: docker-19.03.13ce-1.amzn2.x86_64\n",
    "--> Processing Dependency: libcgroup for package: docker-19.03.13ce-1.amzn2.x86_64\n",
    "--> Running transaction check\n",
    "---> Package containerd.x86_64 0:1.4.1-2.amzn2 will be installed\n",
    "---> Package libcgroup.x86_64 0:0.41-21.amzn2 will be installed\n",
    "---> Package pigz.x86_64 0:2.3.4-1.amzn2.0.1 will be installed\n",
    "---> Package runc.x86_64 0:1.0.0-0.1.20200826.gitff819c7.amzn2 will be installed\n",
    "--> Finished Dependency Resolution\n",
    "\n",
    "Dependencies Resolved\n",
    "\n",
    "=====================================================================================================================\n",
    " Package             Arch            Version                                        Repository                  Size\n",
    "=====================================================================================================================\n",
    "Installing:\n",
    " docker              x86_64          19.03.13ce-1.amzn2                             amzn2extra-docker           37 M\n",
    "Installing for dependencies:\n",
    " containerd          x86_64          1.4.1-2.amzn2                                  amzn2extra-docker           24 M\n",
    " libcgroup           x86_64          0.41-21.amzn2                                  amzn2-core                  66 k\n",
    " pigz                x86_64          2.3.4-1.amzn2.0.1                              amzn2-core                  81 k\n",
    " runc                x86_64          1.0.0-0.1.20200826.gitff819c7.amzn2            amzn2extra-docker          3.7 M\n",
    "\n",
    "Transaction Summary\n",
    "=====================================================================================================================\n",
    "Install  1 Package (+4 Dependent packages)\n",
    "\n",
    "Total download size: 65 M\n",
    "Installed size: 270 M\n",
    "Downloading packages:\n",
    "(1/5): libcgroup-0.41-21.amzn2.x86_64.rpm                                                     |  66 kB  00:00:00\n",
    "(2/5): pigz-2.3.4-1.amzn2.0.1.x86_64.rpm                                                      |  81 kB  00:00:00\n",
    "(3/5): containerd-1.4.1-2.amzn2.x86_64.rpm                                                    |  24 MB  00:00:00\n",
    "(4/5): runc-1.0.0-0.1.20200826.gitff819c7.amzn2.x86_64.rpm                                    | 3.7 MB  00:00:00\n",
    "(5/5): docker-19.03.13ce-1.amzn2.x86_64.rpm                                                   |  37 MB  00:00:00\n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "Total                                                                                103 MB/s |  65 MB  00:00:00\n",
    "Running transaction check\n",
    "Running transaction test\n",
    "Transaction test succeeded\n",
    "Running transaction\n",
    "  Installing : runc-1.0.0-0.1.20200826.gitff819c7.amzn2.x86_64                                                   1/5\n",
    "  Installing : containerd-1.4.1-2.amzn2.x86_64                                                                   2/5\n",
    "  Installing : pigz-2.3.4-1.amzn2.0.1.x86_64                                                                     3/5\n",
    "  Installing : libcgroup-0.41-21.amzn2.x86_64                                                                    4/5\n",
    "  Installing : docker-19.03.13ce-1.amzn2.x86_64                                                                  5/5\n",
    "  Verifying  : docker-19.03.13ce-1.amzn2.x86_64                                                                  1/5\n",
    "  Verifying  : libcgroup-0.41-21.amzn2.x86_64                                                                    2/5\n",
    "  Verifying  : pigz-2.3.4-1.amzn2.0.1.x86_64                                                                     3/5\n",
    "  Verifying  : containerd-1.4.1-2.amzn2.x86_64                                                                   4/5\n",
    "  Verifying  : runc-1.0.0-0.1.20200826.gitff819c7.amzn2.x86_64                                                   5/5\n",
    "\n",
    "Installed:\n",
    "  docker.x86_64 0:19.03.13ce-1.amzn2\n",
    "\n",
    "Dependency Installed:\n",
    "  containerd.x86_64 0:1.4.1-2.amzn2                 libcgroup.x86_64 0:0.41-21.amzn2 pigz.x86_64 0:2.3.4-1.amzn2.0.1\n",
    "  runc.x86_64 0:1.0.0-0.1.20200826.gitff819c7.amzn2\n",
    "\n",
    "Complete!\n",
    "  0  ansible2                 available    \\\n",
    "        [ =2.4.2  =2.4.6  =2.8  =stable ]\n",
    "  2  httpd_modules            available    [ =1.0  =stable ]\n",
    "  3  memcached1.5             available    \\\n",
    "        [ =1.5.1  =1.5.16  =1.5.17 ]\n",
    "  5  postgresql9.6            available    \\\n",
    "        [ =9.6.6  =9.6.8  =stable ]\n",
    "  6  postgresql10             available    [ =10  =stable ]\n",
    "  8  redis4.0                 available    \\\n",
    "        [ =4.0.5  =4.0.10  =stable ]\n",
    "  9  R3.4                     available    [ =3.4.3  =stable ]\n",
    " 10  rust1                    available    \\\n",
    "        [ =1.22.1  =1.26.0  =1.26.1  =1.27.2  =1.31.0  =1.38.0\n",
    "          =stable ]\n",
    " 11  vim                      available    [ =8.0  =stable ]\n",
    " 15  php7.2                   available    \\\n",
    "        [ =7.2.0  =7.2.4  =7.2.5  =7.2.8  =7.2.11  =7.2.13  =7.2.14\n",
    "          =7.2.16  =7.2.17  =7.2.19  =7.2.21  =7.2.22  =7.2.23\n",
    "          =7.2.24  =7.2.26  =stable ]\n",
    " 17  lamp-mariadb10.2-php7.2  available    \\\n",
    "        [ =10.2.10_7.2.0  =10.2.10_7.2.4  =10.2.10_7.2.5\n",
    "          =10.2.10_7.2.8  =10.2.10_7.2.11  =10.2.10_7.2.13\n",
    "          =10.2.10_7.2.14  =10.2.10_7.2.16  =10.2.10_7.2.17\n",
    "          =10.2.10_7.2.19  =10.2.10_7.2.22  =10.2.10_7.2.23\n",
    "          =10.2.10_7.2.24  =stable ]\n",
    " 18  libreoffice              available    \\\n",
    "        [ =5.0.6.2_15  =5.3.6.1  =stable ]\n",
    " 19  gimp                     available    [ =2.8.22 ]\n",
    " 20  docker=latest            enabled      \\\n",
    "        [ =17.12.1  =18.03.1  =18.06.1  =18.09.9  =stable ]\n",
    " 21  mate-desktop1.x          available    \\\n",
    "        [ =1.19.0  =1.20.0  =stable ]\n",
    " 22  GraphicsMagick1.3        available    \\\n",
    "        [ =1.3.29  =1.3.32  =1.3.34  =stable ]\n",
    " 23  tomcat8.5                available    \\\n",
    "        [ =8.5.31  =8.5.32  =8.5.38  =8.5.40  =8.5.42  =8.5.50\n",
    "          =stable ]\n",
    " 24  epel                     available    [ =7.11  =stable ]\n",
    " 25  testing                  available    [ =1.0  =stable ]\n",
    " 26  ecs                      available    [ =stable ]\n",
    " 27  corretto8                available    \\\n",
    "        [ =1.8.0_192  =1.8.0_202  =1.8.0_212  =1.8.0_222  =1.8.0_232\n",
    "          =1.8.0_242  =stable ]\n",
    " 28  firecracker              available    [ =0.11  =stable ]\n",
    " 29  golang1.11               available    \\\n",
    "        [ =1.11.3  =1.11.11  =1.11.13  =stable ]\n",
    " 30  squid4                   available    [ =4  =stable ]\n",
    " 31  php7.3                   available    \\\n",
    "        [ =7.3.2  =7.3.3  =7.3.4  =7.3.6  =7.3.8  =7.3.9  =7.3.10\n",
    "          =7.3.11  =7.3.13  =stable ]\n",
    " 32  lustre2.10               available    \\\n",
    "        [ =2.10.5  =2.10.8  =stable ]\n",
    " 33  java-openjdk11           available    [ =11  =stable ]\n",
    " 34  lynis                    available    [ =stable ]\n",
    " 35  kernel-ng                available    [ =stable ]\n",
    " 36  BCC                      available    [ =0.x  =stable ]\n",
    " 37  mono                     available    [ =5.x  =stable ]\n",
    " 38  nginx1                   available    [ =stable ]\n",
    " 39  ruby2.6                  available    [ =2.6  =stable ]\n",
    " 40  mock                     available    [ =stable ]\n",
    " 41  postgresql11             available    [ =11  =stable ]\n",
    " 42  php7.4                   available    [ =stable ]\n",
    " 43  livepatch                available    [ =stable ]\n",
    " 44  python3.8                available    [ =stable ]\n",
    " 45  haproxy2                 available    [ =stable ]\n",
    " 46  collectd                 available    [ =stable ]\n",
    " 47  aws-nitro-enclaves-cli   available    [ =stable ]\n",
    " 48  R4                       available    [ =stable ]\n",
    "\n",
    "[ec2-user@ip-10-0-1-170 ~]$ sudo service docker start\n",
    "Redirecting to /bin/systemctl start docker.service\n",
    "\n",
    "[ec2-user@ip-10-0-1-170 ~]$ sudo usermod -a -G docker ec2-user\n",
    "\n",
    "[ec2-user@ip-10-0-1-170 ~]$ exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "터미널 로그아웃 후 다시 ec2로 재접속하고 아래와 같이 명령어를 실행했을때 sudo 명령어를 안줘도 되는지 확인해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-0-1-170 ~]$ docker info\n",
    "Client:\n",
    " Debug Mode: false\n",
    "\n",
    "Server:\n",
    " Containers: 0\n",
    "  Running: 0\n",
    "  Paused: 0\n",
    "  Stopped: 0\n",
    " Images: 0\n",
    " Server Version: 19.03.13-ce\n",
    " Storage Driver: overlay2\n",
    "  Backing Filesystem: xfs\n",
    "  Supports d_type: true\n",
    "  Native Overlay Diff: true\n",
    " Logging Driver: json-file\n",
    " Cgroup Driver: cgroupfs\n",
    " Plugins:\n",
    "  Volume: local\n",
    "  Network: bridge host ipvlan macvlan null overlay\n",
    "  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\n",
    " Swarm: inactive\n",
    " Runtimes: runc\n",
    " Default Runtime: runc\n",
    " Init Binary: docker-init\n",
    " containerd version: c623d1b36f09f8ef6536a057bd658b3aa8632828\n",
    " runc version: ff819c7e9184c13b7c2607fe6c30ae19403a7aff\n",
    " init version: de40ad0 (expected: fec3683)\n",
    " Security Options:\n",
    "  seccomp\n",
    "   Profile: default\n",
    " Kernel Version: 4.14.193-149.317.amzn2.x86_64\n",
    " Operating System: Amazon Linux 2\n",
    " OSType: linux\n",
    " Architecture: x86_64\n",
    " CPUs: 2\n",
    " Total Memory: 3.794GiB\n",
    " Name: ip-10-0-1-170.ap-northeast-2.compute.internal\n",
    " ID: SPCF:Y75R:B22D:BLRW:ELZ5:3O4B:HPDJ:R3JI:DSQM:W74R:TRA2:BXWE\n",
    " Docker Root Dir: /var/lib/docker\n",
    " Debug Mode: false\n",
    " Registry: https://index.docker.io/v1/\n",
    " Labels:\n",
    " Experimental: false\n",
    " Insecure Registries:\n",
    "  127.0.0.0/8\n",
    " Live Restore Enabled: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같은 docker run 명령어를 실행해서 제플린 개발환경을 구성한다.\n",
    "\n",
    "`docker run -p 4040:4040 -p 8080:8080 --privileged=true -v $PWD/logs:/logs -v $PWD/notebook:/notebook -e ZEPPELIN_NOTEBOOK_DIR='/notebook' -e ZEPPELIN_LOG_DIR='/logs' -d apache/zeppelin:0.8.1 /zeppelin/bin/zeppelin.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ec2-user@ip-10-0-1-170 ~]$ docker run -p 4040:4040 -p 8080:8080 --privileged=true -v $PWD/logs:/logs -v $PWD/notebook:/notebook \\\n",
    "> -e ZEPPELIN_NOTEBOOK_DIR='/notebook' \\\n",
    "> -e ZEPPELIN_LOG_DIR='/logs' \\\n",
    "> -d apache/zeppelin:0.8.1 \\\n",
    "> /zeppelin/bin/zeppelin.sh\n",
    "Unable to find image 'apache/zeppelin:0.8.1' locally\n",
    "0.8.1: Pulling from apache/zeppelin\n",
    "7b722c1070cd: Pull complete\n",
    "5fbf74db61f1: Pull complete\n",
    "ed41cb72e5c9: Pull complete\n",
    "7ea47a67709e: Pull complete\n",
    "7ba34fd9f5e0: Pull complete\n",
    "8f2f09b83582: Pull complete\n",
    "40260f0a8f69: Pull complete\n",
    "48946af5572c: Pull complete\n",
    "8b38acee7e8d: Pull complete\n",
    "a806f41d7e41: Pull complete\n",
    "7dcaf396dead: Pull complete\n",
    "8db355f40e66: Pull complete\n",
    "Digest: sha256:a3a90ec1579f5171ebac565e739547b885ed75efc1ec7581128ec1033a4496cb\n",
    "Status: Downloaded newer image for apache/zeppelin:0.8.1\n",
    "9276b4344e403414c354ec41816abcd6005e637836d31f691319323b672fe279\n",
    "\n",
    "[ec2-user@ip-10-0-1-170 ~]$ docker ps -a\n",
    "CONTAINER ID        IMAGE                   COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n",
    "9276b4344e40        apache/zeppelin:0.8.1   \"/usr/bin/tini -- /z…\"   7 seconds ago       Up 5 seconds        0.0.0.0:4040->4040/tcp, 0.0.0.0:8080->8080/tcp   confident_swirles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 정상적으로 도커를 실행하고 나면 웹브라우저를 열고 `[ec2 public ip]:8080` 으로 접속해서 제플린으로 정상 접근가능한지 확인해본다.\n",
    "\n",
    "접속한 다음에 `Create new note`을 클릭해서 spark 노트북을 새로 만들어준다. 그런 다음에 아래 그림과 같이 첫번째 블럭에 spark 문자열을 입력후 실행해본다. 실행하면 sparksession이 구동되는 것을 확인할 수 있다.\n",
    "\n",
    "![1](https://user-images.githubusercontent.com/41605276/101123798-f3b92e00-3638-11eb-8be0-71e23ed84eae.PNG)\n",
    "\n",
    "그런 다음에 웹브라우저를 하나 새로열고 `[ec2 public ip]:4040`를 입력해서 아래 그림과 같이 spark ui에 정상접속하는지 확인해본다.\n",
    "\n",
    "\n",
    "![2](https://user-images.githubusercontent.com/41605276/101123952-4e528a00-3639-11eb-9a9e-889d83083460.PNG)\n",
    "\n",
    "생성한 제플린 노트북에서 아래와 같은 코드로 실습을 해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark\n",
    "//sparksession driver process,어플리케이션 기준 \n",
    "spark\n",
    "\n",
    "\n",
    "%spark\n",
    "//한개의 컬럼과 1000개의 row(0~999)를 생성한 데이터 프레임을 만든다\n",
    "val myRange = spark.range(1000).toDF(\"number\")\n",
    "\n",
    "\n",
    "%spark\n",
    "//transformation - narrow dependency\n",
    "val divisBy2 = myRange.where(\"number % 2 = 0\")\n",
    "\n",
    "%spark\n",
    "//action\n",
    "divisBy2.count()\n",
    "\n",
    "\n",
    "%sh\n",
    "wget https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/flight-data/csv/2015-summary.csv\n",
    "//데이터 다운로드\n",
    "\n",
    "%sh\n",
    "head /zeppelin/2015-summary.csv\n",
    "//데이터 확인\n",
    "\n",
    "\n",
    "%spark\n",
    "val flightData2015 = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"/zeppelin/2015-summary.csv\")\n",
    "//CSV데이터 데이터 프레임으로 바로 읽기, visit to http://localhost:4040\n",
    "\n",
    "\n",
    "%spark\n",
    "flightData2015.take(3)\n",
    "//3건만 가져오기\n",
    "\n",
    "%spark\n",
    "flightData2015.sort(\"count\").explain()\n",
    "//실행계획 확인하기\n",
    "\n",
    "%spark\n",
    "flightData2015.sort(\"count\").take(2)\n",
    "//count필드로 정렬해서 2개 가져오기\n",
    "\n",
    "\n",
    "%spark\n",
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")\n",
    "//SQL을 사용하기 위해 임시 view만들기\n",
    "\n",
    "%spark\n",
    "val sqlWay = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, count(1)\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\n",
    "//SQL로DEST_COUNTRY_NAME 기준으로 몇건인지 확인하는 spark sql실행하기\n",
    "\n",
    "%spark\n",
    "sqlWay.describe().show()\n",
    "//데이터 프레임 통계 보기\n",
    "\n",
    "\n",
    "%spark\n",
    "val dataFrameWay = flightData2015\n",
    "  .groupBy('DEST_COUNTRY_NAME)\n",
    "  .count.explain\n",
    "//데이터 프레임으로 실행계획 확인하기\n",
    "\n",
    "%spark\n",
    "sqlWay.explain\n",
    "//sql과 dataframe실행계획 비교하기\n",
    "\n",
    "%spark\n",
    "spark.sql(\"SELECT max(count) from flight_data_2015\").take(1)\n",
    "//sql질의로 max값 가져와서 take로 한건만 확인하기\n",
    "\n",
    "%spark\n",
    "import org.apache.spark.sql.functions.max\n",
    "\n",
    "flightData2015.select(max(\"count\")).take(1)\n",
    "//데이터 프레임에서 max함수 이용하여 count의 최대값 가져오기\n",
    "\n",
    "%spark\n",
    "val maxSql = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "//spark sql로 DEST_COUNTRY_NAME 을 집계연산을 수행하여 합계를 구하여 큰 순서대로 5건 확인하기\n",
    "\n",
    "%spark\n",
    "maxSql.explain\n",
    "maxSql.show()\n",
    "//sql결과 확인하기\n",
    "\n",
    "\n",
    "%spark\n",
    "//동일한 쿼리를 dataframe으로 구현해 봅니다.  End-to-End example\n",
    "flightData2015\n",
    ".groupBy(\"DEST_COUNTRY_NAME\")\n",
    ".sum(\"count\")\n",
    ".withColumnRenamed(\"sum(count)\", \"destination_total\")\n",
    ".sort(desc(\"destination_total\"))\n",
    ".limit(5)\n",
    ".explain\n",
    "\n",
    "\n",
    "%spark\n",
    "flightData2015\n",
    ".groupBy(\"DEST_COUNTRY_NAME\")\n",
    ".sum(\"count\")\n",
    ".withColumnRenamed(\"sum(count)\", \"destination_total\")\n",
    ".sort(desc(\"destination_total\"))\n",
    ".limit(5)\n",
    ".show()\n",
    "\n",
    "\n",
    "%spark\n",
    "//임의의 값을 가지고 데이터 프레임 생성하기\n",
    "val data = Seq((\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"),\n",
    "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"),\n",
    "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"),\n",
    "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\"))\n",
    "\n",
    "import spark.sqlContext.implicits._\n",
    "val df = data.toDF(\"Product\",\"Amount\",\"Country\")\n",
    "df.show()\n",
    "\n",
    "//각 제품의 각 국가로 수출 된 총 금액을 얻으려면 제품 별 그룹화, 국가 별 피봇 팅 및 금액 합계를 얻어온다\n",
    "val pivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
    "pivotDF.show()\n",
    "\n",
    "//spark2.0에서 성능향상을 위해 pivot대상열을 선언함 \n",
    "val countries = Seq(\"USA\",\"China\",\"Canada\",\"Mexico\")\n",
    "val pivotDF = df.groupBy(\"Product\").pivot(\"Country\", countries).sum(\"Amount\")\n",
    "pivotDF.show()\n",
    "\n",
    "//성능 향상을 위해 두단계 집계를 사용함 \n",
    "val pivotDF = df.groupBy(\"Product\",\"Country\")\n",
    "      .sum(\"Amount\")\n",
    "      .groupBy(\"Product\")\n",
    "      .pivot(\"Country\")\n",
    "      .sum(\"sum(Amount)\")\n",
    "pivotDF.show()\n",
    "\n",
    "\n",
    "//stack기능을 이용하여 주요 국가에 대해 unpivot을 수행\n",
    "val unPivotDF = pivotDF.select($\"Product\",\n",
    "expr(\"stack(4, 'Canada', Canada, 'China', China, 'Mexico', Mexico, 'USA', USA) as (Country,Total)\"))\n",
    ".where(\"Total is not null\")\n",
    "unPivotDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=XwcDKo0Hce0&feature=youtu.be\n",
    "\n",
    "3시간 21분 8초"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
