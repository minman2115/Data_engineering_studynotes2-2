{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 목적\n",
    "\n",
    "\n",
    "EMR 서비스의 spark(sparkSQL)를 이용해서 S3내에 저장된 csv 형태의 데이터를 join 연산으로 처리해본다.\n",
    "\n",
    "\n",
    "### 데이터 처리 케이스\n",
    "\n",
    "- case 1 : A, B, C 파일을 한번에 join\n",
    "\n",
    "\n",
    "- case 2 : A,B file을 join한 중간결과를 HDFS에 저장한 다음 다시 중간결과를 로딩하여 C file과 join\n",
    "\n",
    "\n",
    "- case 3 : A,B file을 먼저 join하고 중간결과를 메모리에 변수에 할당 후 C file과 join\n",
    "\n",
    "\n",
    "** 참고사항 : A,B,C 데이터는 CSV 형태의 파일 데이터(약 8천 500만건, 12GB규모)를 이름만 각각 A,B,C 라고 다르게 지정하였을뿐 모두 동일한 데이터\n",
    "\n",
    "\n",
    "### 테스트 환경\n",
    "\n",
    "\n",
    "- EMR 5.28.1의 spark 2.4.4\n",
    "\n",
    "\n",
    "- 각 노드는 일괄 m5.xlarge(4 vcpu, 16GB mem) 사양으로, 마스터노드 1EA, 코어노드(64GB의 하드볼륨) 4EA로 구성\n",
    "\n",
    "\n",
    "- EMR 서비스에서 제공하는 jupyter notebook 환경에서 테스트\n",
    "\n",
    "### 테스트 데이터\n",
    "\n",
    "- 전자제품 구매이력 데이터\n",
    "\n",
    "각 컬럼은 구매시간(event_time), 브랜드(brand), 가격(price), 구매자(user_id) 등으로 구성\n",
    "\n",
    "- 약 8천4백만건의 레코드, 12GB 용량\n",
    "\n",
    "\n",
    "- 아래 코드는 실제 원본 테스트 데이터에 대한 샘플 print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227ec7744eed4764822a71eac3ebe828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1597017600341_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-1-10-232.ap-northeast-2.compute.internal:20888/proxy/application_1597017600341_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-1-10-33.ap-northeast-2.compute.internal:8042/node/containerlogs/container_1597017600341_0003_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+----+\n",
      "|          event_time|event_type|product_id|        category_id|       category_code|   brand|  price|  user_id|        user_session|year|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+----+\n",
      "|2019-10-01 00:00:...|      view|  44600062|2103807459595387724|                null|shiseido|  35.79|541312140|72d76fde-8bb3-4e0...|2019|\n",
      "|2019-10-01 00:00:...|      view|   3900821|2053013552326770905|appliances.enviro...|    aqua|  33.20|554748717|9333dfbd-b87a-470...|2019|\n",
      "|2019-10-01 00:00:...|      view|  17200506|2053013559792632471|furniture.living_...|    null| 543.10|519107250|566511c2-e2e3-422...|2019|\n",
      "|2019-10-01 00:00:...|      view|   1307067|2053013558920217191|  computers.notebook|  lenovo| 251.74|550050854|7c90fc70-0e80-459...|2019|\n",
      "|2019-10-01 00:00:...|      view|   1004237|2053013555631882655|electronics.smart...|   apple|1081.98|535871217|c6bd7419-2748-4c5...|2019|\n",
      "|2019-10-01 00:00:...|      view|   1480613|2053013561092866779|   computers.desktop|  pulser| 908.62|512742880|0d0d91c2-c9c2-4e8...|2019|\n",
      "|2019-10-01 00:00:...|      view|  17300353|2053013553853497655|                null|   creed| 380.96|555447699|4fe811e9-91de-46d...|2019|\n",
      "|2019-10-01 00:00:...|      view|  31500053|2053013558031024687|                null|luminarc|  41.16|550978835|6280d577-25c8-414...|2019|\n",
      "|2019-10-01 00:00:...|      view|  28719074|2053013565480109009|  apparel.shoes.keds|   baden| 102.71|520571932|ac1cd4e5-a3ce-422...|2019|\n",
      "|2019-10-01 00:00:...|      view|   1004545|2053013555631882655|electronics.smart...|  huawei| 566.01|537918940|406c46ed-90a4-478...|2019|\n",
      "|2019-10-01 00:00:...|      view|   2900536|2053013554776244595|appliances.kitche...|elenberg|  51.46|555158050|b5bdd0b3-4ca2-4c5...|2019|\n",
      "|2019-10-01 00:00:...|      view|   1005011|2053013555631882655|electronics.smart...| samsung| 900.64|530282093|50a293fb-5940-41b...|2019|\n",
      "|2019-10-01 00:00:...|      view|   3900746|2053013552326770905|appliances.enviro...|   haier| 102.38|555444559|98b88fa0-d8fa-4b9...|2019|\n",
      "|2019-10-01 00:00:...|      view|  44600062|2103807459595387724|                null|shiseido|  35.79|541312140|72d76fde-8bb3-4e0...|2019|\n",
      "|2019-10-01 00:00:...|      view|  13500240|2053013557099889147|furniture.bedroom...|     brw|  93.18|555446365|7f0062d8-ead0-4e0...|2019|\n",
      "|2019-10-01 00:00:...|      view|  23100006|2053013561638126333|                null|    null| 357.79|513642368|17566c27-0a8f-450...|2019|\n",
      "|2019-10-01 00:00:...|      view|   1801995|2053013554415534427|electronics.video.tv|   haier| 193.03|537192226|e3151795-c355-4ef...|2019|\n",
      "|2019-10-01 00:00:...|      view|  10900029|2053013555069845885|appliances.kitche...|   bosch|  58.95|519528062|901b9e3c-3f8f-414...|2019|\n",
      "|2019-10-01 00:00:...|      view|   1306631|2053013558920217191|  computers.notebook|      hp| 580.89|550050854|7c90fc70-0e80-459...|2019|\n",
      "|2019-10-01 00:00:...|      view|   1005135|2053013555631882655|electronics.smart...|   apple|1747.79|535871217|c6bd7419-2748-4c5...|2019|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "record count :  84897528"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"ABC data test\").getOrCreate()\n",
    "df = spark.read.option(\"header\",\"true\").csv(\"s3a://pms-bucket-test/testdata/10G.csv\")\n",
    "df.show()\n",
    "print(\"record count : \", df.count())\n",
    "\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A,B,C 파일을 join 연산을 해보는 것이 목적이기 때문에 join key 역할을 하는 id라는 컬럼을 추가하고 각 파일의 컬럼에 어떤 파일의 컬림인지 구분자를 주기 위해서 위의 원본데이터를 아래와 같이 전처리하여 A,B,C 라는 파일을 생성해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71e9fdff5744cb49aff3c4318b99b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------+-------------------+--------------------+--------+-------+---------+--------------------+------+---+\n",
      "|        event_time_C|event_type_C|product_id_C|      category_id_C|     category_code_C| brand_C|price_C|user_id_C|      user_session_C|year_C| id|\n",
      "+--------------------+------------+------------+-------------------+--------------------+--------+-------+---------+--------------------+------+---+\n",
      "|2019-10-01 00:00:...|        view|    44600062|2103807459595387724|                null|shiseido|  35.79|541312140|72d76fde-8bb3-4e0...|  2019|  0|\n",
      "|2019-10-01 00:00:...|        view|     3900821|2053013552326770905|appliances.enviro...|    aqua|  33.20|554748717|9333dfbd-b87a-470...|  2019|  1|\n",
      "|2019-10-01 00:00:...|        view|    17200506|2053013559792632471|furniture.living_...|    null| 543.10|519107250|566511c2-e2e3-422...|  2019|  2|\n",
      "|2019-10-01 00:00:...|        view|     1307067|2053013558920217191|  computers.notebook|  lenovo| 251.74|550050854|7c90fc70-0e80-459...|  2019|  3|\n",
      "|2019-10-01 00:00:...|        view|     1004237|2053013555631882655|electronics.smart...|   apple|1081.98|535871217|c6bd7419-2748-4c5...|  2019|  4|\n",
      "|2019-10-01 00:00:...|        view|     1480613|2053013561092866779|   computers.desktop|  pulser| 908.62|512742880|0d0d91c2-c9c2-4e8...|  2019|  5|\n",
      "|2019-10-01 00:00:...|        view|    17300353|2053013553853497655|                null|   creed| 380.96|555447699|4fe811e9-91de-46d...|  2019|  6|\n",
      "|2019-10-01 00:00:...|        view|    31500053|2053013558031024687|                null|luminarc|  41.16|550978835|6280d577-25c8-414...|  2019|  7|\n",
      "|2019-10-01 00:00:...|        view|    28719074|2053013565480109009|  apparel.shoes.keds|   baden| 102.71|520571932|ac1cd4e5-a3ce-422...|  2019|  8|\n",
      "|2019-10-01 00:00:...|        view|     1004545|2053013555631882655|electronics.smart...|  huawei| 566.01|537918940|406c46ed-90a4-478...|  2019|  9|\n",
      "|2019-10-01 00:00:...|        view|     2900536|2053013554776244595|appliances.kitche...|elenberg|  51.46|555158050|b5bdd0b3-4ca2-4c5...|  2019| 10|\n",
      "|2019-10-01 00:00:...|        view|     1005011|2053013555631882655|electronics.smart...| samsung| 900.64|530282093|50a293fb-5940-41b...|  2019| 11|\n",
      "|2019-10-01 00:00:...|        view|     3900746|2053013552326770905|appliances.enviro...|   haier| 102.38|555444559|98b88fa0-d8fa-4b9...|  2019| 12|\n",
      "|2019-10-01 00:00:...|        view|    44600062|2103807459595387724|                null|shiseido|  35.79|541312140|72d76fde-8bb3-4e0...|  2019| 13|\n",
      "|2019-10-01 00:00:...|        view|    13500240|2053013557099889147|furniture.bedroom...|     brw|  93.18|555446365|7f0062d8-ead0-4e0...|  2019| 14|\n",
      "|2019-10-01 00:00:...|        view|    23100006|2053013561638126333|                null|    null| 357.79|513642368|17566c27-0a8f-450...|  2019| 15|\n",
      "|2019-10-01 00:00:...|        view|     1801995|2053013554415534427|electronics.video.tv|   haier| 193.03|537192226|e3151795-c355-4ef...|  2019| 16|\n",
      "|2019-10-01 00:00:...|        view|    10900029|2053013555069845885|appliances.kitche...|   bosch|  58.95|519528062|901b9e3c-3f8f-414...|  2019| 17|\n",
      "|2019-10-01 00:00:...|        view|     1306631|2053013558920217191|  computers.notebook|      hp| 580.89|550050854|7c90fc70-0e80-459...|  2019| 18|\n",
      "|2019-10-01 00:00:...|        view|     1005135|2053013555631882655|electronics.smart...|   apple|1747.79|535871217|c6bd7419-2748-4c5...|  2019| 19|\n",
      "+--------------------+------------+------------+-------------------+--------------------+--------+-------+---------+--------------------+------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "record count :  84897528"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ABC data test\").getOrCreate()\n",
    "\n",
    "_list = ['A','B','C']\n",
    "\n",
    "for elem in _list:\n",
    "    \n",
    "    df = spark.read.option(\"header\",\"true\").csv(\"s3a://pms-bucket-test/testdata/10G.csv\")\n",
    "    \n",
    "    # 기존의 컬럼에 A 파일의 컬럼인지,B 인지,C 인지 구분자를 붙여주는 전처리과정\n",
    "    df = df.withColumnRenamed(\"event_time\",\"event_time_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"event_type\",\"event_type_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"product_id\",\"product_id_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"category_id\",\"category_id_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"category_code\",\"category_code_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"brand\",\"brand_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"price\",\"price_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"user_id\",\"user_id_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"year\",\"year_{}\".format(elem))\\\n",
    "           .withColumnRenamed(\"user_session\",\"user_session_{}\".format(elem))\\\n",
    "           .withColumn(\"id\", F.monotonically_increasing_id()) ## id라는 레코드별 고유넘버 컬럼을 추가해서 id를 기준으로 join할 예정\n",
    "    \n",
    "    # 1개의 csv 파일형태로 전처리한 데이터를 s3에 저장\n",
    "    df.repartition(1).write.option(\"header\", \"true\").csv(\"s3a://pms-bucket-test/{}\".format(elem))\n",
    "    \n",
    "    \n",
    "df.show()\n",
    "\n",
    "print(\"record count : \", df.count())\n",
    "\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 처리해보기\n",
    "\n",
    "#### 1) case 1\n",
    "\n",
    "A, B, C 파일을 한번에 join\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/41605276/89480390-27419400-d7d0-11ea-8bce-d21e4a5ac9fa.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4da93f142bf47f7af095ec931f96e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of the dataframe records :  84897528\n",
      "code running time (sec): 128.2586009502411 , code finish time (GMT) :  2020-08-10 00:48"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# 코드 스타트 시간 체크\n",
    "start = time.time()\n",
    "\n",
    "# 스파크 세션을 생성하여 데이터 프레임을 구동할 수 있도록 정의\n",
    "spark = SparkSession.builder.appName(\"minmantest\").getOrCreate()\n",
    "\n",
    "# A,B,C 파일을 id 컬럼을 기준으로 join하여 ABC라는 데이터 프레임을 생성\n",
    "ABC_df = spark.read.option(\"header\",\"true\").csv(\"s3a://pms-bucket-test/A.csv\")\\\n",
    "        .join(spark.read.options(header='true').csv(\"s3a://pms-bucket-test/B.csv\"),on='id',how='inner')\\\n",
    "        .join(spark.read.options(header='true').csv(\"s3a://pms-bucket-test/C.csv\"),on='id',how='inner')\n",
    "\n",
    "# ABC 프레임의 레코드 카운트\n",
    "print(\"Number of the dataframe records : \", ABC_df.count())\n",
    "\n",
    "# 코드 종료시간 체크\n",
    "endTimeQuery = time.clock()\n",
    "\n",
    "# 위의 전체 코드에 대한 소요시간\n",
    "print(\"code running time (sec):\", time.time() - start,', code finish time (GMT) : ',datetime.now().strftime('%Y-%m-%d %H:%M'))\n",
    "\n",
    "# 데이터 프레임 캐시제거\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 코드와 동일한 결과를 내는 코드로 sparkSQL 쿼리를 이용해서 처리해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12a3691a8ca4bddb1701f986779d44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|84897528|\n",
      "+--------+\n",
      "\n",
      "code running time (sec): 112.01974630355835 , code finish time (GMT) :  2020-08-10 00:50\n",
      "DataFrame[id: string, event_time_A: string, event_type_A: string, product_id_A: string, category_id_A: string, category_code_A: string, brand_A: string, price_A: string, user_id_A: string, user_session_A: string, year_A: string, event_time_B: string, event_type_B: string, product_id_B: string, category_id_B: string, category_code_B: string, brand_B: string, price_B: string, user_id_B: string, user_session_B: string, year_B: string, event_time_C: string, event_type_C: string, product_id_C: string, category_id_C: string, category_code_C: string, brand_C: string, price_C: string, user_id_C: string, user_session_C: string, year_C: string]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "import pyspark\n",
    "import time\n",
    "\n",
    "# 코드 스타트 시간 체크\n",
    "start = time.time()\n",
    "\n",
    "# 스파크 세션을 생성하여 데이터 프레임을 구동할 수 있도록 정의\n",
    "spark = SparkSession.builder.appName(\"minmantest\").getOrCreate()\n",
    "\n",
    "# A 데이터 로딩, 임시테이블 생성\n",
    "A_df = spark.read.option(\"header\",\"true\").csv(\"s3a://pms-bucket-test/A.csv\")\n",
    "A_df.createOrReplaceTempView(\"Atable\")\n",
    "\n",
    "# B 데이터 로딩, 임시테이블 생성\n",
    "B_df = spark.read.option(\"header\",\"true\").csv(\"s3a://pms-bucket-test/B.csv\")\n",
    "B_df.createOrReplaceTempView(\"Btable\")\n",
    "\n",
    "# C 데이터 로딩, 임시테이블 생성\n",
    "C_df = spark.read.option(\"header\",\"true\").csv(\"s3a://pms-bucket-test/C.csv\")\n",
    "C_df.createOrReplaceTempView(\"Ctable\")\n",
    "\n",
    "# A,B,C 데이터를 한번에 JOIN하여 ABC라는 데이터 프레임 생성 후, createOrReplaceTempView를 이용하여 ABC 임시테이블 생성\n",
    "ABC_df = spark.sql(\"SELECT * FROM Atable JOIN Btable USING(id) JOIN Ctable USING(id)\")\n",
    "ABC_df.createOrReplaceTempView(\"ABCtable\")\n",
    "\n",
    "# ABC 데이터프레임의 레코드 카운트\n",
    "spark.sql(\"SELECT COUNT(*) FROM ABCtable\").show()\n",
    "\n",
    "# 코드 종료시간 체크\n",
    "endTimeQuery = time.clock()\n",
    "\n",
    "# 위의 전체 코드에 대한 소요시간\n",
    "print(\"code running time (sec):\", time.time() - start,', code finish time (GMT) : ',datetime.now().strftime('%Y-%m-%d %H:%M'))\n",
    "\n",
    "# 데이터 프레임 캐시제거\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래와 같이 스파크 세션을 구동할때 아래와 같이 config 값도 부여할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"minmantest\")\\\n",
    "        .config(\"spark.sql.broadcastTimeout\", \"36000\")\\\n",
    "        .config(\"spark.executor.instances\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\", \"4g\")\\\n",
    "        .config(\"spark.executor.cores\", \"3\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고자료\n",
    "\n",
    "1) https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html\n",
    "\n",
    "2) https://aws.amazon.com/ko/blogs/korea/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/\n",
    "\n",
    "3) https://icthuman.tistory.com/entry/Spark%EB%A5%BC-YARN%EC%97%90-%EC%88%98%ED%96%89%ED%95%A0-%EB%95%8C-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EC%84%B8%ED%8C%85-%EC%B6%94%EC%B2%9C-1\n",
    "\n",
    "4) https://m.blog.naver.com/gyrbsdl18/220880041737\n",
    "\n",
    "\n",
    "- 만약에 연산한 데이터 프레임을 HDFS에 저장하고 싶을때는 주피터 노트북에 아래와 같은 코드를 실행하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABC_df.repartition(1).write.csv(\"hdfs:///test/ABC_df_folder\")\n",
    "\n",
    "# 주피터에서 위의 명령어를 이용해서 파일을 저장하고, 그런 다음에\n",
    "# 마스터노드에 ssh 접속해서 아래와 같이 명령어를 입력하면 해당 파일을 확인할 수 있다.\n",
    "\n",
    "[hadoop@ip-10-1-10-143 conf]$ hdfs dfs -ls /test/ABC_df_folder\n",
    "Found 2 items\n",
    "-rw-r--r--   2 livy hadoop           0 2020-08-09 09:23 /test/ABC_df_folder/_SUCCESS\n",
    "-rw-r--r--   2 livy hadoop 36606928013 2020-08-09 09:23 /test/ABC_df_folder/part-00000-68558a96-ed42-4515-ae57-543d34d6fd88-c000.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Test case 2\n",
    "\n",
    "A,B file을 join한 중간결과를 HDFS에 저장한 다음 다시 중간결과를 로딩하여 C file과 join\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/41605276/89480427-43453580-d7d0-11ea-9d4c-97e1119b9e3a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8acdaeb2fe440a792f3e9020ab86ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of the dataframe records :  84897528\n",
      "code running time (sec): 959.8760902881622 , code finish time (GMT) :  2020-08-10 02:28"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# 코드 스타트 시간 체크\n",
    "start = time.time()\n",
    "\n",
    "# 스파크 세션을 생성하여 데이터 프레임을 구동할 수 있도록 정의\n",
    "spark = SparkSession.builder.appName(\"minmantest\").getOrCreate()\n",
    "\n",
    "# A,B 파일을 join하여 AB라는 데이터 프레임을 생성\n",
    "AB_df = spark.read.option(\"header\",\"true\").csv(\"s3a://pms-bucket-test/A.csv\")\\\n",
    "        .join(spark.read.options(header='true').csv(\"s3a://pms-bucket-test/B.csv\"),on='id',how='inner')\\\n",
    "        .persist(StorageLevel.DISK_ONLY) # RDD를 디스크에만 상주시키는 옵션\n",
    "\n",
    "# AB 데이터 프레임에 C파일 join하여 ABC라는 데이터 프레임을 생성\n",
    "ABC_df = AB_df.join(spark.read.options(header='true').csv(\"s3a://pms-bucket-test/C.csv\"),on='id',how='inner')\\\n",
    "         .persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "# ABC 데이터프레임 레코드 카운트\n",
    "print(\"Number of the dataframe records : \", ABC_df.count())\n",
    "\n",
    "# 코드 종료시간 체크\n",
    "endTimeQuery = time.clock()\n",
    "\n",
    "# 위의 전체 코드에 대한 소요시간\n",
    "print(\"code running time (sec):\", time.time() - start,', code finish time (GMT) : ',datetime.now().strftime('%Y-%m-%d %H:%M'))\n",
    "\n",
    "# 데이터 프레임 캐시제거\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sparkSQL 쿼리를 이용해서 처리하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68c43e3e805473bb938efd8bddbd644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|84897528|\n",
      "+--------+\n",
      "\n",
      "code running time (sec): 926.3053381443024 , code finish time (GMT) :  2020-08-10 02:09"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "import pyspark\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# 코드 스타트 시간 체크\n",
    "start = time.time()\n",
    "\n",
    "# 스파크 세션을 생성하여 데이터 프레임을 구동할 수 있도록 정의\n",
    "spark = SparkSession.builder.appName(\"minmantest\").getOrCreate()\n",
    "\n",
    "# A 데이터 로딩, 임시테이블 생성\n",
    "A_df = spark.read.option(\"header\",\"true\").csv(\"s3a://pms-bucket-test/A.csv\")\n",
    "A_df.createOrReplaceTempView(\"Atable\")\n",
    "\n",
    "# B 데이터 로딩, 임시테이블 생성\n",
    "B_df = spark.read.option(\"header\",\"true\").csv(\"s3a://pms-bucket-test/B.csv\")\n",
    "B_df.createOrReplaceTempView(\"Btable\")\n",
    "\n",
    "# C 데이터 로딩, 임시테이블 생성\n",
    "C_df = spark.read.option(\"header\",\"true\").csv(\"s3a://pms-bucket-test/C.csv\")\n",
    "C_df.createOrReplaceTempView(\"Ctable\")\n",
    "\n",
    "# A,B 파일을 join하여 AB라는 데이터 프레임을 생성 후 DISK에 중간저장, AB 임시테이블 생성\n",
    "AB_df = spark.sql(\"SELECT * FROM Atable JOIN Btable USING(id)\").persist(StorageLevel.DISK_ONLY)\n",
    "AB_df.createOrReplaceTempView(\"ABtable\")\n",
    "\n",
    "# AB 데이터 프레임에 C파일 join하여 ABC라는 데이터 프레임 생성 후 DISK에 중간저장, AB 임시테이블 생성\n",
    "ABC_df = spark.sql(\"SELECT * FROM ABtable JOIN Ctable USING(id)\").persist(StorageLevel.DISK_ONLY)\n",
    "ABC_df.createOrReplaceTempView(\"ABCtable\")\n",
    "\n",
    "# ABC 데이터 프레임 레코드 카운트\n",
    "spark.sql(\"SELECT COUNT(*) FROM ABCtable\").show()\n",
    "\n",
    "# 코드 종료시간 체크\n",
    "endTimeQuery = time.clock()\n",
    "\n",
    "# 위의 전체 코드에 대한 소요시간\n",
    "print(\" code running time (sec) : \", time.time() - start,', code finish time (GMT) : ',datetime.now().strftime('%Y-%m-%d %H:%M'))\n",
    "\n",
    "# 데이터 프레임 캐시제거\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RDD Persistance, Stroage level 참고자료\n",
    "\n",
    "1) https://bcho.tistory.com/1029\n",
    "\n",
    "2) https://www.quora.com/How-will-you-select-the-storage-level-in-Apache-Spark\n",
    "\n",
    "#### 3) Test case 3\n",
    "\n",
    "A,B file을 먼저 join하고 중간결과를 메모리에 변수에 할당 후 C file과 join\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/41605276/89480462-5e17aa00-d7d0-11ea-9200-a85160e5072e.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de39d4f6c6141e4a55037b65e80fa5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of the dataframe records :  84897528\n",
      "code running time (sec): 113.00516510009766 , code finish time (GMT) :  2020-08-10 01:51"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# 코드 스타트 시간 체크\n",
    "start = time.time()\n",
    "\n",
    "# 스파크 세션을 생성하여 데이터 프레임을 구동할 수 있도록 정의\n",
    "spark = SparkSession.builder.appName(\"minmantest\").getOrCreate()\n",
    "\n",
    "# A,B 파일을 join하여 AB라는 데이터 프레임을 생성\n",
    "# 참고로 persist 미지정 시 메모리에 중간저장하게 되어있음\n",
    "AB_df = spark.read.option(\"header\",\"true\").csv(\"s3a://pms-bucket-test/A.csv\")\\\n",
    "        .join(spark.read.options(header='true').csv(\"s3a://pms-bucket-test/B.csv\"),on='id',how='inner')\n",
    "\n",
    "# AB 데이터 프레임에 C파일 join하여 ABC라는 데이터 프레임을 생성\n",
    "ABC_df = AB_df.join(spark.read.options(header='true').csv(\"s3a://pms-bucket-test/C.csv\"),on='id',how='inner')\n",
    "\n",
    "# ABC 데이터프레임 레코드 카운트\n",
    "print(\"Number of the dataframe records : \", ABC_df.count())\n",
    "\n",
    "# 위의 전체 코드에 대한 소요시간\n",
    "print(\" code running time (sec) : \", time.time() - start,', code finish time (GMT) : ',datetime.now().strftime('%Y-%m-%d %H:%M'))\n",
    "\n",
    "# 데이터 프레임 캐시제거\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sparkSQL 쿼리를 이용해서 처리하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9429be7cc6040bea38e8cbdc2dc6172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|84897528|\n",
      "+--------+\n",
      "\n",
      "code running time (sec): 111.46066069602966 , code finish time (GMT) :  2020-08-10 01:54"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "import pyspark\n",
    "import time\n",
    "\n",
    "# 코드 스타트 시간 체크\n",
    "start = time.time()\n",
    "\n",
    "# 스파크 세션을 생성하여 데이터 프레임을 구동할 수 있도록 정의\n",
    "spark = SparkSession.builder.appName(\"minmantest\").getOrCreate()\n",
    "\n",
    "# A 데이터 로딩, 임시테이블 생성\n",
    "A_df = spark.read.option(\"header\",\"true\").csv(\"s3a://pms-bucket-test/A.csv\")\n",
    "A_df.createOrReplaceTempView(\"Atable\")\n",
    "\n",
    "# B 데이터 로딩, 임시테이블 생성\n",
    "B_df = spark.read.option(\"header\",\"true\").csv(\"s3a://pms-bucket-test/B.csv\")\n",
    "B_df.createOrReplaceTempView(\"Btable\")\n",
    "\n",
    "# C 데이터 로딩, 임시테이블 생성\n",
    "C_df = spark.read.option(\"header\",\"true\").csv(\"s3a://pms-bucket-test/C.csv\")\n",
    "C_df.createOrReplaceTempView(\"Ctable\")\n",
    "\n",
    "# AB 데이터 JOIN, 임시테이블 생성\n",
    "AB_df = spark.sql(\"SELECT * FROM Atable JOIN Btable USING(id)\")\n",
    "AB_df.createOrReplaceTempView(\"ABtable\")\n",
    "\n",
    "# JOIN한 AB데이터에 C 데이터를 JOIN, 임시테이블 생성\n",
    "ABC_df = spark.sql(\"SELECT * FROM ABtable JOIN Ctable USING(id)\")\n",
    "ABC_df.createOrReplaceTempView(\"ABCtable\")\n",
    "\n",
    "# join 한 ABC 데이터의 레코드 카운트\n",
    "spark.sql(\"SELECT COUNT(*) FROM ABCtable\").show()\n",
    "\n",
    "# 코드 종료시간 체크\n",
    "endTimeQuery = time.clock()\n",
    "\n",
    "# 위의 전체 코드에 대한 소요시간\n",
    "print(\"code running time (sec):\", time.time() - start,', code finish time (GMT) : ',datetime.now().strftime('%Y-%m-%d %H:%M'))\n",
    "\n",
    "# 데이터 프레임 캐시제거\n",
    "spark.catalog.clearCache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
